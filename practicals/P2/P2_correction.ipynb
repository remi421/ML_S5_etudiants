{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "f29814e2",
      "metadata": {
        "id": "f29814e2"
      },
      "source": [
        "# **Practical session nÂ°2**\n",
        "\n",
        "Notions:\n",
        "- Convolution layers and maxpooling operations\n",
        "- Inference with deep standard neural networks for classification\n",
        "- Visualizing kernels and feature maps\n",
        "\n",
        "Duration: 2 h\n",
        "\n",
        "In this part, we focus on convolutional layers, which are the basic building blocks of classical architectures such as VGG and ResNet.\n",
        "To understand their effect, we manipulate them a bit and introduce the maxpooling operations often associated with them (**A**). Then, we observe how their parameters have converged after training on the ImageNet database (**B**). We also visualize the output signal of the convolutional layers (**C**)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2dc7e9b3",
      "metadata": {
        "id": "2dc7e9b3"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import os\n",
        "import torch\n",
        "import torch.nn as nn   # pre-defined layers\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "# to load pre-trained models on ImageNet:\n",
        "import torchvision\n",
        "from torchvision import datasets, models, transforms"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "945f5b38",
      "metadata": {
        "id": "945f5b38"
      },
      "source": [
        "**A.** Convolutional Layers and Maxpooling\n",
        "\n",
        "Before defining the convolutional layers, let's examine two standard deep learning models (vgg16 and resnet50). In the final layers, you will recognize perceptrons (\"classifier\"), formed from the nn.Linear class. \\\\\n",
        "But, in the first part of the network, you see the 'conv2d' layers that correspond to these convolutional layers."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8503d00c",
      "metadata": {
        "id": "8503d00c"
      },
      "outputs": [],
      "source": [
        "# Two deep neural networks: VGG16 and ResNet50\n",
        "\n",
        "model = models.vgg16(pretrained=False)\n",
        "# model = models.resnet50(pretrained=False)\n",
        "print(model)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "25587e81",
      "metadata": {
        "id": "25587e81"
      },
      "source": [
        "In the simplest case, in one dimension, the \"convolution\" is defined by a relation of the form:\n",
        "\\begin{equation*}\n",
        "output_i = bias + \\sum_{j = 1}^n input_{i + j} \\times kernel_{j}  \\tag{1}\n",
        "\\end{equation*}\n",
        "Here, $kernel$ represents a vector of size $n$ containing the parameters of the neuron. If, for example, $kernel$ is positive and has a sum of 1, it is a moving average. Finally, note that the classic convolution operator differs slightly ($input_{i - j}$ instead of $input_{i + j}$).\n",
        "\n",
        "The PyTorch code is more complicated because:\n",
        "- the input is generally a batch of images with multiple channels\n",
        "- there is not only one neuron in the layer\n",
        "\n",
        "The general form is defined [here](https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html#torch.nn.Conv2d).\n",
        "\n",
        "On this page, $N$, $C_{out}$, and, $C_{in}$ correspond respectively to the batch size, the number of neurons in the layer, and the number of input channels.\n",
        "\n",
        "The operator $\\star$ also hides subtleties: you need to set how the kernel moves on the input (*stride*) and deal with border issues (*padding*). These aspects, which we will not dwell on, are easier to understand from the animations by [Vincent Dumoulin](https://github.com/vdumoulin/conv_arithmetic).\n",
        "\n",
        "To illustrate the effect of a 2D convolutional layer with one neuron, let's load an RGB image."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "M6o8qZZvFq9E",
      "metadata": {
        "id": "M6o8qZZvFq9E"
      },
      "outputs": [],
      "source": [
        "# Clone the repo\n",
        "! git clone https://github.com/nanopiero/ML_S5_etudiants.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "64997eda",
      "metadata": {
        "id": "64997eda"
      },
      "outputs": [],
      "source": [
        "# Load the jpg image of a cat with the library PIL\n",
        "from PIL import Image\n",
        "root = 'ML_S5_etudiants/practicals/P2'\n",
        "path = os.path.join(root, 'cat.jpg')\n",
        "image = Image.open(path).convert(\"RGB\")\n",
        "image = image.resize((256,256))\n",
        "image"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "34e23ad3",
      "metadata": {
        "id": "34e23ad3"
      },
      "source": [
        "The \"convolution\" operators used by the `Conv2d` class are coded in the `torch.nn.functional` module. In the following lines, we define a Gaussian kernel that can then be applied to the image:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "03fca095",
      "metadata": {
        "id": "03fca095"
      },
      "outputs": [],
      "source": [
        "# Definition of a Gaussian kernel of size 10\n",
        "x_range = torch.arange(-5, 5, 0.5)\n",
        "y_range = torch.arange(-5, 5, 0.5)\n",
        "xx, yy = torch.meshgrid(x_range, y_range)\n",
        "var = 10\n",
        "\n",
        "kernel =  (1./(2.*3.14*var)) * torch.exp( - (xx**2 + yy**2)/(2*var) )  # Gaussian\n",
        "\n",
        "plt.imshow(kernel, cmap='gray')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ebf8e9bf",
      "metadata": {
        "id": "ebf8e9bf"
      },
      "outputs": [],
      "source": [
        "# Applying the same kernel to all channels:\n",
        "kernel = kernel.repeat(1,3,1,1)                        # repeat for each R,G,B channel\n",
        "\n",
        "# We added at the beginning the dimension associated with the image indexing in the batch (\"batch dimension\"):\n",
        "print(kernel.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9bba2eae",
      "metadata": {
        "id": "9bba2eae"
      },
      "outputs": [],
      "source": [
        "# Converting the 'cat.jpg' image to torch.tensor:\n",
        "img = transforms.ToTensor()(image)\n",
        "img = img.unsqueeze(dim=0)  # add the \"batch dimension\"\n",
        "\n",
        "# Convolution:\n",
        "import torch.nn.functional as F\n",
        "output = F.conv2d(img, kernel)\n",
        "fig = plt.figure()\n",
        "plt.imshow(output[0,0,:,:], cmap='gray')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ce0e4b69",
      "metadata": {
        "id": "ce0e4b69"
      },
      "source": [
        "In practice, kernels of such large size are rarely used. For example, in a VGG, the kernels are of size 3*3. This is sufficient to extract interesting features, such as contours.\n",
        "\n",
        "**Exercise 1:** Apply a [Prewitt filter](https://en.wikipedia.org/wiki/Prewitt_operator) to the image using PyTorch's `conv2d` (complete the following code)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b9cc3a7d",
      "metadata": {
        "deletable": false,
        "id": "b9cc3a7d",
        "nbgrader": {
          "cell_type": "code",
          "checksum": "fe5bc0bf0a87eb5b4a5d64d1064a63eb",
          "grade": false,
          "grade_id": "exercise-1",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        }
      },
      "outputs": [],
      "source": [
        "# kernel2 = ...\n",
        "# kernel2 = ...\n",
        "# YOUR CODE HERE\n",
        "raise NotImplementedError()\n",
        "#output1 = ..\n",
        "#output2 = ..\n",
        "# YOUR CODE HERE\n",
        "raise NotImplementedError()\n",
        "output = (output1**2 + output2**2).sqrt()\n",
        "\n",
        "fig = plt.figure()\n",
        "plt.imshow(output[0,0,:,:], cmap='gray')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# kernel2 = ...\n",
        "# kernel2 = ...\n",
        "### BEGIN SOLUTION\n",
        "kernel1 = torch.tensor([[1.,0.,-1.],[1.,0.,-1.],[1.,0.,-1.]])\n",
        "kernel1 = kernel1.repeat(1,3,1,1)\n",
        "kernel2 = torch.tensor([[1.,1.,1.],[0.,0.,0.],[-1.,-1.,-1.]])\n",
        "kernel2 = kernel2.repeat(1,3,1,1)\n",
        "### END SOLUTION\n",
        "#output1 = ..\n",
        "#output2 = ..\n",
        "### BEGIN SOLUTION\n",
        "output1 = F.conv2d(img, kernel1)\n",
        "output2 = F.conv2d(img, kernel2)\n",
        "### END SOLUTION\n",
        "output = (output1**2 + output2**2).sqrt()\n",
        "\n",
        "fig = plt.figure()\n",
        "plt.imshow(output[0,0,:,:], cmap='gray')"
      ],
      "metadata": {
        "id": "60msFjlNtXWt"
      },
      "id": "60msFjlNtXWt",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "a5c8f81d",
      "metadata": {
        "id": "a5c8f81d"
      },
      "source": [
        "However, the Prewitt filter cannot be encoded in a standard neural network: it involves a square root. The nonlinearities implemented in these networks are simpler:\n",
        "\n",
        "- the **ReLU function**. It is simply the \"positive part\" function.\n",
        "- **maxpooling**. It is a form of downsampling. In its most common form, the image is divided into 2*2 pixel squares, and the maximum value in each square is returned. The spatial dimensions of the output tensor are thus divided by two.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e2e1094d",
      "metadata": {
        "id": "e2e1094d"
      },
      "outputs": [],
      "source": [
        "# Positive part: ReLU function\n",
        "x = torch.rand(1,1,4,4) - 0.5\n",
        "print(x)\n",
        "print(x.relu())\n",
        "\n",
        "# Maxpooling:\n",
        "x = F.max_pool2d(x, kernel_size=2)\n",
        "print(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0dc793f3",
      "metadata": {
        "deletable": false,
        "id": "0dc793f3",
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "79ff8e02ad8c85d9c3a110f3c13b902e",
          "grade": true,
          "grade_id": "exercise-2",
          "locked": false,
          "points": 0,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "tags": []
      },
      "source": [
        "**Exercise 2:** What is the size of the tensor at the output of layer 30 of vgg16, compared to that of the input image?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "There are 5 successive maxpoolings, so the size has been divided by 32. However, the number of channels has increased from 3 to 512.\n",
        "When the signal passes through a VGG16 (or a ResNet50), successive maxpoolings decrease the number of pixels while the number of channels increases.  \n",
        "It is essential to understand that at the output of the last convolutional layers, the calculated values depend on a large area of the input image.  "
      ],
      "metadata": {
        "id": "qjvQL815tl8F"
      },
      "id": "qjvQL815tl8F"
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "f2ZFa1YAtkRE"
      },
      "id": "f2ZFa1YAtkRE"
    },
    {
      "cell_type": "markdown",
      "source": [
        "**B.** Convolution Kernels After Training\n",
        "\n",
        "Let's now observe the convolutional layers after training on a very large set of annotated images (~1M) from the ImageNet database ([http://www.image-net.org/challenges/LSVRC/2010/index](https://)). First, let's see what happens to the kernels associated with the 64 neurons in the first convolutional layer of a pre-trained ResNet50:"
      ],
      "metadata": {
        "id": "AkAmkw5OYOYu"
      },
      "id": "AkAmkw5OYOYu"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1eef565b",
      "metadata": {
        "id": "1eef565b"
      },
      "outputs": [],
      "source": [
        "# Before training:\n",
        "model = models.resnet50(pretrained=False)\n",
        "first_layer = model.conv1.weight.data\n",
        "\n",
        "print(first_layer.shape)\n",
        "\n",
        "plt.figure(figsize=(15, 15))\n",
        "for i in range(first_layer.shape[0]):\n",
        "    plt.subplot(8, 8, i+1) #\n",
        "    plt.imshow(first_layer[i, 0, :, :], cmap='seismic')\n",
        "    plt.axis('off')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5a6943e5",
      "metadata": {
        "id": "5a6943e5"
      },
      "outputs": [],
      "source": [
        "# After training:\n",
        "model = models.resnet50(pretrained=True)\n",
        "first_layer = model.conv1.weight.data\n",
        "\n",
        "print(first_layer.shape)\n",
        "\n",
        "plt.figure(figsize=(15, 15))\n",
        "for i in range(first_layer.shape[0]):\n",
        "    plt.subplot(8, 8, i+1) #\n",
        "    plt.imshow(first_layer[i, 0, :, :], cmap='seismic')\n",
        "    plt.axis('off')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "46bddbb6",
      "metadata": {
        "id": "46bddbb6"
      },
      "source": [
        "Among these kernels, we find contour extractors similar to the Prewitt filter.  \n",
        "Signal processing enthusiasts will even recognize patterns very close to the [Morlet wavelets](https://www.google.com/search?q=wavelet+morlet+2d&rlz=1C1AVFC_enFR826FR857&hl=fr&sxsrf=ALeKk01sWHdzUO6bRogEv0KFx2gRgOWz_Q:1610077971021&source=lnms&tbm=isch&sa=X&ved=2ahUKEwiIq5fst4vuAhXPxYUKHaskDqAQ_AUoAXoECAUQAw&biw=915&bih=591).  \n",
        "This is remarkable: through a simple gradient descent, effective filters for shape recognition have emerged.\n",
        "\n",
        "**Exercise 3:** How many \"neurons\" does the first convolutional layer of a ResNet50 have (one kernel per neuron)?  \n",
        "How many weights are there in a kernel?  \n",
        "How many weights does this layer contain?"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "There are 64 channels at the output of the layer, and thus 64 neurons.\n",
        "\n",
        "A kernel is a tensor of order three (one dimension for the channels of the input tensor, two spatial dimensions).\n",
        "As there is no bias, there are 3 x 7 x 7 = 147 parameters\n",
        "\n",
        "There are therefore 64 x 147 = 9408 weights in total in this layer"
      ],
      "metadata": {
        "id": "bazc0HsbuQmM"
      },
      "id": "bazc0HsbuQmM"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3ec98dfb",
      "metadata": {
        "id": "3ec98dfb"
      },
      "outputs": [],
      "source": [
        "# This can be verified with the module.parameters() generator,\n",
        "# applicable to any instance of torch.nn\n",
        "nb_weights = 0\n",
        "for parameter in model.conv1.parameters():\n",
        "  # numel: counts the number of elements in a tensor\n",
        "  nb_weights += torch.numel(parameter)\n",
        "\n",
        "print(nb_weights)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2770555e",
      "metadata": {
        "id": "2770555e"
      },
      "source": [
        "**C.** Feature Maps\n",
        "\n",
        "Now let's see what happens to the image as it passes through the network. Let's take VGG16, pass it on our cat image, and see how the signal changes as it propagates through the network. First, let's see if the network recognizes the correct class among the thousand classes in the dataset. The list of classes is [here](https://gist.github.com/yrevar/942d3a0ac09ec9e5eb3a)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9ac79949",
      "metadata": {
        "id": "9ac79949"
      },
      "outputs": [],
      "source": [
        "image = Image.open(path).convert(\"RGB\")\n",
        "image = image.resize((256,256))\n",
        "\n",
        "img = transforms.ToTensor()(image)\n",
        "\n",
        "img = transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])(img)\n",
        "img = img.unsqueeze(dim=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ee2c7fdc",
      "metadata": {
        "id": "ee2c7fdc"
      },
      "outputs": [],
      "source": [
        "model = models.vgg16(pretrained=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9c5c62e4",
      "metadata": {
        "id": "9c5c62e4"
      },
      "outputs": [],
      "source": [
        "# Raw output\n",
        "output = model(img)\n",
        "\n",
        "# Softmax function\n",
        "output = output.softmax(dim=1).detach()\n",
        "\n",
        "# Prediction\n",
        "_, c  =torch.max(output, dim=1)\n",
        "print(c)\n",
        "\n",
        "# \"Probabilities\" associated with classes\n",
        "plt.plot(output.squeeze())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "16a485a6",
      "metadata": {
        "id": "16a485a6"
      },
      "source": [
        "You may notice that by running the last cell several times, the output probabilities differ even though the input does not change. This is due to \"dropout\" (check `print(model)`), which deactivates a random subset of the classifier neurons (this operation helps combat **overfitting**).  \n",
        "\n",
        "To disable \"dropout\" and freeze the network, switch to \"eval\" mode with the command:  \n",
        "\n",
        "*model.eval()*"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "261fd242",
      "metadata": {
        "id": "261fd242"
      },
      "source": [
        "Now, let's visualize the signal at the output of a convolutional layer. For this, we can use the [*hook*](https://pytorch.org/tutorials/beginner/former_torchies/nnft_tutorial.html) command. The channels of these intermediate signals are called **feature maps**. Let's see the feature maps associated with the first convolutional layer:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0caff8f9",
      "metadata": {
        "id": "0caff8f9"
      },
      "outputs": [],
      "source": [
        "z = []\n",
        "# Function that stores in the list z the \"feature maps\"\n",
        "def store_layer_output(model, input, output):\n",
        "        z.append(output.detach())\n",
        "\n",
        "\n",
        "model.features[0].register_forward_hook(store_layer_output)\n",
        "model.features[10].register_forward_hook(store_layer_output)\n",
        "model.features[17].register_forward_hook(store_layer_output)\n",
        "model.features[28].register_forward_hook(store_layer_output)\n",
        "\n",
        "output = model(img)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "19e531fe",
      "metadata": {
        "id": "19e531fe"
      },
      "outputs": [],
      "source": [
        "for fm in z:\n",
        "  print(fm.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8c3ec5bb",
      "metadata": {
        "id": "8c3ec5bb"
      },
      "source": [
        "**Exercise 4:** Visualize the feature maps of layers 0, 10, 17, 28. You will notice that at level 10, the neuron's response is often specific to a particular trait."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e125627f",
      "metadata": {
        "deletable": false,
        "id": "e125627f",
        "nbgrader": {
          "cell_type": "code",
          "checksum": "5675a175932ad90e49d4ba4f7c9b56fb",
          "grade": false,
          "grade_id": "exercise-4-1",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "tags": []
      },
      "outputs": [],
      "source": [
        "feature_maps = z[1]\n",
        "\n",
        "print(feature_maps.shape)\n",
        "plt.figure(figsize=(20, 17))\n",
        "# for i in range(...):\n",
        "#    plt.subplot(...)\n",
        "#    plt.imshow(...)\n",
        "#    plt.axis('off') # don't show plot axis\n",
        "# YOUR CODE HERE\n",
        "raise NotImplementedError()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "feature_maps = z[1]\n",
        "\n",
        "print(feature_maps.shape)\n",
        "plt.figure(figsize=(20, 17))\n",
        "# for i in range(...):\n",
        "#    plt.subplot(...)\n",
        "#    plt.imshow(...)\n",
        "#    plt.axis('off') # don't show plot axis\n",
        "### BEGIN SOLUTION\n",
        "for i in range(64):\n",
        "    plt.subplot(8, 8, i+1) #\n",
        "    plt.imshow(feature_maps[0, i, :, :], cmap='seismic')\n",
        "    plt.axis('off')\n",
        "### END SOLUTION\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "Q_J5-VM1um9j"
      },
      "id": "Q_J5-VM1um9j",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "e4086e2c",
      "metadata": {
        "id": "e4086e2c"
      },
      "source": [
        "Overlay on the original image:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4c4919dd",
      "metadata": {
        "deletable": false,
        "id": "4c4919dd",
        "nbgrader": {
          "cell_type": "code",
          "checksum": "97b755004638909c0d2544ef8f8ad43a",
          "grade": false,
          "grade_id": "exercise-4-2",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "tags": []
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "# load image\n",
        "# YOUR CODE HERE\n",
        "raise NotImplementedError()\n",
        "\n",
        "f, axes = plt.subplots(8,8)\n",
        "for i in range(64):\n",
        "    ax = axes[i//8][i%8]\n",
        "    # fm1 = feature map\n",
        "    # YOUR CODE HERE\n",
        "    raise NotImplementedError()\n",
        "    fm1 = cv2.resize(fm1, (image.size))\n",
        "    fm1 /= 0.5*np.max(fm1)\n",
        "    fm1 = np.uint8(fm1*255)\n",
        "    fm1 = cv2.applyColorMap(fm1,  cv2.COLORMAP_HOT)\n",
        "    fm1 = Image.fromarray(fm1)\n",
        "\n",
        "    superp = Image.blend(image, fm1, 0.5)\n",
        "    ax.axis('off')\n",
        "    ax.imshow(superp)\n",
        "f.set_figheight(20)\n",
        "f.set_figwidth(20)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "# load image\n",
        "### BEGIN SOLUTION\n",
        "image = Image.open(path).convert(\"RGB\")\n",
        "image = image.resize((256,256))\n",
        "### END SOLUTION\n",
        "\n",
        "f, axes = plt.subplots(8,8)\n",
        "for i in range(64):\n",
        "    ax = axes[i//8][i%8]\n",
        "    # fm1 = feature map\n",
        "    ### BEGIN SOLUTION\n",
        "    fm1 = feature_maps[0, i, :, :].numpy()\n",
        "    ### END SOLUTION\n",
        "    fm1 = cv2.resize(fm1, (image.size))\n",
        "    fm1 /= 0.5*np.max(fm1)\n",
        "    fm1 = np.uint8(fm1*255)\n",
        "    fm1 = cv2.applyColorMap(fm1,  cv2.COLORMAP_HOT)\n",
        "    fm1 = Image.fromarray(fm1)\n",
        "\n",
        "    superp = Image.blend(image, fm1, 0.5)\n",
        "    ax.axis('off')\n",
        "    ax.imshow(superp)\n",
        "f.set_figheight(20)\n",
        "f.set_figwidth(20)"
      ],
      "metadata": {
        "id": "9epJxHfaurk8"
      },
      "id": "9epJxHfaurk8",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}