{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MSgiKnAs1BbN"
      },
      "source": [
        "# Practical session n°7 : Learning to Rank\n",
        "\n",
        "Notions:\n",
        "- learning to order things\n",
        "- ranking function (or ranker)\n",
        "- curriculum learning\n",
        "- rank correlation(s)\n",
        "- hinge loss and ranknet loss\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "Duration: 1 h 30"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P1waEt7NJgSx"
      },
      "source": [
        "Several problems fall under the label \"[learning-to-rank](https://link.springer.com/content/pdf/10.1007/978-3-642-15880-3_20.pdf).\" One example is sorting a given list of labels (e.g. URL of websites) by relevance to an input query. This problem is referred to as *label ranking*.\\\n",
        " In another scenario, input objects (e.g. images) should be sorted wrt a given criterion (e.g. 'foggy' or 'snowy'). Terms like *object ranking* and *learning to order things* are often used to describe this situation. In both cases, the learning is based on sorted samples, such as pairs of ordered images.\n",
        "\n",
        "In this practical session, we illustrate the latter scenario using very simple synthetic images. All images consist of a mixture of a disc and a variable number of rectangles of different shapes. The goal is to sort the images based on the pixel intensity on the disc. To achieve this, we work in a standard context where we have pairs of ordered images. Using these pairs, we will train a neural network to construct a real-valued \"ranking function\" (*ranker*) whose outputs enable the sorting of images."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qtq1hUDdoSDC"
      },
      "source": [
        "**Exercise 1:** Problem Construction\n",
        "\n",
        "The following cells enable you to:\n",
        "- generate a dataset on colab (train, val and test),\n",
        "- define a dataset that provides pairs of images and a comparison based on the criterion of disc intensity (\"0\" if the disc is more intense in the first image, \"1\" otherwise),\n",
        "- visualize an initial batch."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from os.path import join\n",
        "ls = lambda rep: sorted(os.listdir(rep))\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.optim import lr_scheduler\n",
        "import numpy as np\n",
        "import torchvision\n",
        "from torch.utils.data import Dataset, DataLoader, sampler\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import copy\n",
        "from random import randint, choice"
      ],
      "metadata": {
        "id": "Nv8w-nP22XBa"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! git clone https://github.com/nanopiero/ML_S5_etudiants\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DygJhqc1hRY7",
        "outputId": "e4ec5c9c-1bb1-43cb-b626-561109f335f2"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'ML_S5_etudiants'...\n",
            "remote: Enumerating objects: 165, done.\u001b[K\n",
            "remote: Counting objects: 100% (44/44), done.\u001b[K\n",
            "remote: Compressing objects: 100% (37/37), done.\u001b[K\n",
            "remote: Total 165 (delta 20), reused 16 (delta 7), pack-reused 121 (from 1)\u001b[K\n",
            "Receiving objects: 100% (165/165), 3.31 MiB | 21.43 MiB/s, done.\n",
            "Resolving deltas: 100% (53/53), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! cp ML_S5_etudiants/utils_P7.py .\n",
        "from utils_P7 import *\n",
        "root = r\"/content\""
      ],
      "metadata": {
        "id": "i3L0knfj0MDU"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "metadata": {
        "id": "5ubPaW6jE3n-"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "DM4QiDC95gak"
      },
      "outputs": [],
      "source": [
        "dir_trainval = join(root, r\"train\")\n",
        "generate_dataset(dir_trainval, size_dataset=10000)\n",
        "\n",
        "dir_test = join(root, r\"test\")\n",
        "generate_dataset(dir_test, size_dataset=2000)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Retrieving target values\n",
        "\n",
        "# Image paths:\n",
        "dir_images_trainval = os.path.join(dir_trainval, 'images')\n",
        "dir_images_test = os.path.join(dir_test, 'images')\n",
        "\n",
        "# Target values for train+val\n",
        "label_dict_path_trainval = os.path.join(dir_trainval, 'labels_synthese.pickle')\n",
        "with open(label_dict_path_trainval, 'rb') as handle:\n",
        "    label_dict_trainval = pickle.load(handle)\n",
        "\n",
        "# Target values for test\n",
        "label_dict_path_test = os.path.join(dir_test, 'labels_synthese.pickle')\n",
        "with open(label_dict_path_test, 'rb') as handle:\n",
        "    label_dict_test = pickle.load(handle)\n",
        "\n",
        "# Splitting train / val (8000/2000)\n",
        "all_image_names = np.array(ls(dir_images_trainval))\n",
        "\n",
        "train_indices = list(range(0, 8000))\n",
        "names_train = all_image_names[train_indices]\n",
        "val_indices = list(range(8000, 10000))\n",
        "names_val = all_image_names[val_indices]\n",
        "names_test = ls(dir_images_test)\n",
        "\n",
        "# Note: for random splitting, use sklearn.model_selection.train_test_split as tts"
      ],
      "metadata": {
        "id": "v04NNmhgBbY6"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0pC2CQnsDREe"
      },
      "outputs": [],
      "source": [
        "# if error:\n",
        "# from shutil import rmtree\n",
        "# rmtree(dir_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "fv4HHS6Y67lE"
      },
      "outputs": [],
      "source": [
        "# Data Augmentation\n",
        "class SuperFlip(object):\n",
        "    \"\"\"\n",
        "    The 8 transformations\n",
        "    generated by R(Pi/2) and vertical symmetry/axis\n",
        "    \"\"\"\n",
        "    def __init__(self, num_transforms):\n",
        "        self.num_transforms = num_transforms\n",
        "\n",
        "    def __call__(self, image):\n",
        "        # Note: Ideally, torch.randint should be used here...\n",
        "        n = randint(0, self.num_transforms)\n",
        "        if n == 1:\n",
        "            image = image.flip([1])\n",
        "        elif n == 2:\n",
        "            image = image.flip([2])\n",
        "        elif n == 3:\n",
        "            image = image.transpose(1, 2)\n",
        "        elif n == 4:\n",
        "            image = image.transpose(1, 2).flip([1])\n",
        "        elif n == 5:\n",
        "            image = image.transpose(1, 2).flip([2])\n",
        "        elif n == 6:\n",
        "            image = image.flip([1, 2])\n",
        "        elif n == 7:\n",
        "            image = image.transpose(1, 2).flip([1, 2])\n",
        "        return image\n",
        "\n",
        "super_flip_transform = SuperFlip(8)\n",
        "\n",
        "transforms = {\n",
        "    'train': super_flip_transform,\n",
        "    'val': None,\n",
        "    'test': None\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "E7Qf6WY36-Fo"
      },
      "outputs": [],
      "source": [
        "# Dataset Construction:\n",
        "def oracle(name0, name1, data_dict):\n",
        "    # Load the data:\n",
        "    y0 = data_dict[name0]['y']\n",
        "    y1 = data_dict[name1]['y']\n",
        "\n",
        "    # Determine the comparison:\n",
        "    compa = 0 if y1 < y0 else 1\n",
        "    return compa\n",
        "\n",
        "class DatasetOrderedPairs(torch.utils.data.Dataset):\n",
        "    def __init__(self, images_dir, data_dict, transform=None):\n",
        "        self.images_dir = images_dir\n",
        "        self.transform = transform\n",
        "        self.imgs = ls(images_dir)\n",
        "        self.data_dict = data_dict\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        name0 = self.imgs[idx]\n",
        "        name1 = choice(self.imgs)\n",
        "        label = oracle(name0, name1, self.data_dict)\n",
        "\n",
        "        # Get the images\n",
        "        path0 = os.path.join(self.images_dir, name0)\n",
        "        img0 = torch.load(path0, weights_only=True)\n",
        "        path1 = os.path.join(self.images_dir, name1)\n",
        "        img1 = torch.load(path1, weights_only=True)\n",
        "\n",
        "        if self.transform is not None:\n",
        "            img0 = self.transform(img0)\n",
        "            img1 = self.transform(img1)\n",
        "\n",
        "        return img0, img1, torch.from_numpy(np.array(label)).long(), name0, name1   # -1 if no class 0\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.imgs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "GI1DH45t7AcN"
      },
      "outputs": [],
      "source": [
        "# Dataset instantiation:\n",
        "dataset_train = DatasetOrderedPairs(dir_images_trainval, label_dict_trainval, transforms['train'])\n",
        "dataset_val = DatasetOrderedPairs(dir_images_trainval, label_dict_trainval, transforms['val'])\n",
        "dataset_test = DatasetOrderedPairs(dir_images_test, label_dict_test, transforms['test'])\n",
        "\n",
        "datasets = {'train': dataset_train, 'val': dataset_val, 'test': dataset_test}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "FiM4F3gaAzw3"
      },
      "outputs": [],
      "source": [
        "# Samplers and loaders\n",
        "train_sampler = torch.utils.data.sampler.SubsetRandomSampler(train_indices)\n",
        "val_sampler = torch.utils.data.sampler.SubsetRandomSampler(val_indices)\n",
        "\n",
        "samplers = {'train': train_sampler, 'val': val_sampler}\n",
        "\n",
        "batch_size = 64\n",
        "\n",
        "dataloaders = {x: torch.utils.data.DataLoader(datasets[x], batch_size=batch_size, shuffle=False, sampler=samplers[x], num_workers=2) for x in ['train', 'val']}\n",
        "dataloaders['test'] = torch.utils.data.DataLoader(datasets['test'], batch_size=batch_size, shuffle=False, num_workers=0)\n",
        "dataset_sizes = {'train': len(names_train), 'val': len(names_val), 'test': len(names_test)}\n",
        "\n",
        "dataloaders['viz'] = torch.utils.data.DataLoader(datasets['train'], batch_size=6, shuffle=False, num_workers=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A0Qib3bU7EUO"
      },
      "outputs": [],
      "source": [
        "# Visualisation\n",
        "\n",
        "img1, img2, labels, _, _ = next(iter(dataloaders['viz']))\n",
        "\n",
        "fig0 = plt.figure(0, figsize=(15, 3))\n",
        "voir_batch2D(img1, nx = 8, fig = fig0, k=0, min_scale=0,max_scale=10)\n",
        "fig1 = plt.figure(1, figsize=(15, 3))\n",
        "voir_batch2D(img2, nx = 8, fig = fig1, k=0, min_scale=0,max_scale=10)\n",
        "\n",
        "print(labels)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q0** How is done the separation between training and validation done here?"
      ],
      "metadata": {
        "id": "iycy2im3U8wg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using specific samplers."
      ],
      "metadata": {
        "id": "KaY8x8YOVICz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q1** What is the role of *super_flip*? And that of the *oracle* function?"
      ],
      "metadata": {
        "id": "3QR_3fcbEBJg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- *super_flip* implements eight very simple plan transformations: all those generated by a rotation of angle $\\pi/2$ and symmetry about the vertical axis.\n",
        "- *oracle* uses data that is not accessible in a real-world scenario: the numerical values of the feature. It compares these values to provide the result of the comparison."
      ],
      "metadata": {
        "id": "0uYq3R0zE0kb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q2** Are all pairs of images equally easy to order?"
      ],
      "metadata": {
        "id": "Ln7gZSUGEnmU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ordering becomes challenging when one of the discs is obscured or noisy, or when the two values are close."
      ],
      "metadata": {
        "id": "DTpPTeZUEK2j"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Exercise 2:** Siamese Learning\n",
        "\n",
        "During training, batches of image pairs are compared. Basic siamese network training involves passing each image in the pair independently through the model and penalizing the model when the outputs are arranged in the wrong order.\n",
        "\n",
        "The simplest way to do this is to consider the positive part of the difference between the outputs. This is what the following cost function does:"
      ],
      "metadata": {
        "id": "hFv2OfMMF1mF"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "eYZtrZy07G-r"
      },
      "outputs": [],
      "source": [
        "# Hinge Loss function\n",
        "\n",
        "def label_to_sgn(label):  # 0 -> 1  and 1 -> -1\n",
        "    sgn = torch.where(label == 0, 1, -1)\n",
        "    return sgn\n",
        "\n",
        "class HingeLoss(torch.nn.Module):\n",
        "    def __init__(self, margin=0.1):\n",
        "        super(HingeLoss, self).__init__()\n",
        "        self.margin = margin\n",
        "\n",
        "    def forward(self, output0, output1, label):\n",
        "        sgn = label_to_sgn(label)\n",
        "        diff = sgn * (output1 - output0)\n",
        "\n",
        "        loss = torch.relu(diff + self.margin).mean()\n",
        "        return loss"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q1** Write the training loop and run it for 20 epochs. Keep track of the successive accuracies."
      ],
      "metadata": {
        "id": "YmVCCdfxUN86"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "04USzeWb7I8m"
      },
      "outputs": [],
      "source": [
        "num_epochs = 20\n",
        "channels = 1\n",
        "\n",
        "# With a ResNet18\n",
        "from torchvision.models import resnet18\n",
        "model = resnet18(num_classes=1)\n",
        "print(model.inplanes)\n",
        "model.conv1 =  nn.Conv2d(channels, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
        "nn.init.kaiming_normal_( model.conv1.weight, mode=\"fan_out\", nonlinearity=\"relu\")\n",
        "\n",
        "\n",
        "device = torch.device(\"cuda:0\")\n",
        "model = model.to(device)\n",
        "\n",
        "\n",
        "criterion = HingeLoss(0.1)\n",
        "optimizer = optim.Adam(model.parameters(), lr = 0.002 )\n",
        "\n",
        "\n",
        "# Loss function and optimizer\n",
        "criterion = HingeLoss(margin=0.1)\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i1wHjBb77Ly5"
      },
      "outputs": [],
      "source": [
        "from tqdm import tqdm, notebook\n",
        "\n",
        "\n",
        "train_accs = []\n",
        "val_accs = []\n",
        "\n",
        "phases = ['train', 'val']\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    print(f'Epoch {epoch+1}/{num_epochs}')\n",
        "    print('-' * 10)\n",
        "\n",
        "    # Each epoch has a training and validation phase\n",
        "    for phase in phases:\n",
        "        if phase == 'train':\n",
        "            model.train()  # Set model to training mode\n",
        "        else:\n",
        "            model.eval()   # Set model to evaluate mode\n",
        "\n",
        "        running_loss = 0.0\n",
        "        running_corrects = 0\n",
        "        data_loader = notebook.tqdm(dataloaders[phase], desc=f'{phase.capitalize()} Epoch {epoch}', leave= phase == 'train')\n",
        "        # Iterate over data.\n",
        "        for img1, img2, labels, _, _ in data_loader:\n",
        "            img1 = img1.to(device)\n",
        "            img2 = img2.to(device)\n",
        "            labels = labels.to(device).detach()\n",
        "\n",
        "            # zero the parameter gradients\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # forward\n",
        "            # track history if only in train\n",
        "            with torch.set_grad_enabled(phase == 'train'):\n",
        "                output1 = model(img1)\n",
        "                output2 = model(img2)\n",
        "                _, preds = torch.max(torch.cat((output1, output2), dim=1), 1)\n",
        "\n",
        "                loss = criterion(output1, output2, labels.to(torch.float32).unsqueeze(dim=1))\n",
        "\n",
        "                # backward + optimize only if in training phase\n",
        "                if phase == 'train':\n",
        "                    loss.backward()\n",
        "                    optimizer.step()\n",
        "\n",
        "            # statistics\n",
        "            running_loss += loss.item() * img1.size(0)\n",
        "            running_corrects += torch.sum(preds == labels.data).double().item()\n",
        "\n",
        "            data_loader.set_postfix(loss=f'{loss.item():.4f}', refresh=False)\n",
        "\n",
        "        epoch_loss = running_loss / dataset_sizes[phase]\n",
        "        epoch_acc = running_corrects / dataset_sizes[phase]\n",
        "\n",
        "        print(f'{phase} Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}')\n",
        "\n",
        "        if phase == 'train':\n",
        "            train_accs.append(epoch_acc)\n",
        "\n",
        "        if phase == 'val':\n",
        "            val_accs.append(epoch_acc)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q3** Check the learning curve."
      ],
      "metadata": {
        "id": "HU951Grd1tWR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(range(num_epochs), val_accs)\n",
        "plt.title('accuracy (validation set)')"
      ],
      "metadata": {
        "id": "BX89CrFa3GoS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q4** Interprete the following scatterplot:"
      ],
      "metadata": {
        "id": "MPhXKjr7VW0a"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jSL2b6Ro7P_l"
      },
      "outputs": [],
      "source": [
        "# true (mean) pixel intensity on the disk:\n",
        "ys = []\n",
        "# outputs of the network:\n",
        "yhats = []\n",
        "\n",
        "# browse the test set:\n",
        "for name in names_test:\n",
        "    # get true mean intensity on the disk\n",
        "    y = label_dict_test[name]['y']\n",
        "    ys.append(y)\n",
        "\n",
        "    path = join(dir_images_test, name)\n",
        "    image = torch.load(path)\n",
        "    image = image.cuda().unsqueeze(dim=0)\n",
        "\n",
        "    # get model output\n",
        "    yhat = model.eval()(image)\n",
        "    yhat = yhat.item()\n",
        "    yhats.append(yhat)\n",
        "\n",
        "# make np array\n",
        "ys = np.array(ys)\n",
        "yhats = np.array(yhats)\n",
        "\n",
        "# scatterplot\n",
        "plt.figure(num=10)\n",
        "plt.scatter(ys, yhats, marker='+')\n",
        "plt.xlabel('y')\n",
        "plt.ylabel('$\\hat{y}$')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q5** Calculate the Spearman and Kendall rank correlations. Which of the two is related to the accuracy measure?\n"
      ],
      "metadata": {
        "id": "RIiUvbAi1WYq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Compute correlation ranks\n",
        "from scipy.stats import spearmanr, kendalltau\n",
        "\n",
        "spearman, ps = spearmanr(ys, yhats)\n",
        "kendall, pk = kendalltau(ys, yhats)\n",
        "\n",
        "print(spearman)\n",
        "print(kendall)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vDzdt7gd1Z9Z",
        "outputId": "8ca34b30-51d4-41ef-a838-9aef5b0f9a95"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.9383585405896352\n",
            "0.7959699849924962\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mZpLh5fFMzmq"
      },
      "source": [
        "**Q6** How would you improve performance ?"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Curriculum Learning:\n",
        "- Seek more \"challenging\" pairs towards the end of training.\n",
        "\n",
        "Optimization:\n",
        "- Consider adding a learning rate scheduler.\n",
        "\n",
        "Other Loss Functions:\n",
        "- Explore RankNet loss.\n",
        "- Explore Listnet Loss."
      ],
      "metadata": {
        "id": "jw6CdRxcV0AC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Exercice 3** Curriculum Learning"
      ],
      "metadata": {
        "id": "mokDJcGBncXF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Les cinq cellules suivantes ont été utilisées pour générer un checkpoint. Elles ne font pas vraiment partie de la correction."
      ],
      "metadata": {
        "id": "d_OJyVbcU5WM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "num_epochs = 50\n",
        "channels = 1\n",
        "\n",
        "# With a ResNet18\n",
        "from torchvision.models import resnet18\n",
        "model = resnet18(num_classes=1)\n",
        "print(model.inplanes)\n",
        "model.conv1 =  nn.Conv2d(channels, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
        "nn.init.kaiming_normal_( model.conv1.weight, mode=\"fan_out\", nonlinearity=\"relu\")\n",
        "\n",
        "\n",
        "device = torch.device(\"cuda:0\")\n",
        "model = model.to(device)\n",
        "\n",
        "\n",
        "criterion = HingeLoss(0.1)\n",
        "optimizer = optim.Adam(model.parameters(), lr = 0.002 )\n",
        "\n",
        "\n",
        "# Loss function and optimizer\n",
        "criterion = HingeLoss(margin=0.1)\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sOGe4tBay0EZ",
        "outputId": "47605341-2b48-416c-8137-512fd20a25cd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "512\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm, notebook\n",
        "\n",
        "best_model_wts = copy.deepcopy(model.state_dict())\n",
        "best_acc = 0.0\n",
        "train_accs = []\n",
        "val_accs = []\n",
        "\n",
        "phases = ['train', 'val']\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    print(f'Epoch {epoch+1}/{num_epochs}')\n",
        "    print('-' * 10)\n",
        "\n",
        "    # Each epoch has a training and validation phase\n",
        "    for phase in phases:\n",
        "        if phase == 'train':\n",
        "            model.train()  # Set model to training mode\n",
        "        else:\n",
        "            model.eval()   # Set model to evaluate mode\n",
        "\n",
        "        running_loss = 0.0\n",
        "        running_corrects = 0\n",
        "        data_loader = notebook.tqdm(dataloaders[phase], desc=f'{phase.capitalize()} Epoch {epoch}', leave= phase == 'train')\n",
        "        # Iterate over data.\n",
        "        for img1, img2, labels, _, _ in data_loader:\n",
        "            img1 = img1.to(device)\n",
        "            img2 = img2.to(device)\n",
        "            labels = labels.to(device).detach()\n",
        "\n",
        "            # zero the parameter gradients\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # forward\n",
        "            # track history if only in train\n",
        "            with torch.set_grad_enabled(phase == 'train'):\n",
        "                output1 = model(img1)\n",
        "                output2 = model(img2)\n",
        "                _, preds = torch.max(torch.cat((output1, output2), dim=1), 1)\n",
        "\n",
        "                loss = criterion(output1, output2, labels.to(torch.float32).unsqueeze(dim=1))\n",
        "\n",
        "                # backward + optimize only if in training phase\n",
        "                if phase == 'train':\n",
        "                    loss.backward()\n",
        "                    optimizer.step()\n",
        "\n",
        "            # statistics\n",
        "            running_loss += loss.item() * img1.size(0)\n",
        "            running_corrects += torch.sum(preds == labels.data).double().item()\n",
        "\n",
        "            data_loader.set_postfix(loss=f'{loss.item():.4f}', refresh=False)\n",
        "\n",
        "        epoch_loss = running_loss / dataset_sizes[phase]\n",
        "        epoch_acc = running_corrects / dataset_sizes[phase]\n",
        "\n",
        "        print(f'{phase} Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}')\n",
        "\n",
        "        if phase == 'train':\n",
        "            train_accs.append(epoch_acc)\n",
        "\n",
        "        if phase == 'val':\n",
        "            val_accs.append(epoch_acc)\n",
        "\n",
        "        # Deep copy the model\n",
        "        if phase == 'val' and epoch_acc > best_acc:\n",
        "            best_acc = epoch_acc\n",
        "            best_model_wts = copy.deepcopy(model.state_dict())"
      ],
      "metadata": {
        "id": "UG6Bz7ev6fcT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "import os\n",
        "drive.mount('/content/drive')\n"
      ],
      "metadata": {
        "id": "FrbUMdqa9gir"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "os.chdir('/content/drive/MyDrive/TP_ENM_2324/TP_DEV/TP5_LTR_&_ReID')"
      ],
      "metadata": {
        "id": "iAkC0ZAh957t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "checkpoint = {\n",
        "    'val_accs': val_accs,\n",
        "    'model': model.state_dict(),\n",
        "    'optimizer': optimizer.state_dict()}\n",
        "model_name = 'resnet18_50ep.checkpoint'\n",
        "PATH = join('./' + model_name)\n",
        "torch.save(checkpoint, PATH)"
      ],
      "metadata": {
        "id": "xs0RtBGc9Ig1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "One idea is to make the problem harder after an initial training phase. The following code allows you to retrieve a ResNet18 trained for 50 epochs, the optimizer, and the associated learning curve:"
      ],
      "metadata": {
        "id": "ZouHyg51niH8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install huggingface_hub\n",
        "from huggingface_hub import hf_hub_download\n",
        "model_hf_name = \"resnet18_50ep.checkpoint\"\n",
        "PATH_checkpoint = hf_hub_download(repo_id=\"nanopiero/models_ML_S5_P7\", filename=model_hf_name)"
      ],
      "metadata": {
        "id": "f9gMzRyBD-Ln"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q1** Continue training for an additional 20 epochs without changing the dataloader. Remember to store accuracies related to the validation set and outputs related to the test set."
      ],
      "metadata": {
        "id": "9cbFjkshyhB4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataloaders = {x: torch.utils.data.DataLoader(datasets[x], batch_size=batch_size, shuffle=False, sampler=samplers[x], num_workers=2) for x in ['train', 'val']}"
      ],
      "metadata": {
        "id": "48mbnbqsROjM"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = resnet18(num_classes=1)\n",
        "model.conv1 =  nn.Conv2d(channels, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
        "device = torch.device(\"cuda:0\")\n",
        "model = model.to(device)\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "checkpoint = torch.load(PATH_checkpoint, weights_only=True)\n",
        "print(checkpoint.keys())\n",
        "\n",
        "model.load_state_dict(checkpoint['model'])\n",
        "optimizer.load_state_dict(checkpoint['optimizer'])\n",
        "val_accs_control = checkpoint['val_accs']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U-vJZUGLEf3G",
        "outputId": "b1005acd-97b2-467f-f147-0ea124fd25f2"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dict_keys(['val_accs', 'model', 'optimizer'])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# To save the best control model\n",
        "best_control_model_wts = copy.deepcopy(model.state_dict())\n",
        "best_acc = 0.0\n",
        "\n",
        "train_accs = []\n",
        "num_epochs = 20\n",
        "\n",
        "phases = ['train', 'val']\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    print(f'Epoch {epoch+1}/{num_epochs}')\n",
        "    print('-' * 10)\n",
        "\n",
        "    # Each epoch has a training and validation phase\n",
        "    for phase in phases:\n",
        "        if phase == 'train':\n",
        "            model.train()  # Set model to training mode\n",
        "        else:\n",
        "            model.eval()   # Set model to evaluate mode\n",
        "\n",
        "        running_loss = 0.0\n",
        "        running_corrects = 0\n",
        "        data_loader = notebook.tqdm(dataloaders[phase], desc=f'{phase.capitalize()} Epoch {epoch}', leave= phase == 'train')\n",
        "        # Iterate over data.\n",
        "        for img1, img2, labels, _, _ in data_loader:\n",
        "            img1 = img1.to(device)\n",
        "            img2 = img2.to(device)\n",
        "            labels = labels.to(device).detach()\n",
        "\n",
        "            # zero the parameter gradients\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # forward\n",
        "            # track history if only in train\n",
        "            with torch.set_grad_enabled(phase == 'train'):\n",
        "                output1 = model(img1)\n",
        "                output2 = model(img2)\n",
        "                _, preds = torch.max(torch.cat((output1, output2), dim=1), 1)\n",
        "\n",
        "                loss = criterion(output1, output2, labels.to(torch.float32).unsqueeze(dim=1))\n",
        "\n",
        "                # backward + optimize only if in training phase\n",
        "                if phase == 'train':\n",
        "                    loss.backward()\n",
        "                    optimizer.step()\n",
        "\n",
        "            # statistics\n",
        "            running_loss += loss.item() * img1.size(0)\n",
        "            running_corrects += torch.sum(preds == labels.data).double().item()\n",
        "\n",
        "            data_loader.set_postfix(loss=f'{loss.item():.4f}', refresh=False)\n",
        "\n",
        "        epoch_loss = running_loss / dataset_sizes[phase]\n",
        "        epoch_acc = running_corrects / dataset_sizes[phase]\n",
        "\n",
        "        print(f'{phase} Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}')\n",
        "\n",
        "\n",
        "        if phase == 'val':\n",
        "            val_accs_control.append(epoch_acc)\n",
        "\n",
        "        if phase == 'val' and epoch_acc > best_acc:\n",
        "            best_acc = epoch_acc\n",
        "            best_control_model_wts = copy.deepcopy(model.state_dict())"
      ],
      "metadata": {
        "id": "C8SVWL1jEXK2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.load_state_dict(best_control_model_wts)\n",
        "yhats_control = []\n",
        "\n",
        "# browse the test set:\n",
        "for name in names_test:\n",
        "    # get true mean intensity on the disk\n",
        "    y = label_dict_test[name]['y']\n",
        "\n",
        "    path = join(dir_images_test, name)\n",
        "    image = torch.load(path)\n",
        "    image = image.cuda().unsqueeze(dim=0)\n",
        "\n",
        "    # get model output\n",
        "    yhat = model.eval()(image)\n",
        "    yhat = yhat.item()\n",
        "    yhats_control.append(yhat)\n",
        "\n",
        "# make np array\n",
        "yhats_control = np.array(yhats_control)"
      ],
      "metadata": {
        "id": "OuKTXxRxW6SV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q2** We will now continue training on pairs that are harder to order. For this purpose, we have the 'Dataset_finer_pairs' dataset below. Train for 20 epochs using this dataset."
      ],
      "metadata": {
        "id": "GzxxFnHNF8_X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def make_dic_ray(imgs, dic, ray):\n",
        "\n",
        "  ys = np.array([dic[img]['y'] for img in imgs])\n",
        "  imgs = np.array(imgs)\n",
        "  dic_ray = {}\n",
        "  for i,img in enumerate(imgs):\n",
        "    y = ys[i]\n",
        "    # on limite la paire à des disques proches en intensité\n",
        "    dic_ray[img] = list(imgs[np.abs(ys - y) < ray])\n",
        "\n",
        "  return dic_ray\n",
        "\n",
        "class Dataset_finer_pairs(torch.utils.data.Dataset):\n",
        "    def __init__(self, images_dir,  dic, transfo = None, ray=0.5):\n",
        "        self.images_dir = images_dir\n",
        "        self.transfo = transfo\n",
        "        self.imgs = sorted(ls(images_dir))\n",
        "        self.dic = dic\n",
        "        self.dic_ray = make_dic_ray(self.imgs, dic, ray)\n",
        "\n",
        "    def __getitem__(self,idx):\n",
        "\n",
        "        name0 = self.imgs[idx]\n",
        "        name1 = choice(self.dic_ray[name0])\n",
        "        label = oracle(name0, name1, self.dic)\n",
        "\n",
        "\n",
        "        #get the images\n",
        "        path0 = os.path.join(self.images_dir, name0)\n",
        "        img0 =  torch.load(path0, weights_only=True)\n",
        "        path1 = os.path.join(self.images_dir, name1)\n",
        "        img1 = torch.load(path1, weights_only=True)\n",
        "\n",
        "\n",
        "        if self.transfo is not None:\n",
        "            img0 = self.transfo(img0)\n",
        "            img1 = self.transfo(img1)\n",
        "\n",
        "        return img0, img1,  torch.from_numpy(np.array(label)).long(), name0, name1\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.imgs)"
      ],
      "metadata": {
        "id": "lGG_CL2GF8JY"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Correction :"
      ],
      "metadata": {
        "id": "jallN2reP0Er"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "    epoch_ray = 1.\n",
        "    dataset_train = Dataset_finer_pairs(dir_images_trainval,\n",
        "                                        label_dict_trainval,\n",
        "                                        transforms['train'],\n",
        "                                        ray = epoch_ray)\n",
        "    dataloaders['train'] = torch.utils.data.DataLoader(dataset_train,\n",
        "                                                batch_size=batch_size,\n",
        "                                                shuffle=False,\n",
        "                                                sampler = samplers['train'],\n",
        "                                                num_workers=2)"
      ],
      "metadata": {
        "id": "UIU01fJAPxmj"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = resnet18(num_classes=1)\n",
        "model.conv1 =  nn.Conv2d(channels, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
        "device = torch.device(\"cuda:0\")\n",
        "model = model.to(device)\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "checkpoint = torch.load(PATH_checkpoint)\n",
        "\n",
        "model.load_state_dict(checkpoint['model'])\n",
        "optimizer.load_state_dict(checkpoint['optimizer'])\n",
        "val_accs_curriculum = checkpoint['val_accs']"
      ],
      "metadata": {
        "id": "uAU1RM6VJFWg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_epochs = 20\n",
        "train_accs = []\n",
        "\n",
        "phases = ['train', 'val']\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    print(f'Epoch {epoch+1}/{num_epochs}')\n",
        "    print('-' * 10)\n",
        "\n",
        "\n",
        "    # Each epoch has a training and validation phase\n",
        "    for phase in phases:\n",
        "        if phase == 'train':\n",
        "            model.train()  # Set model to training mode\n",
        "        else:\n",
        "            model.eval()   # Set model to evaluate mode\n",
        "\n",
        "        running_loss = 0.0\n",
        "        running_corrects = 0\n",
        "\n",
        "        # Iterate over data.\n",
        "        data_loader = notebook.tqdm(dataloaders[phase],\n",
        "                                    desc=f'{phase.capitalize()} Epoch {epoch}',\n",
        "                                    leave= phase == 'train')\n",
        "        for img1, img2, labels, img3 , _ in data_loader:\n",
        "            img1 = img1.to(device)\n",
        "            img2 = img2.to(device)\n",
        "            # if phase == 'train':\n",
        "            #   img3 = img3.to(device)\n",
        "            #print(inputs)\n",
        "            labels = labels.to(device).detach()\n",
        "\n",
        "            # zero the parameter gradients\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # forward\n",
        "            # track history if only in train\n",
        "            with torch.set_grad_enabled(phase == 'train'):\n",
        "                output1 = model(img1)\n",
        "                output2 = model(img2)\n",
        "\n",
        "                _, preds = torch.max(torch.cat((output1,output2), dim=1), 1)\n",
        "\n",
        "                loss =  criterion(output1,output2,labels.to(torch.float32).unsqueeze(dim=1))\n",
        "\n",
        "                # backward + optimize only if in training phase\n",
        "                if phase == 'train':\n",
        "                    loss.backward()\n",
        "                    optimizer.step()\n",
        "\n",
        "\n",
        "            # statistics\n",
        "            running_loss += loss.item() * img1.size(0)\n",
        "            running_corrects += torch.sum(preds == labels.data)\n",
        "\n",
        "            #del\n",
        "            del img1\n",
        "            del img2\n",
        "            del labels\n",
        "            del loss\n",
        "            del output1\n",
        "            del output2\n",
        "            torch.cuda.empty_cache()\n",
        "\n",
        "\n",
        "\n",
        "        epoch_loss = running_loss / dataset_sizes[phase]\n",
        "        epoch_acc = running_corrects.double().item() / dataset_sizes[phase]\n",
        "\n",
        "        print('{} Loss: {:.4f} Acc: {:.4f}'.format(\n",
        "            phase, epoch_loss, epoch_acc))\n",
        "\n",
        "        if phase == 'train':\n",
        "          train_accs.append(epoch_acc)\n",
        "\n",
        "        if phase == 'val':\n",
        "          val_accs_curriculum.append(epoch_acc)\n"
      ],
      "metadata": {
        "id": "P-kF2AGdIFyu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q3** Why is the training accuracy lower than before? Compare the learning curves (validation accuracies) and the results on the test set. Discuss."
      ],
      "metadata": {
        "id": "HcWzYRyzM1wb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(val_accs_control)\n",
        "plt.plot(val_accs_curriculum)\n",
        "plt.legend(['control', 'curriculum'])\n",
        "plt.ylim(0.85, 0.97)"
      ],
      "metadata": {
        "id": "JFrTalP5Ll1h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "yhats_curriculum = []\n",
        "\n",
        "# browse the test set:\n",
        "for name in names_test:\n",
        "    # get true mean intensity on the disk\n",
        "    y = label_dict_test[name]['y']\n",
        "\n",
        "    path = join(dir_images_test, name)\n",
        "    image = torch.load(path)\n",
        "    image = image.cuda().unsqueeze(dim=0)\n",
        "\n",
        "    # get model output\n",
        "    yhat = model.eval()(image)\n",
        "    yhat = yhat.item()\n",
        "    yhats_curriculum.append(yhat)\n",
        "\n",
        "# make np array\n",
        "yhats_curriculum = np.array(yhats_curriculum)"
      ],
      "metadata": {
        "id": "skjbMTI3Xmge",
        "outputId": "d7701be7-8a1f-4a6a-8e1d-10979fc236f9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-58-f2b1c07dcff6>:9: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  image = torch.load(path)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# scatterplot\n",
        "plt.figure(num=10)\n",
        "plt.scatter(ys, yhats_control, marker='+', color='blue')\n",
        "plt.scatter(ys, yhats_curriculum, marker='+', color='orange')\n",
        "plt.xlabel('y')\n",
        "plt.ylabel('outputs')\n",
        "plt.legend(['control', 'curriculum'])\n"
      ],
      "metadata": {
        "id": "BJT9d3czPhKj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "spearman, ps = spearmanr(ys, yhats_control)\n",
        "kendall, pk = kendalltau(ys, yhats_control)\n",
        "\n",
        "print(spearman)\n",
        "print(kendall)"
      ],
      "metadata": {
        "id": "TjmruSnYZ4lP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "spearman, ps = spearmanr(ys, yhats_curriculum)\n",
        "kendall, pk = kendalltau(ys, yhats_curriculum)\n",
        "\n",
        "print(spearman)\n",
        "print(kendall)"
      ],
      "metadata": {
        "id": "aQfYYuGKZQ-N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Exercise 4** RankNet Loss"
      ],
      "metadata": {
        "id": "5Dafc8NUV7Gn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A milder version of the Hinge Loss has been widely used, particularly in search engine learning, known as the RankNet Loss.\n",
        "\n",
        "This cost function is derived from a parametric probabilistic model, the [Bradley-Terry model](https://en.wikipedia.org/wiki/Bradley%E2%80%93Terry_model).\n",
        "\n",
        "In a general version, it is assumed that the outcome of a comparison (or match) between two objects \"0\" and \"1\" (or two teams) is random and depends on real values associated with the objects (the \"team levels\") as follows:\n",
        "\\begin{align}\n",
        "P_0 = \\dfrac{f(y_0)}{f(y_0) + f(y_1)}\n",
        "\\tag{1}\n",
        "\\end{align}\n",
        "Where $P_0$ is the probability of choosing object \"0\" (or the first team winning), and $f$ is a strictly increasing function with positive values."
      ],
      "metadata": {
        "id": "vrXFNbAtWqva"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q1** In the case where $f(y) = e^{\\sigma y}$, what do the choice probabilities depend on? Write the log-likelihood of the event \"object $x$ is chosen.\""
      ],
      "metadata": {
        "id": "HnGSr-f0ZYUe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dans le cas où $f(y) = e^{\\sigma y}$, les probabilités de choix dépendent uniquement de la différence $y_1 - y_0$. Lorsque des matchs sont joués, la valeur la plus vraisemblable de $y$ est interprétée comme le \"niveau\" d'un joueur (voir [classement Elo](https://fr.wikipedia.org/wiki/Classement_Elo))."
      ],
      "metadata": {
        "id": "mM5vHrlTB-Di"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q2** Derive an appropriate cost function for our ranking problem based on the given log-likelihood."
      ],
      "metadata": {
        "id": "06sul9qLaLnt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Under this model, the log-likelihood of the choice $c$ between two objects 0 and 1 is given by:\n",
        "$$ \\delta_{c=0} \\; \\ln \\left[ \\frac{e^{\\sigma y_0}}{e^{\\sigma y_0} + e^{\\sigma y_1}} \\right] + \\delta_{c=1} \\; \\ln \\left[ \\frac{e^{\\sigma y_1}}{e^{\\sigma y_0} + e^{\\sigma y_1}} \\right] $$\n",
        "\n",
        "To constrain the network to order objects based on whether they are chosen or not by the annotator on a given criterion, one can seek to maximize the log-likelihood, which is equivalent to minimizing the following cost function (equality left as an exercise):\n",
        "\n",
        "$$ \\mathcal{L}(y_0, y_1, c \\; ; \\sigma) = - \\delta_{c=1} \\; \\sigma \\; (y_1 - y_0) \\; + \\; \\ln(1 + e^{\\sigma (y_1 - y_0)})  $$"
      ],
      "metadata": {
        "id": "QRCHYKYhDvfe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q3** Implement and compare over twenty epochs with a VGG11."
      ],
      "metadata": {
        "id": "UMHptL6PbLN3"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "id": "v2cqC1psMxmj"
      },
      "outputs": [],
      "source": [
        "class RankNetLoss(torch.nn.Module):\n",
        "    def __init__(self, sigma=1):\n",
        "        super(RankNetLoss, self).__init__()\n",
        "        self.sigma = sigma\n",
        "\n",
        "    def forward(self, y0, y1, labels):\n",
        "        diffs = self.sigma * (y1 - y0)\n",
        "\n",
        "        # Attention to the exponential function\n",
        "        diffs = diffs.clamp(min=-5, max=5)\n",
        "\n",
        "        losses = -1. * (labels == 1) * diffs + torch.log(1 + torch.exp(diffs))\n",
        "\n",
        "        loss = losses.mean()\n",
        "        return loss"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision import models\n",
        "vgg11 = models.vgg11(pretrained=False)"
      ],
      "metadata": {
        "id": "7AlvhBCLKvLT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vgg11.features[0] = nn.Conv2d(channels, 64, kernel_size=3, stride=1, padding=1)\n",
        "vgg11.classifier[6] = nn.Linear(in_features=4096, out_features=1, bias=True)\n",
        "model = vgg11.to(device)\n",
        "\n",
        "criterion = RankNetLoss(1.0)\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n"
      ],
      "metadata": {
        "id": "yjI6LlqJLOja"
      },
      "execution_count": 84,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5ZWfQ_IebGWh"
      },
      "outputs": [],
      "source": [
        "best_model_wts = copy.deepcopy(model.state_dict())\n",
        "best_acc = 0.0\n",
        "train_accs_rn = []\n",
        "val_accs_rn = []\n",
        "\n",
        "phases = ['train', 'val']\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    print(f'Epoch {epoch+1}/{num_epochs}')\n",
        "    print('-' * 10)\n",
        "\n",
        "    # Each epoch has a training and validation phase\n",
        "    for phase in phases:\n",
        "        if phase == 'train':\n",
        "            model.train()  # Set model to training mode\n",
        "        else:\n",
        "            model.eval()   # Set model to evaluate mode\n",
        "\n",
        "        running_loss = 0.0\n",
        "        running_corrects = 0\n",
        "        data_loader = notebook.tqdm(dataloaders[phase], desc=f'{phase.capitalize()} Epoch {epoch}', leave=phase == 'train')\n",
        "        # Iterate over data.\n",
        "        for img1, img2, labels, _, _ in data_loader:\n",
        "            img1 = 0.1 * img1.to(device)\n",
        "            img2 = 0.1 * img2.to(device)\n",
        "            labels = labels.to(device).detach()\n",
        "\n",
        "            # zero the parameter gradients\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # forward\n",
        "            # track history if only in train\n",
        "            with torch.set_grad_enabled(phase == 'train'):\n",
        "                output1 = model(img1)\n",
        "                output2 = model(img2)\n",
        "                _, preds = torch.max(torch.cat((output1, output2), dim=1), 1)\n",
        "                loss = criterion(output1, output2, labels.to(torch.float32).unsqueeze(dim=1))\n",
        "\n",
        "                # backward + optimize only if in training phase\n",
        "                if phase == 'train':\n",
        "                    loss.backward()\n",
        "                    optimizer.step()\n",
        "\n",
        "            # statistics\n",
        "            running_loss += loss.item() * img1.size(0)\n",
        "            running_corrects += torch.sum(preds == labels.data)\n",
        "\n",
        "            data_loader.set_postfix(loss=f'{loss.item():.4f}', refresh=False)\n",
        "\n",
        "        epoch_loss = running_loss / dataset_sizes[phase]\n",
        "        epoch_acc = running_corrects.double() / dataset_sizes[phase]\n",
        "\n",
        "        print(f'{phase} Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}')\n",
        "\n",
        "        if phase == 'train':\n",
        "            train_accs_rn.append(epoch_acc)\n",
        "\n",
        "        if phase == 'val':\n",
        "            val_accs_rn.append(epoch_acc)\n",
        "\n",
        "        # deep copy the model\n",
        "        if phase == 'val' and epoch_acc > best_acc:\n",
        "            best_acc = epoch_acc\n",
        "            best_model_wts = copy.deepcopy(model.state_dict())"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot([v.cpu().numpy() for v in val_accs], color='blue', label='Model 1')\n",
        "plt.plot([v.cpu().numpy() for v in val_accs_rn], color='red', label='Model 2')\n",
        "plt.legend()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 467
        },
        "id": "A1h7KdtLwLoy",
        "outputId": "9114049e-fc05-4e52-933b-70617e097455"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.legend.Legend at 0x7f18bc3f8370>"
            ]
          },
          "metadata": {},
          "execution_count": 52
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ys = []\n",
        "yhats = []\n",
        "\n",
        "for name in names_test:\n",
        "    y = label_dict_test[name]['y']\n",
        "    ys.append(y)\n",
        "\n",
        "    path = join(dir_images_test, name)\n",
        "    image = 0.1 * torch.load(path).cuda().unsqueeze(dim=0)\n",
        "    yhat = model.eval()(image).item()\n",
        "    yhats.append(yhat)\n",
        "\n",
        "ys = np.array(ys)\n",
        "yhats = np.array(yhats)\n",
        "\n",
        "# Plot\n",
        "plt.figure(num=10)\n",
        "plt.scatter(ys, yhats)\n",
        "plt.xlabel('y')\n",
        "plt.ylabel('$\\hat{y}$')\n",
        "\n",
        "# Compute correlation ranks\n",
        "from scipy.stats import spearmanr, kendalltau\n",
        "\n",
        "spearman, ps = spearmanr(ys, yhats)\n",
        "kendall, pk = kendalltau(ys, yhats)\n",
        "\n",
        "print(spearman)\n",
        "print(kendall)"
      ],
      "metadata": {
        "id": "8DAvqKr_rgzt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Conclusion**:\n",
        "\n",
        "In a simple task, the results are roughly equivalent, but the network's progression appears to be more consistent with the RankNet Loss. However, this observation is not always applicable, and it is recommended to experiment with both loss functions to determine their effectiveness for a given task."
      ],
      "metadata": {
        "id": "_hU59QhqL7u1"
      }
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}