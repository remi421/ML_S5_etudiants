{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "L4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Practical session n°5\n",
        "\n",
        "Notions:\n",
        "- Quantile regression\n",
        "- Pinball Loss\n",
        "\n",
        "\n",
        "Duration: 1 h 30\n",
        "\n",
        "In this practical, we immerse ourselves again in the context where the training inputs ($x$) and targets ($y$) are images of the same spatial dimensions. However, we assume that the dependence of $y$ on $x$ is probabilistic in nature. In other words, there exist conditional laws $p(y | x)$, and these laws need to be characterized.\n",
        "\n",
        "Here, we limit ourselves to conditional marginals, meaning the laws $p(y_{i,j} | x)$ where $y_{i,j}$ represents the value of the target $y$ at the pixel $(i,j)$. To characterize these laws, several approaches are possible. For instance, one can attempt to estimate their quantiles or moments. The following exercises illustrate a method for estimating quantiles."
      ],
      "metadata": {
        "id": "ef8WCkg6UrCs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import os\n",
        "import time\n",
        "import torch\n",
        "import torchvision\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import datasets, models, transforms"
      ],
      "metadata": {
        "id": "8IQ1Flnxj_Js"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! git clone https://github.com/nanopiero/ML_S5_etudiants\n",
        "import sys\n",
        "sys.path.append('ML_S5_etudiants/practicals/P5')\n",
        "from utils_P5 import gen_proba, voir_batch2D, UNet"
      ],
      "metadata": {
        "id": "rBewKJe7i4oa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Exercise 1**: on the PINBALL loss\n",
        "\n",
        "As seen in the previous practical session n°4, using the Mean Absolute Error (MAE) led the model to provide the conditional median, which is a particular quantile. To estimate other quantiles, it is sufficient to modify the MAE.\n",
        "\n",
        "This is what we are going to do on a simple problem where the link between the input and the target is defined by:\n",
        "$$y_{i,j} \\sim \\mathcal{N}(\\mu(x_{i,j}), \\sigma(x_{i,j})) $$\n",
        "where $\\mu$ and $\\sigma$ are a priori unknown.\n",
        "\n",
        "The gen_proba function from the utile.py module will allow us to \"explore\" our dataset, or in other words, to sample the random pair $(x, y)$."
      ],
      "metadata": {
        "id": "69GLJkfCEdLc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# sampling (x,y) :\n",
        "\n",
        "input, target = gen_proba(6)\n",
        "# Inputs\n",
        "fig0 = plt.figure(0, figsize=(36, 6))\n",
        "voir_batch2D(input, 6, fig0, k=0, min_scale=0,max_scale=1)\n",
        "\n",
        "# Targets\n",
        "fig1 = plt.figure(1, figsize=(36, 6))\n",
        "voir_batch2D(target, 6, fig1, k=0, min_scale=0,max_scale=1)"
      ],
      "metadata": {
        "id": "kxJ66fmSj8ru"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's consider the cost function be defined at the pixel level by:\n",
        "$$\\mathcal{L}_t(y,z) =  \\rho_t(z - y) $$  \n",
        "where:\n",
        "$$\\rho_t(u) = t \\times max(u,0) + (t-1) \\times min(u,0) $$\n",
        "\n",
        "**Q1** Plot the graph of $\\rho_t$ for different values of $t$ in the range [0, 1] using *matplotlib.pyplot*.\n",
        "How does $\\mathcal{L}_t$ generalize the MAE?"
      ],
      "metadata": {
        "id": "PNcYn6T-R7pC"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "hXEf9uUackpx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q2** In the following cell, code a cost function adapted to the estimation of $t$-th quantiles (use *.clamp*( ))"
      ],
      "metadata": {
        "id": "LYJ2Be76YWom"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def rho(u,t):\n",
        "        loss = ...\n",
        "        ...\n",
        "        return loss"
      ],
      "metadata": {
        "id": "Gv-Mh-2uYV3u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q3** Train a U-Net to estimate the ninth decile."
      ],
      "metadata": {
        "id": "ilrsuaKaeKCa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def criterion(output, target):\n",
        "  return ..."
      ],
      "metadata": {
        "id": "CsmHW72ZeP52"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fcn = UNet(1,1,8).cuda()\n",
        "import torch.optim as optim\n",
        "optimizer = optim.Adam(fcn.parameters(), 10**(-3))"
      ],
      "metadata": {
        "id": "iXOYVV0e9WeP"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nepochs = 50\n",
        "nbatches = 100\n",
        "batchsize = 64\n",
        "\n",
        "train_losses = []\n",
        "\n",
        "for epoch in range(nepochs):\n",
        "\n",
        "    print(\"Epoch \" + str(epoch))\n",
        "    epoch_losses  = []\n",
        "    for i in range(nbatches):\n",
        "\n",
        "        ...\n",
        "\n",
        "        del target, input, loss\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "    epoch_loss = np.mean(epoch_losses)\n",
        "    train_losses.append(epoch_loss)\n",
        "    print('epoch loss : \\n')\n",
        "    print(epoch_loss)"
      ],
      "metadata": {
        "id": "loxEIBT79uOY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q4** The ground truth is given by:\n",
        "$$ \\mu(x_{i,j}) = {x}_{i,j}^2 $$\n",
        "and\n",
        "$$ \\sigma(x_{i,j}) = 0.2 \\times x_{i,j} $$\n",
        "\n",
        "Use [torch.distributions.normal.Normal](https://pytorch.org/docs/stable/distributions.html) to verify that the result corresponds well to the expected quantile."
      ],
      "metadata": {
        "id": "ti8ozTvawblT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# viz:\n",
        "fcn.eval()\n",
        "input, target = gen_proba(6)\n",
        "output = fcn(input.cuda())\n",
        "\n",
        "fig = plt.figure(0, figsize=(36, 6))\n",
        "voir_batch2D(input, 6, fig, k=0, min_scale=0,max_scale=1) # inputs\n",
        "fig1 = plt.figure(1, figsize=(36, 6))\n",
        "voir_batch2D(output.detach().cpu(), 6, fig1, k=0, min_scale=0,max_scale=0.1) # outputs\n",
        "\n",
        "# true quantile\n",
        "...\n",
        "\n",
        "fig2 = plt.figure(2, figsize=(36, 6))\n",
        "voir_batch2D(true_quantiles, 6, fig2, k=0, min_scale=0.,max_scale=0.1) # ninth decile\n",
        "fig3 = plt.figure(3, figsize=(36, 6))\n",
        "voir_batch2D(output.detach().cpu() - true_quantiles, 6, fig2, k=0, min_scale=-0.05,max_scale=0.05) # outputs - ninth decile\n"
      ],
      "metadata": {
        "id": "kZAI6PuM-vRn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Exercise 2**: a trickier dependence\n",
        "\n",
        "Let's now work with the following *gen_proba_2* function."
      ],
      "metadata": {
        "id": "lip_lH8dBcBF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from utils_P5 import make_batch\n",
        "def gen_proba_2(n):\n",
        "  target1 = make_batch(n, rec = 0., noisy_rec= 0., disc = 0.0015)\n",
        "  target2 = make_batch(n, rec = 0.001, noisy_rec= 0., disc = 0.)\n",
        "  m = torch.normal(2, 2. + target2)\n",
        "  input = target1 + target2\n",
        "  target = m\n",
        "  return  input, target\n"
      ],
      "metadata": {
        "id": "I7sY_lSjC7yh"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here, the dependence of $y$ upon $x$ is not clearly visible in the input-target pairs:"
      ],
      "metadata": {
        "id": "Vb1Lbunoa54V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "input, target = gen_proba_2(6)\n",
        "# Inputs\n",
        "fig0 = plt.figure(0, figsize=(36, 6))\n",
        "voir_batch2D(input, 6, fig0, k=0, min_scale=0,max_scale=1)\n",
        "\n",
        "# Targets\n",
        "fig1 = plt.figure(1, figsize=(36, 6))\n",
        "voir_batch2D(target, 6, fig1, k=0, min_scale=0,max_scale=5)"
      ],
      "metadata": {
        "id": "cylBm7rzGFNZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q1** Estimate the 0.1 quantiles of the marginal distributions and visualize the result."
      ],
      "metadata": {
        "id": "JQJhwixjiRr0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def criterion(output, target):\n",
        "  return rho(target-output, 0.1)  #on vise le premier décile\n",
        "\n",
        "nepochs = 50\n",
        "nbatches = 100\n",
        "batchsize = 64\n",
        "\n",
        "train_losses = []\n",
        "\n",
        "for epoch in range(nepochs):\n",
        "\n",
        "    print(\"Epoch \" + str(epoch))\n",
        "    epoch_losses  = []\n",
        "    for i in range(nbatches):    # nbatch = datasetsize/batchsize\n",
        "\n",
        "        ...\n",
        "\n",
        "        epoch_losses.append(loss.detach().cpu())\n",
        "\n",
        "        del target, input, loss\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "    epoch_loss = np.mean(epoch_losses)\n",
        "    train_losses.append(epoch_loss)\n",
        "    print('epoch loss : \\n')\n",
        "    print(epoch_loss)"
      ],
      "metadata": {
        "id": "8aDSslTlJCpz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#visualisation:\n",
        "\n",
        "fcn.eval()\n",
        "\n",
        "input, target = gen_proba_2(6)\n",
        "\n",
        "...\n",
        "\n",
        "fig = plt.figure(0, figsize=(36, 6))\n",
        "voir_batch2D(input, 6, fig, k=0, min_scale=0,max_scale=1)\n",
        "fig1 = plt.figure(1, figsize=(36, 6))\n",
        "voir_batch2D(output.detach().cpu(), 6, fig1, k=0, min_scale=-1,max_scale=0)"
      ],
      "metadata": {
        "id": "PuhjQfFvKOCH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q2** Do outputs depend on inputs? Is it a pixel-wise dependence as in exercise 1? (i.e., does the distribution of $y_{i,j}$ depend only on $x_{i,j}$)"
      ],
      "metadata": {
        "id": "nRTsCV_9kcVu"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "pqHiSFyPeMl_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "**Exercise 3**:\n",
        "\n",
        "One can attempt to estimate $n$ quantiles simultaneously. To achieve this, the simplest way is to:\n",
        "\n",
        "* change the number of output channels of the U-Net—e.g., 99 channels if you are estimating the $n = 100$ quantiles of orders 1%, 2%, ... 99%.\n",
        "* define the overall cost function by summing individual cost functions for each of the channels.\n",
        "The goal of this exercise is to code a matrix version of this overall cost function and illustrate the convergence of the learning process.\n"
      ],
      "metadata": {
        "id": "-oBcUlyHU0s2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q1** Let $t$ be a vector of size $n-1$, and $M$ be a 4th-order tensor. Code the function: $$dot(t, M) = M^{\\prime}$$ where: $$M^{\\prime}_{b,i,j} = \\underset{0<c<n}{\\sum} t_c \\times M_{b,c,i,j}$$.\n",
        "\n",
        "This function will allow to apply $\\rho_{t_c}$ (see **Exercise 1**) to each channel $c$ of the output $M$ (where $b$ represents the batch index and $i,j$ represent spatial dimensions)."
      ],
      "metadata": {
        "id": "stNIvyET1K3w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def dot(t, M):\n",
        "\treturn ..."
      ],
      "metadata": {
        "id": "8A0ITnjs09T6"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q2** Complete the class *QuantileLoss*:"
      ],
      "metadata": {
        "id": "h6cEsNlLAoHM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class QuantileLoss(torch.nn.Module):\n",
        "\n",
        "    def __init__(self , n):\n",
        "        super(QuantileLoss, self).__init__()\n",
        "        self.n = n   # n= 100 -> centiles\n",
        "        self.t = torch.arange(1/n,1,1/n).cuda()  # quantile orders\n",
        "\n",
        "    def forward(self, output, target):\n",
        "        bs, _, nx, ny = target.shape\n",
        "        ...\n",
        "        loss = torch.mean(loss_by_pix)\n",
        "\n",
        "        return loss"
      ],
      "metadata": {
        "id": "PgTmPsD4PVQx"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q3** Start the training with *gen_proba* and $n=100$, then evaluate the estimation."
      ],
      "metadata": {
        "id": "RL-KYh52B2BY"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ISbNbOtlIw1B"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}