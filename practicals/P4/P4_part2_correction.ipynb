{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Practical session n°4\n",
        "\n",
        "Notions:\n",
        "- Semantic segmentation\n",
        "- Intersection over Union\n",
        "- Image Denoising\n",
        "- Fully convolutional networks, U-Net\n",
        "- Weak supervision (in part II): The noise-to-noise and the Neural Eggs Separation scenarios.\n",
        "\n",
        "Duration: 2 h + 2 h\n",
        "\n",
        "In P3, we illustrated how Convolutional Neural Networks (CNNs) are trained for image classification tasks. In this practical session, we demonstrate how to achieve pixel-level predictions for tasks like semantic segmentation and image denoising.\n",
        "\n",
        "To start, we’ll simply apply an off-the-shelf model. Then, we’ll focus on training a model from scratch (part I, exercise 2 and part II).\n",
        "\n",
        "In P3, we also introduced a crucial set of methods known as \"transfer learning,\" which is particularly effective when there’s limited training data. In this session, we’ll explore another equally important set of methods called \"weak supervision,\" which is well-suited for cases where ground truth is imperfectly known (Part II)."
      ],
      "metadata": {
        "id": "MCN6FPP8tPo-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part II: denoising with FCN - weakly-supervised approaches\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "EpYCZBnPH0RL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import os\n",
        "import time\n",
        "\n",
        "import torch\n",
        "import torchvision\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import datasets, models, transforms\n",
        "import torch.optim as optim\n",
        "from PIL import Image"
      ],
      "metadata": {
        "id": "iqp_Y4AhIqKZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "First, import the functions contained in the \"utile\" module."
      ],
      "metadata": {
        "id": "Wpu3GQiDAX0I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! git clone https://github.com/nanopiero/ML_S5_etudiants\n",
        "! cp ML_S5_etudiants/practicals/P4/utile.py .\n",
        "from utile import gen_noise2noise, voir_batch2D"
      ],
      "metadata": {
        "id": "ShfsOSzse6-H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**Exercise 1: The \"noise to noise\" scenario**\n",
        "\n",
        "In Exercise 2, Part I, we had pairs (noisy version, clean version) to learn from. However, in real-world applications, clean versions are barely available. \\\n",
        "Sometimes, there are instances where multiple noisy versions are accessible. Consider, for example, photos taken with a telephoto lens with several seconds between shots: the effects of air turbulence on image quality are independent from one image to another. \\\n",
        "In the \"noise to noise\" scenario [(Lehtinen,2018)](https://arxiv.org/pdf/1803.04189.pdf), we have pairs of noisy images, organized into two sets $B^1_{noisy}$ and $B^2_{noisy}$.\n",
        "To situate ourselves in this scenario, we utilize the synthetic images from Exercise 1. The *gen_noise2noise* function allows us to sample from both datasets:"
      ],
      "metadata": {
        "id": "xypzPxQjBDzR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "image1, image2 = gen_noise2noise(6)\n",
        "fig = plt.figure(0, figsize=(36, 6))\n",
        "voir_batch2D(image1, 6, fig, k=0, min_scale=0,max_scale=1)\n",
        "\n",
        "fig2 = plt.figure(1, figsize=(36, 6))\n",
        "voir_batch2D(image2.detach().cpu(), 6, fig2, k=0, min_scale=0,max_scale=1)"
      ],
      "metadata": {
        "id": "2WAABDUPBDBr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q1:** Define a learning procedure that effectively denoises the image."
      ],
      "metadata": {
        "id": "55CM4Dt_r5o4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fcn = UNet(1,1,16).cuda()\n",
        "\n",
        "import torch.optim as optim\n",
        "optimizer = optim.Adam(fcn.parameters(), 10**(-3))"
      ],
      "metadata": {
        "id": "NvCsUxP9QF0u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nepochs = 40\n",
        "nbatches = 100\n",
        "batchsize = 64\n",
        "\n",
        "train_losses = []\n",
        "\n",
        "\n",
        "def criterion(output,target):\n",
        "  return torch.mean(torch.abs(output-target))  # idea: use the MAE loss to target the conditionnal median\n",
        "\n",
        "for epoch in range(nepochs):\n",
        "\n",
        "\n",
        "    print(\"Epoch \" + str(epoch))\n",
        "    epoch_losses  = []\n",
        "    for i in range(nbatches):\n",
        "        #Load inputs\n",
        "\n",
        "        # Generation\n",
        "        input,target = gen_noise2noise(batchsize)\n",
        "\n",
        "        # go to GPU\n",
        "        input = input.cuda()\n",
        "        target = target.cuda()\n",
        "\n",
        "        # zeroing gradients, forward pass\n",
        "        optimizer.zero_grad()\n",
        "        output = fcn(input)\n",
        "\n",
        "        # loss and compuation of gradients\n",
        "        loss = criterion(output,target)\n",
        "        loss.backward()\n",
        "\n",
        "        # Update weights\n",
        "        optimizer.step()\n",
        "\n",
        "        epoch_losses.append(loss.detach().cpu())\n",
        "\n",
        "        del target, input, loss\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "    epoch_loss = np.mean(epoch_losses)\n",
        "    train_losses.append(epoch_loss)\n",
        "    print('epoch loss : \\n')\n",
        "    print(epoch_loss)"
      ],
      "metadata": {
        "id": "57CKkJpoCm-O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(train_losses)"
      ],
      "metadata": {
        "id": "FHPsYB24RTq9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# viz:\n",
        "\n",
        "fcn.eval()\n",
        "\n",
        "input, target = gen_noise2noise(6)\n",
        "\n",
        "output = fcn(input.cuda())\n",
        "\n",
        "\n",
        "fig = plt.figure(0, figsize=(36, 6))\n",
        "voir_batch2D(input, 6, fig, k=0, min_scale=0,max_scale=1)  # inputs\n",
        "fig2 = plt.figure(1, figsize=(36, 6))\n",
        "voir_batch2D(target, 6, fig2, k=0, min_scale=0,max_scale=1)  # targets\n",
        "fig3 = plt.figure(2, figsize=(36, 6))\n",
        "voir_batch2D(output.detach().cpu(), 6, fig2, k=0, min_scale=0,max_scale=1) # outputs\n"
      ],
      "metadata": {
        "id": "EIEkZ8U1RJo9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q2:** Theoretical analysis:\n",
        "\n",
        "The training procedure involves searching for the weights $\\theta^*$ that satisfy:\n",
        "\n",
        "\\begin{equation}\n",
        "\\theta^* = \\underset{\\theta}{argmin} {\\big \\{} \\mathbb{E}_{(X,Y)}[ \\ \\mathcal{L} (f_\\theta(X), Y) \\ ] {\\big \\}} \\tag{1}\n",
        "\\end{equation}\n",
        "\n",
        "Rewrite equation (1) to justify the approach taken for **Q1**.\n",
        "\n"
      ],
      "metadata": {
        "id": "Qf3GTjMhsrQn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We rewrite the expectation to be minimized in the case of a cost function equal to the Mean Absolute Error (MAE):\n",
        "\n",
        "\\begin{equation}\n",
        " \\mathbb{E}_{(X,Y)}[ \\ \\mathcal{L} (f_\\theta(X), Y) \\ ] = \\mathbb{E}_{X}[ \\mathbb{E}_{Y | X}\\ |f_\\theta(X) - Y | \\ ]\n",
        "\\end{equation}\n",
        "\n",
        "To reach the minimum, the network must provide the conditional median of Y given X. As the probability of being affected by a clutter is less than 0.5 (in this simple version of the scenario), the conditional median targeted by the network is the clean target !\n",
        "\n",
        "In the case of a quadratic cost function, the network will suggest the conditional expectation. Unlike the median, this is biased, as clutter always contribute positively.\n"
      ],
      "metadata": {
        "id": "HUgkSArnkXeK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**Exercise 2: The \"NES\" scenario (Neural Egg Separation)**\n",
        "\n",
        "In another scenario, two sources of images can be sampled independently. The first provides noisy images containing the object of interest. Images from the second source contain only noise. Additionally, the corruption process, i.e., how the noise is combined with the clean image, is assumed to be known. This is a relatively common scenario. For instance, in the case of meteorological radar images, it is possible to extract images containing only noise during non-precipitation periods.\n",
        "\n",
        "This scenario was defined by [Halperin et al (2018)](https://arxiv.org/pdf/1811.12739.pdf). These authors propose an iterative method to address it (Neural Egg Separation). In this exercise, we will rely on a fairly similar principle, still using our synthetic images.\n",
        "\n",
        "The functions gen1_NES and gen2_NES allow sampling from the two sources. The corruption process is trivial: the noise (rectangles) is simply added to the clean image (cells).   \n",
        "\n"
      ],
      "metadata": {
        "id": "qeHiaJE5O44_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "image1 = gen1_NES(6)\n",
        "noise = gen2_NES(6)\n",
        "\n",
        "# sample noisy pictures:\n",
        "fig = plt.figure(0, figsize=(36, 6))\n",
        "voir_batch2D(image1, 6, fig, k=0, min_scale=0,max_scale=1)\n",
        "\n",
        "# sample noise:\n",
        "fig2 = plt.figure(1, figsize=(36, 6))\n",
        "voir_batch2D(noise, 6, fig2, k=0, min_scale=0,max_scale=1)"
      ],
      "metadata": {
        "id": "NR-X2s_iQdW1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q1:** Propose a learning strategy that leads to effective denoising.\n",
        "\n",
        "**Q2:** Discuss the limitations of the method and mention some avenues for improvement."
      ],
      "metadata": {
        "id": "qn6-Nsd030qO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The idea is to add images containing noise and target the original image. By adding enough noise so that the additional artifacts dominate and by choosing the cost function appropriately, the network learns to denoise:"
      ],
      "metadata": {
        "id": "Numa1eAal4Bs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Examples of (input, target) couples:\n",
        "\n",
        "batchsize = 6\n",
        "# Get image + noise and additional noises\n",
        "image = gen1_NES(batchsize)\n",
        "noise = gen2_NES(batchsize)\n",
        "noise2 = gen2_NES(batchsize)\n",
        "\n",
        "# Build input and targets\n",
        "input = image + noise + noise2\n",
        "target = image\n",
        "\n",
        "# inputs\n",
        "fig = plt.figure(0, figsize=(36, 6))\n",
        "voir_batch2D(input, 6, fig, k=0, min_scale=0,max_scale=1)\n",
        "\n",
        "# targets\n",
        "fig2 = plt.figure(1, figsize=(36, 6))\n",
        "voir_batch2D(target, 6, fig2, k=0, min_scale=0,max_scale=1)"
      ],
      "metadata": {
        "id": "WPa_qwfmr7-o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fcn = UNet(1,1,16).cuda()  #1 canal entrée, 1 canal de sortie, paramètre taille du réseau: 16\n",
        "\n",
        "import torch.optim as optim\n",
        "optimizer = optim.Adam(fcn.parameters(), 10**(-3))"
      ],
      "metadata": {
        "id": "h-j9ayhtTmwv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def criterion(output,target):\n",
        "  return torch.mean(torch.abs(output-target))  # again, hope the conditionnal median\n",
        "                                               # is a good estimator.\n",
        "\n",
        "nepochs = 40\n",
        "nbatches = 100\n",
        "batchsize = 64\n",
        "\n",
        "train_losses = []\n",
        "\n",
        "\n",
        "for epoch in range(nepochs):\n",
        "\n",
        "\n",
        "    print(\"Epoch \" + str(epoch))\n",
        "    epoch_losses  = []\n",
        "    for i in range(nbatches):    # nbatch = datasetsize/batchsize\n",
        "\n",
        "        # generation\n",
        "        image = gen1_NES(batchsize).cuda()\n",
        "        noise = gen2_NES(batchsize).cuda()\n",
        "        noise2 = gen2_NES(batchsize).cuda()\n",
        "\n",
        "        # build the sum\n",
        "        input = image + noise + noise2  #on ajoute deux images de bruit\n",
        "        target = image\n",
        "\n",
        "        # zeroing gradients, forward pass\n",
        "        optimizer.zero_grad()\n",
        "        output = fcn(input)\n",
        "\n",
        "        # loss, gradients\n",
        "        loss = criterion(output,target)\n",
        "        loss.backward()\n",
        "\n",
        "        # update weights\n",
        "        optimizer.step()\n",
        "\n",
        "        epoch_losses.append(loss.detach().cpu())\n",
        "\n",
        "        del target, input, loss\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "    epoch_loss = np.mean(epoch_losses)\n",
        "    train_losses.append(epoch_loss)\n",
        "    print('epoch loss : \\n')\n",
        "    print(epoch_loss)"
      ],
      "metadata": {
        "id": "nG-U2-_kUOZw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# viz:\n",
        "\n",
        "fcn.eval()\n",
        "input = gen1_NES(6)\n",
        "output = fcn(input.cuda())\n",
        "\n",
        "\n",
        "\n",
        "fig = plt.figure(0, figsize=(36, 6))\n",
        "voir_batch2D(input, 6, fig, k=0, min_scale=0,max_scale=1)\n",
        "\n",
        "fig3 = plt.figure(2, figsize=(36, 6))\n",
        "voir_batch2D(output.detach().cpu(), 6, fig2, k=0, min_scale=0,max_scale=1)"
      ],
      "metadata": {
        "id": "cIhemt4VWxs_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can see that the reconstruction is not entirely perfect. However, these imperfections are explainable! When an overlap occurs, there is more than a fifty-fifty chance that at least one of the two patterns is present in the original image. To improve the processing, there are several methods. Exercise sheet number 2 provides an opportunity to explore one of them."
      ],
      "metadata": {
        "id": "N-I26XR7th6c"
      }
    }
  ]
}