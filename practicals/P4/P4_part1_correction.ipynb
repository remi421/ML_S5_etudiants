{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "c235c577-43a5-4698-9c3e-f92245eeb63a",
      "metadata": {
        "id": "c235c577-43a5-4698-9c3e-f92245eeb63a"
      },
      "source": [
        "# Practical session n°4\n",
        "\n",
        "Notions:\n",
        "- Semantic segmentation\n",
        "- Intersection over Union\n",
        "- Image Denoising\n",
        "- Fully convolutional networks, U-Net\n",
        "- Weak supervision (in part II): The noise-to-noise and the Neural Eggs Separation scenarios.\n",
        "\n",
        "Duration: 2 h + 2 h\n",
        "\n",
        "In P3, we illustrated how Convolutional Neural Networks (CNNs) are trained for image classification tasks. In this practical session, we demonstrate how to achieve pixel-level predictions for tasks like semantic segmentation and image denoising.\n",
        "\n",
        "To start, we’ll simply apply an off-the-shelf model. Then, we’ll focus on training a model from scratch (part I, exercise 2 and part II).\n",
        "\n",
        "In P3, we also introduced a crucial set of methods known as \"transfer learning,\" which is particularly effective when there’s limited training data. In this session, we’ll explore another equally important set of methods called \"weak supervision,\" which is well-suited for cases where ground truth is imperfectly known (Part II).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4e922d59-300f-4e0d-ad71-3f584bb87089",
      "metadata": {
        "id": "4e922d59-300f-4e0d-ad71-3f584bb87089"
      },
      "source": [
        "## Part I: Semantic Segmentation and Image Denoising with Fully Convolutional Networks"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "322901ab-9073-4620-be90-f264fa14ec9c",
      "metadata": {
        "id": "322901ab-9073-4620-be90-f264fa14ec9c"
      },
      "source": [
        "This part aims to familiarize you with Fully Convolutional Networks and  pixel-level predictions.\n",
        "\n",
        "By definition, a Fully Convolutional Network (FCN) does not contain fully connected layers. As a result, the output retains spatial dimensions. This configuration is useful when the learning target itself is an image. This is the case for tasks such as:\n",
        "- Semantic segmentation, where each pixel is assigned a semantic class (e.g., ground, sky, clouds, buildings, etc.).\n",
        "- Pixel-wise regression\n",
        "- Image denoising\n",
        "- Super-resolution\n",
        "\n",
        "The first exercise features an FCN built from a ResNet50 for a simple segmentation task defined from a set of real images segmented by hand.\n",
        "\n",
        "The second exercise proposes a pixel-wise regression task, completely supervised, defined on a set of dynamically generated synthetic images."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Exercise 1: Semantic Segmentation with FCN-ResNet (no training)**"
      ],
      "metadata": {
        "id": "4VEEDrnUIB-C"
      },
      "id": "4VEEDrnUIB-C"
    },
    {
      "cell_type": "markdown",
      "id": "b7a950ac-4a53-419e-ba51-32c2961ed8b5",
      "metadata": {
        "id": "b7a950ac-4a53-419e-ba51-32c2961ed8b5"
      },
      "source": [
        "**A.** Presentation of the Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3c946232-77c7-48a6-97a3-b859dc1d4e98",
      "metadata": {
        "id": "3c946232-77c7-48a6-97a3-b859dc1d4e98"
      },
      "source": [
        "In the following cells, we load the necessary libraries, download the set of segmented images prepared for the [Pascal VOC 2007](http://host.robots.ox.ac.uk/pascal/VOC/voc2007/segexamples/index.html) challenge, and visualize input-target pairs from the training set:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d9a87a8a-065c-4d96-9056-96fe3f2f6d89",
      "metadata": {
        "id": "d9a87a8a-065c-4d96-9056-96fe3f2f6d89"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import os\n",
        "import time\n",
        "import torch\n",
        "import torchvision\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import datasets, models, transforms\n",
        "import torch.optim as optim\n",
        "from PIL import Image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cba2f280-3609-4fc6-9d47-ba9eb29236e7",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cba2f280-3609-4fc6-9d47-ba9eb29236e7",
        "outputId": "c4eb3981-3ac9-4f4f-dc83-8660fc99bd83"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "You are on GPU !\n"
          ]
        }
      ],
      "source": [
        "# Check GPU availability\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "  device = torch.device(\"cuda\")\n",
        "  print(\"You are on GPU !\")\n",
        "else:\n",
        "  print('Change the runtime to GPU or continue with CPU, but this should slow down your trainings')\n",
        "  device = torch.device(\"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3bb6b382-a270-4e4e-b7e5-95943b600159",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3bb6b382-a270-4e4e-b7e5-95943b600159",
        "outputId": "750aa385-47ed-4641-aa27-c256f4b04ea4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading http://host.robots.ox.ac.uk/pascal/VOC/voc2007/VOCtrainval_06-Nov-2007.tar to ./data/VOCtrainval_06-Nov-2007.tar\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 460M/460M [00:18<00:00, 25.3MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/VOCtrainval_06-Nov-2007.tar to ./data\n"
          ]
        }
      ],
      "source": [
        "# Load the training dataset\n",
        "\n",
        "input_resize = transforms.Resize((128, 128))\n",
        "target_resize = transforms.Resize((128, 128))\n",
        "\n",
        "train_dataset_viz = datasets.VOCSegmentation(\n",
        "    './data',\n",
        "    year='2007',\n",
        "    image_set='train',\n",
        "    download = True,\n",
        "    transform=input_resize,\n",
        "    target_transform=target_resize,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bd8f19ad-c3db-4b4a-a3f4-d7b90941b0bc",
      "metadata": {
        "id": "bd8f19ad-c3db-4b4a-a3f4-d7b90941b0bc"
      },
      "outputs": [],
      "source": [
        "# Viz some images\n",
        "\n",
        "import math\n",
        "\n",
        "def plot_images(images, num_per_row=4, title=None):\n",
        "    num_rows = int(math.ceil(len(images) / num_per_row))\n",
        "\n",
        "    fig, axes = plt.subplots(num_rows, num_per_row,figsize=(4*num_per_row,4*num_rows))\n",
        "    #fig.subplots_adjust(wspace=0, hspace=0)\n",
        "\n",
        "    for image, ax in zip(images, axes.flat):\n",
        "        ax.imshow(image)\n",
        "        ax.axis('off')\n",
        "\n",
        "    return fig\n",
        "\n",
        "# Echantilonnage du dataset de viz:\n",
        "\n",
        "inputs, ground_truths = list(zip(*[train_dataset_viz[i] for i in range(8)]))\n",
        "\n",
        "_ = plot_images(inputs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e74e9396-8ed9-4729-9142-4fd1a6241366",
      "metadata": {
        "id": "e74e9396-8ed9-4729-9142-4fd1a6241366"
      },
      "outputs": [],
      "source": [
        "# Viz some targets\n",
        "\n",
        "_ = plot_images(ground_truths)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0478d313-a90d-420a-a74a-2e6c78b5ae44",
      "metadata": {
        "id": "0478d313-a90d-420a-a74a-2e6c78b5ae44"
      },
      "source": [
        "**Question 1:** How many classes are there? \\\\\n",
        "Search the web for the difference between semantic segmentation and instance segmentation. What type of segmentation is this dataset about?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "TSQMGJaS5Pqs",
      "metadata": {
        "id": "TSQMGJaS5Pqs",
        "nbgrader": {
          "grade": false,
          "grade_id": "cell-9636b6792aadc531",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "tags": []
      },
      "outputs": [],
      "source": [
        "# num_classes = ...\n",
        "\n",
        "num_classes = 21\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "MC1WQOva5s8j",
      "metadata": {
        "id": "MC1WQOva5s8j",
        "nbgrader": {
          "grade": true,
          "grade_id": "cell-b2ebe011b0b5e6f6",
          "locked": false,
          "points": 0,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "tags": []
      },
      "source": [
        "\n",
        "1. **Semantic Segmentation:**\n",
        "   - **Objective:** Assign a class label to each pixel in an image, indicating the category or type of the object to which it belongs.\n",
        "   - **Target:** The segmentation masks indicate the class or category of each pixel.\n",
        "\n",
        "\n",
        "2. **Instance Segmentation:**\n",
        "   - **Objective:** Identify and outline individual objects in an image.\n",
        "   - **Target:** The mask contains the same value for each set of pixels associated with the same physical object.\n",
        "\n",
        "\n",
        "\n",
        "Here, the segmentation task is a semantic segmentation, where each pixel is assigned a class label representing the category of the object to which it belongs. The classes may include categories like ground, sky, clouds, buildings, etc.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2ad29c10-d73a-4e9e-a68f-3de640091792",
      "metadata": {
        "id": "2ad29c10-d73a-4e9e-a68f-3de640091792"
      },
      "source": [
        "**B.** Presentation of an FCN-ResNet"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "67298d17-176b-4f62-af9b-083aee1b52c3",
      "metadata": {
        "id": "67298d17-176b-4f62-af9b-083aee1b52c3"
      },
      "source": [
        "In the next cell, we load an [FCN](https://pytorch.org/vision/stable/models/fcn.html) built from a [ResNet50](https://arxiv.org/pdf/1512.03385.pdf).This model achieved state-of-the-art performance in 2015 according to [paperswithcode](https://paperswithcode.com/sota/semantic-segmentation-on-ade20k)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "30500a47-f15e-420a-83eb-1f977a4fbedd",
      "metadata": {
        "id": "30500a47-f15e-420a-83eb-1f977a4fbedd"
      },
      "outputs": [],
      "source": [
        "fcn = torchvision.models.segmentation.fcn_resnet50(weights_backbone = None)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c13c65d5-8231-4071-99ec-a1295f2bd924",
      "metadata": {
        "id": "c13c65d5-8231-4071-99ec-a1295f2bd924"
      },
      "source": [
        "**Q2:** What is different from a standard ResNet50? Does the FCN provide an output of the same size as the input? Test and explain."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# sample the dataset, convert to torch.tensor:\n",
        "batch_size = 4\n",
        "imagenet_mean, imagenet_std = [0.485, 0.456, 0.406], [0.229, 0.224, 0.225]\n",
        "input_resize = transforms.Resize([64,64])\n",
        "target_resize = transforms.Resize([64,64])\n",
        "\n",
        "# Transforms used during the training :\n",
        "input_transform = transforms.Compose(\n",
        "    [\n",
        "        input_resize,\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(imagenet_mean, imagenet_std),\n",
        "    ]\n",
        ")\n",
        "\n",
        "def replace_tensor_value_(tensor, a, b):\n",
        "    tensor[tensor == a] = b\n",
        "    return tensor\n",
        "\n",
        "target_transform = transforms.Compose(\n",
        "    [\n",
        "        target_resize,\n",
        "        transforms.PILToTensor(),\n",
        "        transforms.Lambda(lambda x: replace_tensor_value_(x.squeeze(0).long(), 255, 21)),\n",
        "    ]\n",
        ")\n",
        "\n",
        "# Def of the Dataset object :\n",
        "train_dataset = datasets.VOCSegmentation(\n",
        "    './data',\n",
        "    year='2007',\n",
        "    download=False,\n",
        "    image_set='train',\n",
        "    transform=input_transform,\n",
        "    target_transform=target_transform,\n",
        ")\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
        "inputs, targets  = next(iter(train_loader))\n",
        "\n",
        "\n",
        "# Test here :\n",
        "y = fcn(inputs)\n",
        "print(y.keys())\n",
        "print(inputs.shape, targets.shape, y['out'].shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5BPa-_fhNg6q",
        "outputId": "d65db0af-52c3-4f9b-f750-3b273e580cb5"
      },
      "id": "5BPa-_fhNg6q",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "odict_keys(['out'])\n",
            "torch.Size([4, 3, 64, 64]) torch.Size([4, 64, 64]) torch.Size([4, 21, 64, 64])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fd8c3b5f-18c2-4cce-a922-eedf42bdb61c",
      "metadata": {
        "id": "fd8c3b5f-18c2-4cce-a922-eedf42bdb61c"
      },
      "outputs": [],
      "source": [
        "resnet50 =  torchvision.models.resnet50()\n",
        "print(resnet50)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4af7e6db-f35b-4934-801c-3260035a9c19",
      "metadata": {
        "id": "4af7e6db-f35b-4934-801c-3260035a9c19"
      },
      "outputs": [],
      "source": [
        "print(fcn)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Decomposition:\n",
        "y = fcn.backbone(inputs)\n",
        "print(y['out'].shape)\n",
        "\n",
        "print(fcn.classifier(y['out']).shape)\n",
        "\n",
        "\n",
        "import inspect\n",
        "\n",
        "# Assuming `model` is your PyTorch model instance\n",
        "print(inspect.getsource(fcn.forward))"
      ],
      "metadata": {
        "id": "OQM9N0GsP-la"
      },
      "id": "OQM9N0GsP-la",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "c673afd6-c837-44af-b066-8733f2cb123b",
      "metadata": {
        "id": "c673afd6-c837-44af-b066-8733f2cb123b"
      },
      "source": [
        "In summary, compared to a standard ResNet50:\n",
        "- The final perceptron is replaced by a fully convolutional \"head\" ([FCNhead](https://github.com/pytorch/vision/blob/main/torchvision/models/segmentation/fcn.py)).\n",
        "- The stride = 2 is replaced by dilation. While a stride = 2 implies a spatial dimension reduction by half, this is not the case with a dilation = 2 / 4 (see this [illustration](https://github.com/vdumoulin/conv_arithmetic)).\n",
        "- The forward pass involves an interpolation step at the end."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f354b3a6-fecb-4fe3-8b9e-ad3ff5f62b67",
      "metadata": {
        "id": "f354b3a6-fecb-4fe3-8b9e-ad3ff5f62b67"
      },
      "source": [
        "**C. Testing a Pretrained Network**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c5be37f2-bf35-4060-97a8-0f3d8c0c698a",
      "metadata": {
        "id": "c5be37f2-bf35-4060-97a8-0f3d8c0c698a"
      },
      "source": [
        "In this exercise, we simply **test** a model trained on another segmentation dataset. An extension to this exercise provides an opportunity to train the model introduced in exercise sheet #2."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8ad4a0f3-108f-47f5-b7fe-7452547a4c27",
      "metadata": {
        "id": "8ad4a0f3-108f-47f5-b7fe-7452547a4c27"
      },
      "outputs": [],
      "source": [
        "fcn = torchvision.models.segmentation.fcn_resnet50(weights='COCO_WITH_VOC_LABELS_V1')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "31ab4d24-23d5-487c-9dbe-e5625fbfc44d",
      "metadata": {
        "id": "31ab4d24-23d5-487c-9dbe-e5625fbfc44d"
      },
      "outputs": [],
      "source": [
        "batch_size = 16\n",
        "imagenet_mean, imagenet_std = [0.485, 0.456, 0.406], [0.229, 0.224, 0.225]\n",
        "input_resize = transforms.Resize((256,256))\n",
        "target_resize = transforms.Resize((256,256))\n",
        "\n",
        "input_transform = transforms.Compose(\n",
        "    [\n",
        "        input_resize,\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(imagenet_mean, imagenet_std),\n",
        "    ]\n",
        ")\n",
        "\n",
        "target_transform = transforms.Compose(\n",
        "    [\n",
        "        target_resize,\n",
        "        transforms.PILToTensor(),\n",
        "    ]\n",
        ")\n",
        "\n",
        "test_dataset = datasets.VOCSegmentation(\n",
        "    './data',\n",
        "    year='2007',\n",
        "    download=False,\n",
        "    image_set='val',\n",
        "    transform=input_transform,\n",
        "    target_transform=target_transform,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6c498bcc-fcfc-44f2-a30f-2a0747e04075",
      "metadata": {
        "id": "6c498bcc-fcfc-44f2-a30f-2a0747e04075"
      },
      "outputs": [],
      "source": [
        "# Creating loaders\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size,\n",
        "                         shuffle=False, num_workers=2)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6l5HcC7VAq4C",
      "metadata": {
        "id": "6l5HcC7VAq4C",
        "nbgrader": {
          "grade": true,
          "grade_id": "cell-840366ac0521f360",
          "locked": false,
          "points": 0,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "tags": []
      },
      "source": [
        "**Q3:** According to the preceding code lines, which set (validation or test) of PascalVOC2007 are we testing the model on? Why?\n",
        "\n",
        "\n",
        "\n",
        "We are testing our model on a \"validation set\". Generally, the test targets of a challenge are not published. For this reason, such a validation set is often used as a test bench, even in the literature. \\\\\n",
        "It doesn't mean that there was no validation step during the training of the model. In our specific case, the real validation took place on [COCO](https://cocodataset.org/#stuff-2017), and our test is performed on the PascalVOC validation set.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2c815484-4636-4659-90dc-52af6f299b69",
      "metadata": {
        "id": "2c815484-4636-4659-90dc-52af6f299b69"
      },
      "source": [
        "Now, let's visualize some model outputs on this set:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "73e20b2e-6317-4c3c-b099-cf4fef58b5fe",
      "metadata": {
        "id": "73e20b2e-6317-4c3c-b099-cf4fef58b5fe"
      },
      "outputs": [],
      "source": [
        "# Color palette for segmentation masks\n",
        "PALETTE = np.array(\n",
        "    [\n",
        "        [0, 0, 0],\n",
        "        [128, 0, 0],\n",
        "        [0, 128, 0],\n",
        "        [128, 128, 0],\n",
        "        [0, 0, 128],\n",
        "        [128, 0, 128],\n",
        "        [0, 128, 128],\n",
        "        [128, 128, 128],\n",
        "        [64, 0, 0],\n",
        "        [192, 0, 0],\n",
        "        [64, 128, 0],\n",
        "        [192, 128, 0],\n",
        "        [64, 0, 128],\n",
        "        [192, 0, 128],\n",
        "        [64, 128, 128],\n",
        "        [192, 128, 128],\n",
        "        [0, 64, 0],\n",
        "        [128, 64, 0],\n",
        "        [0, 192, 0],\n",
        "        [128, 192, 0],\n",
        "        [0, 64, 128],\n",
        "    ]\n",
        "    + [[0, 0, 0] for i in range(256 - 22)]\n",
        "    + [[255, 255, 255]],\n",
        "    dtype=np.uint8,\n",
        ")\n",
        "\n",
        "\n",
        "def array1d_to_pil_image(array):\n",
        "    pil_out = Image.fromarray(array.astype(np.uint8), mode='P')\n",
        "    pil_out.putpalette(PALETTE)\n",
        "    return pil_out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d0bfa5b3-75c9-4af1-9b7c-f0cfbd02355d",
      "metadata": {
        "id": "d0bfa5b3-75c9-4af1-9b7c-f0cfbd02355d"
      },
      "outputs": [],
      "source": [
        "inputs, targets = next(iter(test_loader))\n",
        "outputs = fcn(inputs)['out']\n",
        "outputs = outputs.argmax(1)\n",
        "\n",
        "outputs = replace_tensor_value_(outputs, 21, 255)\n",
        "targets = replace_tensor_value_(targets, 21, 255)\n",
        "targets = targets.squeeze(dim=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9c7e48ab-584f-4c58-8284-881c29b6fa37",
      "metadata": {
        "id": "9c7e48ab-584f-4c58-8284-881c29b6fa37"
      },
      "outputs": [],
      "source": [
        "plt_inputs = np.clip(inputs.numpy().transpose((0, 2, 3, 1)) * imagenet_std + imagenet_mean, 0, 1)\n",
        "fig = plot_images(plt_inputs)\n",
        "fig.suptitle(\"Images\")\n",
        "\n",
        "pil_outputs = [array1d_to_pil_image(out) for out in outputs.numpy()]\n",
        "fig = plot_images(pil_outputs)\n",
        "fig.suptitle(\"Predictions\")\n",
        "\n",
        "pil_targets = [array1d_to_pil_image(gt) for gt in targets.numpy()]\n",
        "fig = plot_images(pil_targets)\n",
        "_ = fig.suptitle(\"Ground truths\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d8ffe212-798a-4ee7-9f9f-f3ce99283f66",
      "metadata": {
        "id": "d8ffe212-798a-4ee7-9f9f-f3ce99283f66"
      },
      "source": [
        "Now, let's evaluate the model on the entire set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f92deea8-818e-4dfb-bb61-ce952f86188b",
      "metadata": {
        "id": "f92deea8-818e-4dfb-bb61-ce952f86188b"
      },
      "outputs": [],
      "source": [
        "# For the test metric\n",
        "!pip install torchmetrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c1cc4832-79eb-403e-a665-ecb8371b75e2",
      "metadata": {
        "id": "c1cc4832-79eb-403e-a665-ecb8371b75e2"
      },
      "outputs": [],
      "source": [
        "import torchmetrics\n",
        "IoU = torchmetrics.JaccardIndex(num_classes=21, ignore_index=255,task=\"multiclass\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4bffe4f6-c159-4e3d-9200-47c2afa85d9f",
      "metadata": {
        "id": "4bffe4f6-c159-4e3d-9200-47c2afa85d9f",
        "nbgrader": {
          "grade": true,
          "grade_id": "cell-d0d00b1251df1c27",
          "locked": false,
          "points": 0,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "tags": []
      },
      "source": [
        "Q4: Jaccard Index is used instead of accuracy. How is it defined? What is its other name? What is its advantage?\n",
        "\n",
        "\n",
        "Jaccard Index, also known as Intersection over Union (IoU), measures the overlap between two sets (predicted and observed), calculated as the ratio of the intersection to the union.\n",
        "\n",
        "   $$IoU = \\frac{\\text{Area of Intersection}}{\\text{Area of Union}}$$\n",
        "\n",
        "In a multiclass context, this value is computed for each class and averaged.\n",
        "Jaccard Index is suitable for imbalanced classes, and insensitive to absolute size because it focuses on relative overlap.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1ccfd8be-aeda-4d7a-8f3a-7c97103f4caf",
      "metadata": {
        "id": "1ccfd8be-aeda-4d7a-8f3a-7c97103f4caf"
      },
      "source": [
        "Q5: Modify the following code to obtain an average IoU over the entire set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0e346cbb-87b2-4481-9856-f12cd989f20f",
      "metadata": {
        "id": "0e346cbb-87b2-4481-9856-f12cd989f20f"
      },
      "outputs": [],
      "source": [
        "fcn = fcn.to(device)\n",
        "fcn.eval()\n",
        "nbatch = 0\n",
        "sum_batch_IoU = 0\n",
        "for i, (inputs, targets) in enumerate(test_loader):\n",
        "  targets = targets.squeeze(dim=1)\n",
        "  inputs = inputs.to(device)\n",
        "  targets = targets.to(device)\n",
        "  outputs = fcn(inputs)['out']\n",
        "\n",
        "  outputs = outputs.cpu()\n",
        "  targets = targets.cpu()\n",
        "  batch_IoU = IoU(outputs, targets)\n",
        "  nbatch +=1\n",
        "  sum_batch_IoU += batch_IoU\n",
        "  print(batch_IoU)\n",
        "\n",
        "print('mean IoU :', sum_batch_IoU/nbatch)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cxyeO2UYIVy6",
      "metadata": {
        "id": "cxyeO2UYIVy6"
      },
      "source": [
        "### **Exercise 2: Perfectly Supervised Denoising**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "MyxF-5a-ITtd",
      "metadata": {
        "id": "MyxF-5a-ITtd"
      },
      "source": [
        "In a perfectly supervised denoising problem, we have two sets of images $B_{noisy}$ and $B_{clean}$.\n",
        "Each clean image in $B_{clean}$ corresponds to a noisy version in $B_{noisy}$. \\\\\n",
        "In this exercise, we will train an FCN to transform the noisy version into the clean one. To avoid using real data that would take up space on the drive, we work on synthetic images generated on the fly. We are thus in the ideal scenario where the datasets are infinitely large, and overfitting is not a concern.\n",
        "In this context, data augmentation and validation steps are unnecessary, simplifying the training procedures.\n",
        "The following cells visualize data from $B_{noisy}$ and their corresponding \"clean\" counterparts. The function $gen$ (in the module  **utile.py**) samples batches on the fly."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c81b943d-4b2f-4314-9f4a-d53631db4cce",
      "metadata": {
        "id": "c81b943d-4b2f-4314-9f4a-d53631db4cce"
      },
      "outputs": [],
      "source": [
        "! git clone https://github.com/nanopiero/ML_S5_etudiants\n",
        "! cp ML_S5_etudiants/P4/src/utile.py .\n",
        "from utile import gen, voir_batch2D"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5bb7367e-e140-4fb2-ad5a-97904a6a56dd",
      "metadata": {
        "id": "5bb7367e-e140-4fb2-ad5a-97904a6a56dd"
      },
      "outputs": [],
      "source": [
        "input, target = gen(6)\n",
        "\n",
        "# Noisy versions (filled and noisy rectangles)\n",
        "fig0 = plt.figure(0, figsize=(36, 6))\n",
        "voir_batch2D(input, 6, fig0, k=0, min_scale=0, max_scale=1)\n",
        "\n",
        "# Clean versions (only cells)\n",
        "fig1 = plt.figure(1, figsize=(36, 6))\n",
        "voir_batch2D(target, 6, fig1, k=0, min_scale=0, max_scale=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0d62c467-2edd-4fae-b73b-a75dc66a2d47",
      "metadata": {
        "id": "0d62c467-2edd-4fae-b73b-a75dc66a2d47"
      },
      "source": [
        "The task is to remove the rectangles (filled or not) from the image. It is a well-posed but a priori challenging problem: spatial context is necessary to achieve it. To address this, we will use another popular FCN: a simplified [U-Net](https://arxiv.org/abs/1505.04597) (see utile.py)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d063992f-8cb0-4786-ad30-11766a9ce0f3",
      "metadata": {
        "id": "d063992f-8cb0-4786-ad30-11766a9ce0f3"
      },
      "outputs": [],
      "source": [
        "ch_in = 1\n",
        "ch_out = 1\n",
        "size = 16\n",
        "\n",
        "fcn = UNet(ch_in, ch_out, size).to(device)  # 1 input channel, 1 output channel, network size parameter: 16"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b60d163b-0fdd-49f1-ae76-6e889815aceb",
      "metadata": {
        "id": "b60d163b-0fdd-49f1-ae76-6e889815aceb"
      },
      "source": [
        "**Q1:** How many convolution layers does this U-Net contain? How many weights in total when $size = 16$?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "74504800-b929-4f8f-9f8b-6f19d45110d7",
      "metadata": {
        "id": "74504800-b929-4f8f-9f8b-6f19d45110d7"
      },
      "outputs": [],
      "source": [
        "#print(fcn)\n",
        "i=0\n",
        "for module in fcn.modules():\n",
        "    if isinstance(module, nn.Conv2d):\n",
        "      i+=1\n",
        "\n",
        "print(i)\n",
        "\n",
        "pconv=0\n",
        "pconvt=0\n",
        "pbn=0\n",
        "ptot=0\n",
        "\n",
        "for module in fcn.modules():\n",
        "    if isinstance(module, nn.Conv2d):\n",
        "      for parameter in module.parameters():\n",
        "        pconv += torch.numel(parameter) #numel: counts the number of elements in a matrix\n",
        "\n",
        "    if isinstance(module, nn.ConvTranspose2d):\n",
        "      for parameter in module.parameters():\n",
        "        pconvt += torch.numel(parameter) #numel: counts the number of elements in a matrix\n",
        "\n",
        "    if isinstance(module, nn.BatchNorm2d):\n",
        "      for parameter in module.parameters():\n",
        "        pbn += torch.numel(parameter)\n",
        "\n",
        "print(pconv)  # number of weights in convolution layers\n",
        "print(pconvt) #  \" \" in transpose convolution layers\n",
        "print(pbn)    # number of weights in BatchNorm layers\n",
        "\n",
        "print(pconv + pconvt + pbn)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "283f11bf-270a-46e3-9072-697c4e2a45f3",
      "metadata": {
        "id": "283f11bf-270a-46e3-9072-697c4e2a45f3"
      },
      "source": [
        "**Q2:** For an input image with spatial dimensions $64\\times 64$, what is the dimension of the intermediate feature map?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ae5133a9-e427-41c3-a95e-166e07f858bc",
      "metadata": {
        "id": "ae5133a9-e427-41c3-a95e-166e07f858bc"
      },
      "outputs": [],
      "source": [
        "fcn"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e9b2a441-eb46-41a0-a2de-5d67cd17cc23",
      "metadata": {
        "id": "e9b2a441-eb46-41a0-a2de-5d67cd17cc23"
      },
      "source": [
        "The successive convolutions preserve the dimensions (due to \"padding\" - a layer of zeros on each side). The four successive maxpooling operations reduce the spatial dimensions to $4 \\times 4$."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "430736ae-866d-48d5-bc3f-bbe05c5ccde2",
      "metadata": {
        "id": "430736ae-866d-48d5-bc3f-bbe05c5ccde2"
      },
      "source": [
        "**Q3:** How is the multi-scale processing ensured? What is the purpose of \"skip-connections\"?"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "49ffef15-1f8d-443a-bffd-2653662a53a9",
      "metadata": {
        "id": "49ffef15-1f8d-443a-bffd-2653662a53a9",
        "nbgrader": {
          "grade": true,
          "grade_id": "cell-a6df0366b6e381d7",
          "locked": false,
          "points": 0,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "tags": []
      },
      "source": [
        "### BEGIN SOLUTION\n",
        "\n",
        "In the forward function of the network (see utile.py), the feature maps calculated during the compression phase are directly reinjected into the inputs of the final modules, by concatenation. These short-circuits (\"skip-connections\") allow the mixing of low-level, local information with high-level, more global information. According to sources, these skip-connections improve the resolution of the output image and facilitate the backpropagation of gradients to the early layers of the network.\n",
        "\n",
        "### END SOLUTION"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cb01a417-5cf3-4397-8170-67b329898019",
      "metadata": {
        "id": "cb01a417-5cf3-4397-8170-67b329898019"
      },
      "source": [
        "**Q4:** Define a simple training procedure (no validation) using Mean Square Error (MSE) as the cost function and Adam as the optimization method (learning rate of 0.001). Once the training procedure is ready, associate a GPU with the notebook and ensure that the denoising task is well learned."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "59ca7773-6945-41e9-bc4f-d853386f7c46",
      "metadata": {
        "id": "59ca7773-6945-41e9-bc4f-d853386f7c46"
      },
      "outputs": [],
      "source": [
        "fcn = fcn.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "08555c72-b5db-4b3a-966d-a7a5e51ea08a",
      "metadata": {
        "id": "08555c72-b5db-4b3a-966d-a7a5e51ea08a"
      },
      "outputs": [],
      "source": [
        "def criterion(output, target):\n",
        "    return torch.mean((output - target)**2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0c45b419-f1f6-4dad-bb09-661d7ab0a451",
      "metadata": {
        "id": "0c45b419-f1f6-4dad-bb09-661d7ab0a451"
      },
      "outputs": [],
      "source": [
        "import torch.optim as optim\n",
        "optimizer = optim.Adam(fcn.parameters(), 10**(-4))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cc47f885-862f-4205-a316-6045aea86aa8",
      "metadata": {
        "id": "cc47f885-862f-4205-a316-6045aea86aa8",
        "nbgrader": {
          "grade": false,
          "grade_id": "cell-631d663900b2f330",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "tags": []
      },
      "outputs": [],
      "source": [
        "nepochs = 40  # brew a tea while waiting\n",
        "nbatches = 100  # Number of batches per epoch\n",
        "batchsize = 32  # Batch size\n",
        "\n",
        "train_losses = []  # List to store training losses\n",
        "\n",
        "fcn = fcn.to(device)\n",
        "\n",
        "for epoch in range(nepochs):\n",
        "    print(f\"Epoch {epoch + 1}/{nepochs}\")\n",
        "\n",
        "    epoch_losses = []  # List to store losses for each batch in the epoch\n",
        "\n",
        "    for i in range(nbatches):\n",
        "        # Load inputs\n",
        "        inputs, targets = gen(batchsize)\n",
        "        ### BEGIN SOLUTION\n",
        "        # Move data to the GPU\n",
        "        inputs = inputs.to(device)\n",
        "        targets = targets.to(device)\n",
        "\n",
        "        optimizer.zero_grad()  # Zero the gradients\n",
        "        outputs = fcn(inputs)  # Forward pass\n",
        "        loss = criterion(outputs, targets)  # Compute the loss\n",
        "        loss.backward()  # Backward pass\n",
        "        optimizer.step()  # Update the weights\n",
        "\n",
        "        epoch_losses.append(loss.detach().cpu().item())  # Append the loss\n",
        "        ### END SOLUTION\n",
        "    epoch_loss = np.mean(epoch_losses)\n",
        "    train_losses.append(epoch_loss)\n",
        "\n",
        "    print(f'Epoch loss: {epoch_loss:.4f}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "81b82258-13cc-4015-bbc8-33d59eccb9bb",
      "metadata": {
        "id": "81b82258-13cc-4015-bbc8-33d59eccb9bb"
      },
      "outputs": [],
      "source": [
        "plt.plot(train_losses)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "de7ef0ee-0cf3-402b-a41e-631b1836d9e4",
      "metadata": {
        "id": "de7ef0ee-0cf3-402b-a41e-631b1836d9e4",
        "nbgrader": {
          "grade": false,
          "grade_id": "cell-1b71282773a481aa",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        }
      },
      "outputs": [],
      "source": [
        "#visualization:\n",
        "\n",
        "fcn.eval()\n",
        "\n",
        "target = make_batch(6, rec=0., noisy_rec=0., disc=0.002).to(device)\n",
        "noise = make_batch(6, rec=0.0003, noisy_rec=0.0003, disc=0.).to(device)\n",
        "input = target + noise\n",
        "\n",
        "# output =\n",
        "\n",
        "### BEGIN SOLUTION\n",
        "output = fcn(input)\n",
        "\n",
        "fig = plt.figure(0, figsize=(36, 6))  # first row: inputs\n",
        "voir_batch2D(input.cpu(), 6, fig, k=0, min_scale=0, max_scale=1)\n",
        "fig2 = plt.figure(1, figsize=(36, 6))  # second row: ground truth\n",
        "voir_batch2D(target.detach().cpu(), 6, fig2, k=0, min_scale=0, max_scale=1)\n",
        "fig3 = plt.figure(2, figsize=(36, 6))  # last row: outputs\n",
        "voir_batch2D(output.detach().cpu(), 6, fig2, k=0, min_scale=0, max_scale=1)\n",
        "### END SOLUTION"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}