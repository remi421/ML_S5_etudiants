{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Practical session n°4\n",
        "\n",
        "Notions:\n",
        "- Semantic segmentation\n",
        "- Intersection over Union\n",
        "- Image Denoising\n",
        "- Fully convolutional networks, U-Net\n",
        "- Weak supervision (in part II): The noise-to-noise and the Neural Eggs Separation scenarios.\n",
        "\n",
        "Duration: 2 h + 2 h\n",
        "\n",
        "In P3, we illustrated how Convolutional Neural Networks (CNNs) are trained for image classification tasks. In this practical session, we demonstrate how to achieve pixel-level predictions for tasks like semantic segmentation and image denoising.\n",
        "\n",
        "To start, we’ll simply apply an off-the-shelf model. Then, we’ll focus on training a model from scratch (part I, exercise 2 and part II).\n",
        "\n",
        "In P3, we also introduced a crucial set of methods known as \"transfer learning,\" which is particularly effective when there’s limited training data. In this session, we’ll explore another equally important set of methods called \"weak supervision,\" which is well-suited for cases where ground truth is imperfectly known (Part II)."
      ],
      "metadata": {
        "id": "MCN6FPP8tPo-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part II: denoising with FCN - weakly-supervised approaches\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "EpYCZBnPH0RL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import os\n",
        "import time\n",
        "\n",
        "import torch\n",
        "import torchvision\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import datasets, models, transforms\n",
        "import torch.optim as optim\n",
        "from PIL import Image"
      ],
      "metadata": {
        "id": "iqp_Y4AhIqKZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "First, import the functions contained in the \"utile\" module."
      ],
      "metadata": {
        "id": "Wpu3GQiDAX0I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! git clone https://github.com/nanopiero/ML_S5_etudiants\n",
        "! cp ML_S5_etudiants/practicals/P4/utile.py .\n",
        "from utile import gen_noise2noise, voir_batch2D"
      ],
      "metadata": {
        "id": "ShfsOSzse6-H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**Exercise 1: The \"noise to noise\" scenario**\n",
        "\n",
        "In Exercise 2, Part I, we had pairs (noisy version, clean version) to learn from. However, in real-world applications, clean versions are barely available. \\\n",
        "Sometimes, there are instances where multiple noisy versions are accessible. Consider, for example, photos taken with a telephoto lens with several seconds between shots: the effects of air turbulence on image quality are independent from one image to another. \\\n",
        "In the \"noise to noise\" scenario [(Lehtinen,2018)](https://arxiv.org/pdf/1803.04189.pdf), we have pairs of noisy images, organized into two sets $B^1_{noisy}$ and $B^2_{noisy}$.\n",
        "To situate ourselves in this scenario, we utilize the synthetic images from Exercise 1. The *gen_noise2noise* function allows us to sample from both datasets:"
      ],
      "metadata": {
        "id": "xypzPxQjBDzR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "image1, image2 = gen_noise2noise(6)\n",
        "fig = plt.figure(0, figsize=(36, 6))\n",
        "voir_batch2D(image1, 6, fig, k=0, min_scale=0,max_scale=1)\n",
        "\n",
        "fig2 = plt.figure(1, figsize=(36, 6))\n",
        "voir_batch2D(image2.detach().cpu(), 6, fig2, k=0, min_scale=0,max_scale=1)"
      ],
      "metadata": {
        "id": "2WAABDUPBDBr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q1:** Define a learning procedure that effectively denoises the image."
      ],
      "metadata": {
        "id": "55CM4Dt_r5o4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fcn = UNet(1,1,16).cuda()\n",
        "\n",
        "import torch.optim as optim\n",
        "optimizer = optim.Adam(fcn.parameters(), 10**(-3))"
      ],
      "metadata": {
        "id": "NvCsUxP9QF0u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nepochs = 40\n",
        "nbatches = 100\n",
        "batchsize = 64\n",
        "\n",
        "..."
      ],
      "metadata": {
        "id": "57CKkJpoCm-O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# viz:\n",
        "fcn.eval()\n",
        "\n",
        "input, target = gen_noise2noise(6)\n",
        "\n",
        "output = fcn(input.cuda())\n",
        "\n",
        "fig = plt.figure(0, figsize=(36, 6))\n",
        "voir_batch2D(input, 6, fig, k=0, min_scale=0,max_scale=1)  # inputs\n",
        "fig2 = plt.figure(1, figsize=(36, 6))\n",
        "voir_batch2D(target, 6, fig2, k=0, min_scale=0,max_scale=1)  # targets\n",
        "fig3 = plt.figure(2, figsize=(36, 6))\n",
        "voir_batch2D(output.detach().cpu(), 6, fig2, k=0, min_scale=0,max_scale=1) # outputs\n"
      ],
      "metadata": {
        "id": "EIEkZ8U1RJo9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q2:** Theoretical analysis:\n",
        "\n",
        "The training procedure involves searching for the weights $\\theta^*$ that satisfy:\n",
        "\n",
        "\\begin{equation}\n",
        "\\theta^* = \\underset{\\theta}{argmin} {\\big \\{} \\mathbb{E}_{(X,Y)}[ \\ \\mathcal{L} (f_\\theta(X), Y) \\ ] {\\big \\}} \\tag{1}\n",
        "\\end{equation}\n",
        "\n",
        "Rewrite $\\mathbb{E}_{(X,Y)}[ \\mathcal{L} (f_\\theta(X), Y) ]$ to justify the approach taken for **Q1**.\n",
        "\n"
      ],
      "metadata": {
        "id": "Qf3GTjMhsrQn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "...\n"
      ],
      "metadata": {
        "id": "HUgkSArnkXeK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**Exercise 2: The \"NES\" scenario (Neural Egg Separation)**\n",
        "\n",
        "In another scenario, two sources of images can be sampled independently. The first provides noisy images containing the object of interest. Images from the second source contain only noise. Additionally, the corruption process, i.e., how the noise is combined with the clean image, is assumed to be known. This is a relatively common scenario. For instance, in the case of meteorological radar images, it is possible to extract images containing only noise during non-precipitation periods.\n",
        "\n",
        "This scenario was defined by [Halperin et al (2018)](https://arxiv.org/pdf/1811.12739.pdf). These authors propose an iterative method to address it (Neural Egg Separation). In this exercise, we will rely on a fairly similar principle, still using our synthetic images.\n",
        "\n",
        "The functions gen1_NES and gen2_NES allow sampling from the two sources. The corruption process is trivial: the noise (rectangles) is simply added to the clean image (cells).   \n",
        "\n"
      ],
      "metadata": {
        "id": "qeHiaJE5O44_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "image1 = gen1_NES(6)\n",
        "noise = gen2_NES(6)\n",
        "\n",
        "# sample noisy pictures:\n",
        "fig = plt.figure(0, figsize=(36, 6))\n",
        "voir_batch2D(image1, 6, fig, k=0, min_scale=0,max_scale=1)\n",
        "\n",
        "# sample noise:\n",
        "fig2 = plt.figure(1, figsize=(36, 6))\n",
        "voir_batch2D(noise, 6, fig2, k=0, min_scale=0,max_scale=1)"
      ],
      "metadata": {
        "id": "NR-X2s_iQdW1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q1:** Propose a learning strategy that leads to effective denoising.\n",
        "\n",
        "**Q2:** Discuss the limitations of the method and mention some avenues for improvement."
      ],
      "metadata": {
        "id": "qn6-Nsd030qO"
      }
    }
  ]
}