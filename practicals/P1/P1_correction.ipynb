{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "b1e4b37a",
      "metadata": {
        "id": "b1e4b37a"
      },
      "source": [
        "# **Practical session n°1**\n",
        "\n",
        "Notions:\n",
        "- Learning a perceptron through stochastic gradient descent.\n",
        "- Multi-layer perceptron.\n",
        "- Cost function adapted to classification tasks.\n",
        "\n",
        "Duration: 2 h\n",
        "\n",
        "This practical introduces neural networks through a presentation of the perceptron. It is also an opportunity to familiarize yourself with PyTorch commands. PyTorch is one of the three most widely used libraries for deep learning, along with Keras and TensorFlow (Keras is built on top of Tensorflow).\n",
        "\n",
        "\"Deep learning\" is, by definition, the learning of \"deep\" neural networks through stochastic gradient descent. By \"deep,\" we mean networks composed of a succession of \"layers\" of neurons.\n",
        "\n",
        "The basic building blocks that allow us to construct these layers are coded in the torch.nn module."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b88495c8",
      "metadata": {
        "id": "b88495c8"
      },
      "outputs": [],
      "source": [
        "from matplotlib import pyplot as plt\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "torch.manual_seed(1)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1b0dcd41",
      "metadata": {
        "id": "1b0dcd41"
      },
      "source": [
        "## **1. A simple binary classification problem**\n",
        "\n",
        "First, let's revisit the perceptron. To introduce learning through stochastic gradient descent, we will address a simple binary classification problem."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b72725a8",
      "metadata": {
        "id": "b72725a8"
      },
      "outputs": [],
      "source": [
        "def BinaryClassProblem(n=100, std=0.5, manual_seed=1):\n",
        "  torch.manual_seed(manual_seed)\n",
        "  # Data to separate:\n",
        "  n = 100\n",
        "  std = 0.5\n",
        "\n",
        "  # Sample 1:\n",
        "  mean0 = torch.tensor([-1., -1.])\n",
        "  ech0 = mean0 + std * torch.randn(n, 2)\n",
        "\n",
        "  # Sample 2:\n",
        "  mean1 = torch.tensor([1., 1.])\n",
        "  ech1 = mean1 + std * torch.randn(n, 2)\n",
        "\n",
        "  echs = [ech0, ech1]\n",
        "\n",
        "  # Scatter plot:\n",
        "  fig, ax = plt.subplots()\n",
        "  ax.axis([-3, 3, -3, 3])\n",
        "\n",
        "  plt.title('data')\n",
        "  colors = ['b', 'r']\n",
        "  labels = ['class 0', 'class 1']\n",
        "\n",
        "  for i, ech in enumerate(echs):\n",
        "      x, y = ech.numpy()[:, 0], ech.numpy()[:, 1]\n",
        "      ax.scatter(x, y, color=colors[i])\n",
        "\n",
        "  plt.legend(labels)\n",
        "  return echs, ax\n",
        "\n",
        "echs, ax = BinaryClassProblem(n=100)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **2. A simple perceptron**"
      ],
      "metadata": {
        "id": "FFlMZsEWCxvZ"
      },
      "id": "FFlMZsEWCxvZ"
    },
    {
      "cell_type": "markdown",
      "id": "43354804",
      "metadata": {
        "id": "43354804"
      },
      "source": [
        "A simple perceptron (single neuron) consists of two parts: a linear part containing a dot product and a \"bias\" (b) and a non-linear part, the activation function (A):\n",
        "\\begin{equation*}\n",
        " f(x; \\omega,b) = \\mathcal{A}({\\sum} \\omega_i x_i  + b )  \\tag{1}\n",
        "\\end{equation*}\n",
        "\n",
        "The class P1 below codes for perceptrons defined on $\\mathbb{R}^2$ and whose activation function is a sigmoid:\n",
        "\\begin{equation*}\n",
        "\\mathcal{A}(y) = \\dfrac{1}{1+e^{-y}}\n",
        "\\end{equation*}\n",
        "The sigmoid function is in the range [0, 1]. Therefore, the neuron's output can be interpreted as the probability of belonging to the first of the two classes. In the P1 class, the neuron actually returns a vector of \"probabilities\":"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "f8986f9b",
      "metadata": {
        "id": "f8986f9b"
      },
      "outputs": [],
      "source": [
        "class P1(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super(P1, self).__init__()\n",
        "        self.fc = nn.Linear(2, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Dot product and bias\n",
        "        x = self.fc(x)\n",
        "        # Activation\n",
        "        x = x.sigmoid()\n",
        "        # Vector of \"probabilities\" (cat: concatenation)\n",
        "        x = torch.cat((x, 1 - x), dim=1)\n",
        "        return x\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4ca91dfe",
      "metadata": {
        "id": "4ca91dfe"
      },
      "source": [
        "In the next cell, basic commands are given to :\n",
        "- define an instance of P1\n",
        "- apply it on a 2D torch.tensor\n",
        "- access the weights (or \"parameters\") of model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1dd27f40",
      "metadata": {
        "id": "1dd27f40"
      },
      "outputs": [],
      "source": [
        "model = P1()\n",
        "\n",
        "# Apply model on a 2D tensor:\n",
        "inputs = torch.tensor([[0.,1.]])\n",
        "print(f'example of output: {model(inputs)}')\n",
        "\n",
        "# Setting weights:\n",
        "model.fc.weight[0, 0].data.fill_(-0.1)\n",
        "model.fc.weight[0, 1].data.fill_(0.5)\n",
        "model.fc.bias.data.fill_(-1)\n",
        "\n",
        "# Retrieving weights:\n",
        "fc = model.fc\n",
        "weights = fc.weight.data.squeeze(dim=0)\n",
        "bias = fc.bias.data\n",
        "\n",
        "print(f'weights: {weights}')\n",
        "print(f'bias: {bias}')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "417aad92",
      "metadata": {
        "id": "417aad92"
      },
      "source": [
        "**Exercise 1**:\n",
        "- For the given set of parameters (-0.1, 0.5, and -1), find an input that produces an output of (0.5, 0.5). Verify it in a single line of code.\n",
        "- For the given input, find parameters that will produce an output of (0.5, 0.5). Verify it in a single line of code.  \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cf22a4d2",
      "metadata": {
        "deletable": false,
        "id": "cf22a4d2",
        "nbgrader": {
          "cell_type": "code",
          "checksum": "5f61797496e6ebc3f98ba5ea49e10e07",
          "grade": false,
          "grade_id": "exercise-1",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "tags": []
      },
      "outputs": [],
      "source": [
        "# YOUR CODE HERE\n",
        "inputs0 = torch.tensor([[-10.,0.]])\n",
        "print(f'output: {model(inputs0)}')\n",
        "\n",
        "model.fc.weight[0, 0].data.fill_(0.)\n",
        "model.fc.weight[0, 1].data.fill_(0.)\n",
        "model.fc.bias.data.fill_(0)\n",
        "print(f'output: {model(inputs)}')\n",
        "#"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "70518cbf",
      "metadata": {
        "id": "70518cbf"
      },
      "source": [
        "To complete the definition of the perceptron, a decision rule is needed. This rule is natural: for $f(x; \\omega, b) = (p_0, p_1)$, we choose class 0 if $p_0 > p_1$."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Exercise 2:**\n",
        "When the input space is 2D or 3D, the boundary that delimits the model's decision regions could be drawn. Complete the code below to plot it."
      ],
      "metadata": {
        "id": "y7Okk0afzYQ9"
      },
      "id": "y7Okk0afzYQ9"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6c2446c6",
      "metadata": {
        "deletable": false,
        "id": "6c2446c6",
        "nbgrader": {
          "cell_type": "code",
          "checksum": "7f1ca095ac2551b73a3153c32ea2ff07",
          "grade": false,
          "grade_id": "exercise-2",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "tags": []
      },
      "outputs": [],
      "source": [
        "def drawDecisionBoundary(model, ax, interval=[-10, 10], color='black'):\n",
        "    x0 = np.arange(interval[0], interval[1], 0.01)\n",
        "    # YOUR CODE HERE\n",
        "    w0 = model.fc.weight[0, 0].data\n",
        "    w1 = model.fc.weight[0, 1].data\n",
        "    b = model.fc.bias.data\n",
        "    x1 = -b/w0 - w1/w0 * x0\n",
        "    #\n",
        "\n",
        "    ax.plot(x0, x1, color=color)\n",
        "\n",
        "\n",
        "echs, ax = BinaryClassProblem()\n",
        "drawDecisionBoundary(model, ax)\n",
        "# ax.set_xlim([-3, 3])\n",
        "# ax.set_ylim([-3, 3])\n",
        "# Display the plot\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **3. Dataset and Dataloader**"
      ],
      "metadata": {
        "id": "RNSM_P4_CONZ"
      },
      "id": "RNSM_P4_CONZ"
    },
    {
      "cell_type": "markdown",
      "id": "b81542b9",
      "metadata": {
        "id": "b81542b9"
      },
      "source": [
        "To train the perceptron to correctly separate the classes, we will use stochastic gradient descent with mini-batches. For this, we need to present the model with (*input*, *target*) pairs in a **random** order. In PyTorch, this selection is done using two objects:\n",
        "- A *Dataset* class\n",
        "- A *Dataloader* class"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "923d8715",
      "metadata": {
        "id": "923d8715"
      },
      "source": [
        "A PyTorch **Dataset** contains a method for accessing data. The following class provides a rudimentary example. Later on, we will integrate data loading, normalization, and data augmentation steps into our datasets:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "00f26701",
      "metadata": {
        "id": "00f26701"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import Dataset, DataLoader\n",
        "class FirstDataset(Dataset):\n",
        "    def __init__(self, inputs, targets):\n",
        "        self.inputs = inputs\n",
        "        self.targets = targets\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.targets.shape[0]\n",
        "\n",
        "    def __getitem__(self, idx):  # idx is an index called by the loader\n",
        "        x = self.inputs[idx, :]\n",
        "        t = self.targets[idx]\n",
        "        return x, t  # (input, target) pair\n",
        "\n",
        "inputs_train = torch.cat(echs, dim=0)\n",
        "targets_train = torch.cat((torch.zeros(n), torch.ones(n)), dim=0).long()\n",
        "ds1 = FirstDataset(inputs_train, targets_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "96e95810",
      "metadata": {
        "id": "96e95810"
      },
      "source": [
        "A **loader** is a Python iterable (like lists, dictionaries, etc.) that we parameterize by batch size and data selection method (with or without replacement, weighting, etc). In particular, with the *shuffle=True* option, the data is reshuffled at every epoch.\n",
        "\n",
        "The option *num_workers* represents how many subprocesses to use for data loading, 0 means that the data will be loaded in the main process (generally interesting to anticipate data loading, to speed up training)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6fee1e46",
      "metadata": {
        "id": "6fee1e46"
      },
      "outputs": [],
      "source": [
        "loader1 = DataLoader(ds1, batch_size=10, shuffle=True, num_workers=0)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "14640f6a",
      "metadata": {
        "id": "14640f6a"
      },
      "source": [
        "In the following figure, we represent a first randomly drawn batch of points. Each time the window is executed, a new batch of points is drawn until exhaustion:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ebbf4375",
      "metadata": {
        "id": "ebbf4375"
      },
      "outputs": [],
      "source": [
        "fig2 = plt.figure()\n",
        "ax2 = fig2.add_subplot(111)\n",
        "ax2.axis([-3, 3, -3, 3])\n",
        "\n",
        "# Drawing a batch of ten points\n",
        "inputs, targets = next(iter(loader1))\n",
        "x, y = inputs.numpy()[:, 0], inputs.numpy()[:, 1]\n",
        "cs = [colors[targets[i]] for i in range(len(targets))]\n",
        "ax2.scatter(x, y, color=cs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5e3051a2",
      "metadata": {
        "id": "5e3051a2"
      },
      "outputs": [],
      "source": [
        "# Iterating over the dataset\n",
        "inputs, targets = next(iter(loader1))\n",
        "x, y = inputs.numpy()[:, 0], inputs.numpy()[:, 1]\n",
        "cs = [colors[targets[i]] for i in range(len(targets))]\n",
        "ax2.scatter(x, y, color=cs)\n",
        "fig2"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **4. A Loss function for Classification. Updating the weights.**"
      ],
      "metadata": {
        "id": "S0p6Oca9DIIR"
      },
      "id": "S0p6Oca9DIIR"
    },
    {
      "cell_type": "markdown",
      "id": "4a2dcbbb",
      "metadata": {
        "id": "4a2dcbbb"
      },
      "source": [
        "For each available batch, we calculate the model's error using a loss function. This loss function penalizes the differences between the network's outputs (here, pairs $(p_0, p_1)$) and the ground truth (here, a class $c \\in \\{ 0 ; 1 \\}$).\n",
        "\n",
        "In classification, we generally use the negative log likelihood. For a batch point, it is defined as:\n",
        "\n",
        "$\\mathcal{L}((p_0, p_1), c) = - ln(p_c)$\n",
        "\n",
        "This quantity is averaged over each batch:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "3f24c290",
      "metadata": {
        "id": "3f24c290"
      },
      "outputs": [],
      "source": [
        "def loss_fn(outputs, targets, show=False):\n",
        "    # all -log(p)\n",
        "    outputs = -torch.log(outputs)\n",
        "    # all -log(pc)\n",
        "    tensor_of_losses = torch.gather(outputs, 1, targets.unsqueeze(dim=1))\n",
        "    # average of -log(pc)\n",
        "    loss = tensor_of_losses.mean()\n",
        "\n",
        "    if show:\n",
        "        print(outputs)\n",
        "        print(targets)\n",
        "        print(tensor_of_losses)\n",
        "\n",
        "    return loss"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6d7ca365",
      "metadata": {
        "id": "6d7ca365"
      },
      "source": [
        "Example:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2b7a632e",
      "metadata": {
        "id": "2b7a632e"
      },
      "outputs": [],
      "source": [
        "inputs, targets = next(iter(loader1))\n",
        "outputs = model(inputs)\n",
        "l = loss_fn(outputs, targets, show=True)\n",
        "\n",
        "print(l)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "409c9f11",
      "metadata": {
        "id": "409c9f11"
      },
      "source": [
        "For each batch, we calculate the derivatives $\\dfrac{\\partial \\mathcal{L_{batch}}}{\\partial{\\omega_i}}$ where $\\mathcal{L_{batch}}$ is the average of the cost function over the batch.\n",
        "\n",
        "PyTorch keeps track of each operation performed with the weights so that it can apply the usual rules of derivation. This calculation is launched with the *backward* method. The derivatives are stored with the weights and can be accessed with *.grad*:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "021822c0",
      "metadata": {
        "id": "021822c0"
      },
      "outputs": [],
      "source": [
        "w = model.fc.weight  # [0,0]\n",
        "print('before backward:' + str(w.grad))\n",
        "\n",
        "l.backward()\n",
        "print('after backward:' + str(w.grad))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "85922688",
      "metadata": {
        "id": "85922688"
      },
      "source": [
        "Last step: updating the weights. For this, many methods are available. We specify the chosen method through the PyTorch \"optimizer\" object. The simplest is written:\n",
        "\n",
        "$w_i := w_i - lr \\times \\dfrac{\\partial \\mathcal{L_{batch}}}{\\partial{\\omega_i}}$  (2)\n",
        "\n",
        "The learning rate ($lr$) controls the amplitude of the increments:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b612c9d9",
      "metadata": {
        "id": "b612c9d9"
      },
      "outputs": [],
      "source": [
        "lr = 0.1\n",
        "# Two commonly used descent methods:\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=lr)  # corresponds to equation (2)\n",
        "# optimizer = torch.optim.Adam(model.parameters(), lr=lr)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cf9a65cc",
      "metadata": {
        "id": "cf9a65cc"
      },
      "source": [
        "In the next cell, we train the perceptron. With each new execution, the dataset is traversed twice (two \"epochs\"):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5756d65f",
      "metadata": {
        "id": "5756d65f"
      },
      "outputs": [],
      "source": [
        "_, ax = BinaryClassProblem()\n",
        "\n",
        "for epoch in range(5):\n",
        "    print(epoch)\n",
        "    # random traversal of the dataset\n",
        "    for x, targets in loader1:\n",
        "        # zeroing gradients\n",
        "        optimizer.zero_grad()\n",
        "        # calculation of (p0, p1)\n",
        "        output = model(x)\n",
        "        # calculation of the error\n",
        "        l = loss_fn(output, targets)\n",
        "        # calculation of gradients\n",
        "        l.backward()\n",
        "        # weight update\n",
        "        optimizer.step()\n",
        "\n",
        "    # Plotting the hyperplane\n",
        "    drawDecisionBoundary(model, ax)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3f458025",
      "metadata": {
        "id": "3f458025"
      },
      "source": [
        "**Exercise 3**: Complete the following code to plot the gradients in the $\\omega_0, \\omega_1$ weight space:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c7c3b29e",
      "metadata": {
        "deletable": false,
        "id": "c7c3b29e",
        "nbgrader": {
          "cell_type": "code",
          "checksum": "1ee62aef14a34c3e87c4ddd0f5a6d03d",
          "grade": false,
          "grade_id": "exercise-3-1",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "tags": []
      },
      "outputs": [],
      "source": [
        "# Initialization of weights:\n",
        "model.fc.weight[0, 0].data.fill_(-0.1)\n",
        "model.fc.weight[0, 1].data.fill_(0.5)\n",
        "model.fc.bias.data.fill_(-1)\n",
        "\n",
        "fig3 = plt.figure()\n",
        "ax3 = fig3.add_subplot(111)\n",
        "ax3.axis([-1.5, 0, -1, 0.6])\n",
        "\n",
        "loader1 = DataLoader(ds1, batch_size=10, shuffle=True)\n",
        "lr = 0.5\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n",
        "\n",
        "for epoch in range(25):\n",
        "    for x, label in loader1:\n",
        "        optimizer.zero_grad()\n",
        "        output = model(x)\n",
        "        l = loss_fn(output, label)\n",
        "        l.backward()\n",
        "\n",
        "        # plotting vectors:\n",
        "        # weights = ...\n",
        "        # YOUR CODE HERE\n",
        "        w0 = model.fc.weight[0, 0].data\n",
        "        w1 = model.fc.weight[0, 1].data\n",
        "        b = model.fc.bias.data\n",
        "        ax3.scatter(w0, w1)\n",
        "        # gradients:\n",
        "        # gradient = ...\n",
        "        # YOUR CODE HERE\n",
        "        g0 = model.fc.weight.grad[0, 0]\n",
        "        g1 = model.fc.weight.grad[0, 1]\n",
        "\n",
        "        # ax3.scatter(...)\n",
        "        # ax3.arrow(...)\n",
        "        # YOUR CODE HERE\n",
        "        ax3.arrow(w0.item(), w1.item(),-g0, -g1, head_width=0.05)\n",
        "\n",
        "        optimizer.step()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **5. Scoring a classification model with accuracy.**"
      ],
      "metadata": {
        "id": "YHLWlqaxFs2B"
      },
      "id": "YHLWlqaxFs2B"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Scoring a model is an essential step. In the following exercise, we present a very simple score, accuracy, which consists of counting the proportion of correctly assigned classes. It is calculated on the training set and on an independent set, the **test set**."
      ],
      "metadata": {
        "id": "mJTpmfw5LJry"
      },
      "id": "mJTpmfw5LJry"
    },
    {
      "cell_type": "markdown",
      "id": "757b7141",
      "metadata": {
        "id": "757b7141"
      },
      "source": [
        "**Exercise 4:**\n",
        "- With the help of the torch.max function, determine the accuracy of the classifier on the **training set**.\n",
        "- Sample an independant **test set** and compute the accuracy on it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a9056a67",
      "metadata": {
        "deletable": false,
        "id": "a9056a67",
        "nbgrader": {
          "cell_type": "code",
          "checksum": "08db64521cb6ad3687ba520557dfa517",
          "grade": false,
          "grade_id": "exercise-3-2",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "tags": []
      },
      "outputs": [],
      "source": [
        "### BEGIN SOLUTION\n",
        "inputs_train = torch.cat(echs, dim=0)\n",
        "targets_train = torch.cat((torch.zeros(n), torch.ones(n)), dim=0).long()\n",
        "echs_test, ax = BinaryClassProblem(n=100, std=0.5, manual_seed=10)\n",
        "inputs_test = torch.cat(echs_test, dim=0)\n",
        "targets_test = torch.cat((torch.zeros(n), torch.ones(n)), dim=0).long()\n",
        "\n",
        "hits = torch.max(model(inputs_train), dim=1)[1] == targets_train\n",
        "acc = hits.sum().float() / (2 * n)*100\n",
        "\n",
        "print(f\"Accuracy train:{acc:.2f}%\")\n",
        "\n",
        "\n",
        "hits = torch.max(model(inputs_test), dim=1)[1] == targets_test\n",
        "acc = hits.sum().float() / (2 * n)*100\n",
        "print(f\"Accuracy test:{acc:.2f}%\")\n",
        "drawDecisionBoundary(model, ax)\n",
        "#\n",
        "### END SOLUTION"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c9216817",
      "metadata": {
        "id": "c9216817"
      },
      "source": [
        "**Note:**\n",
        "\n",
        "A model of the class *P1* corresponds to a class of statistical models widely used with predictors of small dimensions: logistic regression.\n",
        "\n",
        "This model is used to **explain and predict** the value of a binary qualitative variable.\n",
        "\n",
        "Let Z be a random variable with values in $\\{c_1, c_2\\}$.\n",
        "Logistic regression with respect to the predictor $X = (X_1, X_2, ...)$ is written:\n",
        "\n",
        "$$ ℙ(Z = c_1 | X ) = \\sigma (\\sum \\omega_i X_i + b ) $$\n",
        "\n",
        "Where $\\sigma$ is the sigmoid function. However, in the context of logistic regression, the weights $\\omega_i$ are obtained by **maximum likelihood**.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **I.6. A more complex binary classification problem**"
      ],
      "metadata": {
        "id": "CNeQuvuPJom6"
      },
      "id": "CNeQuvuPJom6"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0621505f",
      "metadata": {
        "id": "0621505f"
      },
      "outputs": [],
      "source": [
        "# The XOR problem\n",
        "\n",
        "n = 100\n",
        "std = 0.5\n",
        "# sample 0:\n",
        "meana = torch.tensor((-1., -1.))\n",
        "echa = meana + std * torch.randn(n, 2)\n",
        "meanb = torch.tensor((1., 1.))\n",
        "echb = meanb + std * torch.randn(n, 2)\n",
        "\n",
        "ech0 = torch.cat([echa, echb])\n",
        "\n",
        "# sample :\n",
        "meanc = torch.tensor((1., -1.))\n",
        "echc = meanc + std * torch.randn(n, 2)\n",
        "meand = torch.tensor((-1., 1.))\n",
        "echd = meand + std * torch.randn(n, 2)\n",
        "\n",
        "ech1 = torch.cat([echc, echd])\n",
        "\n",
        "echs2 = [ech0, ech1]\n",
        "\n",
        "# Scatter plot:\n",
        "\n",
        "plt.figure(0)\n",
        "plt.axis([-3, 3, -3, 3])\n",
        "\n",
        "plt.title('data')\n",
        "colors = ['b', 'r']\n",
        "labels = ['0', '1']\n",
        "\n",
        "for i, ech in enumerate(echs2):\n",
        "    x, y = ech.numpy()[:, 0], ech.numpy()[:, 1]\n",
        "    plt.scatter(x, y, color=colors[i])\n",
        "\n",
        "plt.legend(labels)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "97d2f4c8",
      "metadata": {
        "id": "97d2f4c8"
      },
      "source": [
        "**Exercise 5:**\n",
        "\n",
        "- What is the approximate best accuracy achievable with a model of class *P1*?\n",
        "\n",
        "- Complete the *P3* class and train a model to achieve an accuracy of at least 90% on the training set.\n",
        "\n",
        "- Why does the score after 50 epochs vary so much with each new training?\n",
        "\n",
        "- Is it possible, with another class of model, to achieve 100% accuracy?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "7d42117e",
      "metadata": {
        "deletable": false,
        "id": "7d42117e",
        "nbgrader": {
          "cell_type": "code",
          "checksum": "3bb7bc5459ad55cefff593dc6268083b",
          "grade": false,
          "grade_id": "exercise-4-1",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "tags": []
      },
      "outputs": [],
      "source": [
        "class P3(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super(P3, self).__init__()\n",
        "        self.fc1 = nn.Linear(2, 2)  # first layer: 2 neurons\n",
        "        self.fc2 = nn.Linear(2, 1)  # second layer: 1 neuron\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.fc1(x)\n",
        "        ### BEGIN SOLUTION\n",
        "        x = x.relu()\n",
        "        x = self.fc2(x)\n",
        "        x = x.sigmoid()\n",
        "        ### END SOLUTION\n",
        "        x = torch.cat((x, 1 - x), dim=1)  # output of sum 1\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "e382771b",
      "metadata": {
        "deletable": false,
        "id": "e382771b",
        "nbgrader": {
          "cell_type": "code",
          "checksum": "7c55c5e939e35858bb4bcf76ce92104a",
          "grade": false,
          "grade_id": "exercise-4-2",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "tags": []
      },
      "outputs": [],
      "source": [
        "# dataset\n",
        "### BEGIN SOLUTION\n",
        "inputs2 = torch.cat(echs2, dim=0)\n",
        "targets2 = torch.cat((torch.zeros(2 * n), torch.ones(2 * n)), dim=0).long()\n",
        "ds2 = FirstDataset(inputs2, targets2)\n",
        "### END SOLUTION\n",
        "loader2 = DataLoader(ds2, batch_size=10, shuffle=True,num_workers=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "470a8232",
      "metadata": {
        "deletable": false,
        "id": "470a8232",
        "nbgrader": {
          "cell_type": "code",
          "checksum": "22064ec3571500c3c1960e0a6877849e",
          "grade": false,
          "grade_id": "exercise-4-3",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        }
      },
      "outputs": [],
      "source": [
        "from tqdm import tqdm # Progess bar\n",
        "lr = 0.1\n",
        "model = P3()\n",
        "# Two commonly used descent methods:\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n",
        "#optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "\n",
        "for epoch in tqdm(range(100)):\n",
        "    for x, labels in loader2:\n",
        "        ### BEGIN SOLUTION\n",
        "        optimizer.zero_grad()\n",
        "        output = model(x)\n",
        "        l = loss_fn(output, labels)\n",
        "        l.backward()\n",
        "        optimizer.step()\n",
        "        ### END SOLUTION\n",
        "print(\"\\nTraining end\")\n",
        "# accuracy\n",
        "### BEGIN SOLUTION\n",
        "hits = torch.max(model(inputs2),dim=1)[1] == targets2\n",
        "acc = hits.sum().float()/(4*n) * 100\n",
        "print(f\"Accuracy:{acc:.2f}%\")\n",
        "### END SOLUTION"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "c5cdbad4",
      "metadata": {
        "deletable": false,
        "id": "c5cdbad4",
        "nbgrader": {
          "cell_type": "code",
          "checksum": "825a438ac87230317226ee974f5052a9",
          "grade": false,
          "grade_id": "exercise-4-4",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "tags": []
      },
      "outputs": [],
      "source": [
        "class P9(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super(P9, self).__init__()\n",
        "        self.fc1 = nn.Linear(2,4) # a first layer with 4 neurons\n",
        "        self.fc2 = nn.Linear(4,4) # a second layer with 4 neurons\n",
        "        self.fc3 = nn.Linear(4,1) # a third layer with 1 neuron\n",
        "    ### BEGIN SOLUTION\n",
        "    def forward(self, x):\n",
        "        # Max pooling over a (2, 2) window\n",
        "        x = self.fc1(x)\n",
        "        x = x.relu()\n",
        "        x = self.fc2(x)\n",
        "        x = x.relu()\n",
        "        x = self.fc3(x)\n",
        "        x = x.sigmoid()\n",
        "        x = torch.cat((x,1-x), dim = 1)\n",
        "        return x\n",
        "    ### END SOLUTION"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e06efc4f",
      "metadata": {
        "deletable": false,
        "id": "e06efc4f",
        "nbgrader": {
          "cell_type": "code",
          "checksum": "f3b4448662a871a03ff6346959cb7ac1",
          "grade": false,
          "grade_id": "exercise-4-5",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "tags": []
      },
      "outputs": [],
      "source": [
        "model = P9()\n",
        "### BEGIN SOLUTION\n",
        "loader2 = DataLoader(ds2,batch_size = 64, shuffle = True, num_workers = 0)\n",
        "\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr = 0.1)\n",
        "\n",
        "for epoch in tqdm(range(500)):\n",
        "    for x, labels in loader2:\n",
        "        optimizer.zero_grad()\n",
        "        output = model(x)\n",
        "        l = loss_fn(output, labels)\n",
        "        l.backward()\n",
        "        optimizer.step()\n",
        "print(\"\\nTraining end\")\n",
        "hits = torch.max(model(inputs2),dim=1)[1] == targets2\n",
        "acc = hits.sum().float()/(4*n)*100\n",
        "### END SOLUTION\n",
        "print(f\"Accuracy:{acc:.2f}%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0a7c04e0",
      "metadata": {
        "id": "0a7c04e0"
      },
      "source": [
        "As the complexity of the model increases, the boundaries can better adapt to the **training set**."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **I.7. A multi-class classification problem**\n",
        "\n",
        "Now let's see how to generalize the approach to multiple classes:"
      ],
      "metadata": {
        "id": "Yy2IWDSGKEdh"
      },
      "id": "Yy2IWDSGKEdh"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "187ff916",
      "metadata": {
        "id": "187ff916",
        "tags": []
      },
      "outputs": [],
      "source": [
        "n = 100\n",
        "std = 0.5\n",
        "# Sample 0:\n",
        "mean0 = torch.tensor((-1., -1.))\n",
        "ech0 = mean0 + std * torch.randn(n, 2)\n",
        "\n",
        "# Sample 1:\n",
        "mean1 = torch.tensor((1., -1.))\n",
        "ech1 = mean1 + std * torch.randn(n, 2)\n",
        "\n",
        "# Sample 2:\n",
        "mean2 = torch.tensor((0., 1.))\n",
        "ech2 = mean2 + std * torch.randn(n, 2)\n",
        "\n",
        "echs3 = [ech0, ech1, ech2]\n",
        "\n",
        "# Scatter plot:\n",
        "plt.figure(0)\n",
        "plt.axis([-3, 3, -3, 3])\n",
        "\n",
        "plt.title('data')\n",
        "colors = ['b', 'r', 'g']\n",
        "labels = ['0', '1', '2']\n",
        "\n",
        "for i, ech in enumerate(echs3):\n",
        "    x, y = ech.numpy()[:, 0], ech.numpy()[:, 1]\n",
        "    plt.scatter(x, y, color=colors[i])\n",
        "\n",
        "plt.legend(labels)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0f32cbc0",
      "metadata": {
        "id": "0f32cbc0"
      },
      "source": [
        "To separate these points, we need to slightly modify the perceptron. To continue using the log-likelihood, the output layer will have as many neurons as classes. Additionally, to define a probability distribution, normalized exponentials are used (softmax function):\n",
        "\n",
        "$p_i = \\dfrac{e^{y_i}}{\\sum{e^{y_j}}}$\n",
        "  \n",
        "Where the $y_i$ are the outputs of the neurons in the last layer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "id": "d320c067",
      "metadata": {
        "id": "d320c067"
      },
      "outputs": [],
      "source": [
        "class P6(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super(P6, self).__init__()\n",
        "        self.fc1 = nn.Linear(2, 2)  # First layer: 2 neurons\n",
        "        self.fc2 = nn.Linear(2, 3)  # Second layer: 3 neurons\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Max pooling over a (2, 2) window\n",
        "        x = self.fc1(x)\n",
        "        x = x.relu()\n",
        "        x = self.fc2(x)\n",
        "        x = x.softmax(dim=1)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "832ccf0e",
      "metadata": {
        "id": "832ccf0e"
      },
      "source": [
        "**Exercise 6:** Create datasets and dataloaders objects and check if it is possible to separate the points (test accuracy > 90%)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "id": "93f403e4",
      "metadata": {
        "deletable": false,
        "id": "93f403e4",
        "nbgrader": {
          "cell_type": "code",
          "checksum": "2098c6d09146eb711fd7f230f1fb2f40",
          "grade": false,
          "grade_id": "exercise-5-1",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "tags": [],
        "outputId": "285be8f6-53ff-4d54-9fae-fff42bca4111",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([300, 2])\n",
            "torch.Size([300])\n",
            "tensor([-0.6264, -0.9780])\n"
          ]
        }
      ],
      "source": [
        "### BEGIN SOLUTION\n",
        "inputs3 = torch.cat(echs3, dim=0)\n",
        "targets3 = torch.cat((torch.zeros(n), torch.ones(n), 2 * torch.ones(n)), dim=0).long()\n",
        "### END SOLUTION\n",
        "print(inputs3.shape)\n",
        "print(targets3.shape)\n",
        "print(inputs3[0,:])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "44077195",
      "metadata": {
        "deletable": false,
        "id": "44077195",
        "nbgrader": {
          "cell_type": "code",
          "checksum": "e533efa116d03805c84c1743fe3f45ba",
          "grade": false,
          "grade_id": "exercise-5-2",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "tags": []
      },
      "outputs": [],
      "source": [
        "model = P6()\n",
        "### BEGIN SOLUTION\n",
        "ds3 = FirstDataset(inputs3, targets3)\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=0.1)\n",
        "loader3 = DataLoader(ds3, batch_size=100, shuffle=True, num_workers=0)\n",
        "\n",
        "for epoch in tqdm(range(100)):\n",
        "    for x, label in loader3:\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        output = model(x)\n",
        "        l = loss_fn(output, label)\n",
        "        l.backward()\n",
        "        optimizer.step()\n",
        "print(\"\\nTraining end\")\n",
        "hits = torch.max(model(inputs3), dim=1)[1] == targets3\n",
        "acc = hits.sum().float() / (3 * n)*100\n",
        "### END SOLUTION\n",
        "print(f\"Accuracy:{acc:.2f}%\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# On a test set :\n",
        "\n",
        "# Sample 0:\n",
        "mean0 = torch.tensor((-1., -1.))\n",
        "ech0 = mean0 + std * torch.randn(n, 2)\n",
        "\n",
        "# Sample 1:\n",
        "mean1 = torch.tensor((1., -1.))\n",
        "ech1 = mean1 + std * torch.randn(n, 2)\n",
        "\n",
        "# Sample 2:\n",
        "mean2 = torch.tensor((0., 1.))\n",
        "ech2 = mean2 + std * torch.randn(n, 2)\n",
        "\n",
        "echs3 = [ech0, ech1, ech2]\n",
        "\n",
        "\n",
        "inputs3 = torch.cat(echs3, dim=0)\n",
        "targets3 = torch.cat((torch.zeros(n), torch.ones(n), 2 * torch.ones(n)), dim=0).long()\n",
        "print(inputs3.shape)\n",
        "print(targets3.shape)\n",
        "print(inputs3[0,:])\n",
        "\n",
        "print(\"\\nTraining end\")\n",
        "hits = torch.max(model(inputs3), dim=1)[1] == targets3\n",
        "acc = hits.sum().float() / (3 * n)*100\n",
        "### END SOLUTION\n",
        "print(f\"Accuracy:{acc:.2f}%\")"
      ],
      "metadata": {
        "id": "B69EofxHqY7F"
      },
      "id": "B69EofxHqY7F",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}