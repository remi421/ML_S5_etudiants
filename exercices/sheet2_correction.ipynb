{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Correction de la fiche d'exercices n°2\n"
      ],
      "metadata": {
        "id": "AvaKH1dZvg2B"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Exercice 1** : notions de densité, lois marginale, loi jointe, loi conditionnelle, couplage.\n",
        "\n",
        "Soit $X = (X_1, X_2)$ un [vecteur gaussien](https://) de loi $\\mathcal{N}(0, \\Gamma)$ avec $\\Gamma = \\begin{pmatrix} 1 & \\epsilon \\\\ \\epsilon & 1 \\end{pmatrix}$ et $\\epsilon \\in [0,1[$\n",
        "\n",
        "**a.** Quelle est la densité de $X$ ?\n",
        "\n",
        "$$f(x_1,x_2) = \\dfrac{1}{ 2 \\pi \\sqrt{ 1 - \\epsilon^2 }} e^{\\text{-} \\tfrac{1}{2}  \\begin{pmatrix} x_1 & x_2 \\end{pmatrix} \\: \\Gamma^{-1} \\begin{pmatrix} x_1 \\\\ x_2 \\end{pmatrix}} $$\n",
        "\n",
        "avec $$\\Gamma^{-1} =  \\dfrac{1}{1 - \\epsilon^2} \\begin{pmatrix} 1 & \\text{-} \\epsilon \\\\ \\text{-} \\epsilon & 1 \\end{pmatrix}$$\n",
        "**b.** Quelles sont les lois marginales de $X$ ?\n",
        "\n",
        "\n",
        "Pour une loi définie sur un espace à $2$ dimensions par une densité $f(x_1, x_2)$ la loi marginale associée à la coordonnée $x_2$ est définie par la densité:\n",
        "$$f_2(\\boldsymbol{x}) = \\int_{x_1} f(x_{1}, \\boldsymbol{x}) \\; dx_1 $$\n",
        "\n",
        "Dans le cas d'un vecteur gaussien, les lois marginales sont entièrement caractérisées par l'espérance et les $\\Gamma_{j,j}$ de la matrice de covariance. Dans notre cas, les marginales sont des lois normales centrées réduites.\n",
        "\n",
        "**c.** Calculer la loi conditionnelle de $X_1$ sachant $X_2 = x_2$. Vers quel couplage tend-on lorsque $\\epsilon \\to 1$ ?\n",
        "\n",
        "La densité conditionnelle est donnée par:\n",
        "\n",
        "$$\\dfrac{f(x_1, x_2)}{f_2(x_2)}$$\n",
        "\n",
        "Ici:\n",
        " $$f(x_1,x_2) \\; \\alpha \\; \\dfrac{exp(\\text{-} \\tfrac{1}{2}  \\begin{pmatrix} x_1 & x_2 \\end{pmatrix} \\: \\Gamma^{-1} \\begin{pmatrix} x_1 \\\\ x_2 \\end{pmatrix})}{exp(\\text{-} \\tfrac{1}{2} x_2^2)}  $$\n",
        "\n",
        "Ce qui donne après simplification:\n",
        "\n",
        "\n",
        "$$ f(x_1,x_2) \\; \\alpha \\; exp(\\text{-} \\tfrac{1}{2 (1 - \\epsilon^2)} (x_1 - \\epsilon x_2)^2 ) $$\n",
        "\n",
        "**Note**: ce calcul peut être réalisé par un [calcul matriciel](http://www.normalesup.org/~rpeyre/pro/ensgt/13/mc/condgauss.pdf).\n",
        "\n",
        "On reconnaît une loi normal d'espérance $\\epsilon x_2$, de variance $1 - \\epsilon^2$.\n",
        "Pour $\\epsilon \\rightarrow 1$ la loi conditionnelle converge vers la masse de Dirac $\\delta_{x_2}$.\\\n",
        "La loi conjointe, elle, converge vers le couplage défini par $X_2 \\sim \\mathcal{N}(0,1) $ et $X_1 = X_2$, c'est à dire une loi gaussienne dégénérée ($det(\\Gamma) = 0$) dont le dont le support est réduit à la diagonale d'équation $x_2 = x_1$.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "**Note**: un couplage de deux lois $\\mathcal{P}$, $\\mathcal{Q}$ est un couple de variables aléatoires (X,Y) tel que $X \\sim \\mathcal{P}$ et $Y \\sim \\mathcal{Q}$."
      ],
      "metadata": {
        "id": "0Lp3zZE_HAzL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Exercice 2** Montrer que la fonction de coût définie par:\n",
        "$$\\mathcal{L}(Y,f_\\theta(x)) = \\rho_t(Y - f_\\theta(x))$$\n",
        "où:\n",
        "$$\\rho_t(u) = t \\times max(u,0) + (t-1) \\times min(u,0) $$\n",
        "\n",
        "\n",
        "est d'espérance minimale lorsque $f_\\theta(x)$ vaut le quantile d'ordre $t$ associé à $p(Y|X=x)$.\n",
        "\n",
        "\n",
        "Indice: montrer d'abord que l'espérance\n",
        "\n"
      ],
      "metadata": {
        "id": "sX_W4VmBvfZb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Réponse** La démonstration repose sur la dérivation des intégrales paramétriques généralisées. Sans s'attarder sur les détails techniques, on peut écrire:\n",
        "\\begin{equation*}\n",
        " \\partial_q [ \\int_{\\mathbb{R}} \\mathcal{L}(y,q) p(y|x) dy] = \\\\\n",
        "   t \\ \\partial_q [  \\int_{q}^{+\\infty}  (y - q) p(y|x) dy ] + (t-1) \\  \\partial_q [\\int_{-\\infty}^q  (y - q)  p(y|x) dy] \\\\\n",
        "=   t - \\int_{-\\infty}^q p(y|x) dy  \n",
        "\\end{equation*}\n",
        "\n",
        "Cette quantité s'annulant clairement pour $q$ tel que $\\mathbb{P}(Y<q) = t$.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "E1VwiDxH-nSn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Exercice 3** : la divergence de Kullback-Leibler\n",
        "\n",
        "Soient deux lois de probabilités $\\mathcal{P}$ et $\\mathcal{Q}$ admettant comme densités $p$ et $q$, **strictement positives** sur $\\mathbb{R}^d$. La divergence de Kullback-Leibler est donnée par :\n",
        "$$ D_{KL} ( \\mathcal{P} || \\mathcal{Q})=  \\int_{\\mathbb{R^d}} p(x) \\; ln{\\bigg (}\\dfrac{p(x)}{q(x)} {\\bigg)} dx $$\n",
        "\n",
        "**a.** Pourquoi cette quantité est-elle positive? Quand est-elle nulle ?\n",
        "\n",
        "On a $$D_{KL} ( \\mathcal{P} || \\mathcal{Q}) = \\mathbb{E}_\\mathcal{P} {\\bigg [} ln(\\dfrac{p(X)}{q(X)}) {\\bigg ]}$$\n",
        "Par l'inégalité de Jensen et la convexité de $ \\text{-}ln$:\n",
        "\n",
        "$$\\mathbb{E}_\\mathcal{P} {\\bigg [} \\text{-} ln(\\dfrac{p(X)}{q(X)}) {\\bigg ]} \\;\\leq \\; \\text{-} ln ( \\mathbb{E}_\\mathcal{P} {\\bigg [} \\dfrac{q(X)}{p(X)} {\\bigg ]} ) = \\text{-} ln (1) = 0 $$\n",
        "\n",
        "Elle n'est nulle que lorsque les deux distributions sont égales.\n",
        "\n",
        "\n",
        "\n",
        "**b.** Calculer cette quantité pour deux lois normales de même variance ($\\sigma^2$) et d'espérances $\\mu_1$ et $\\mu_2$.\n",
        "\n",
        "On a:\n",
        "$$ ln {\\big [} \\dfrac{f_2(x)}{f_1(x)} {\\big ]} =  { \\text{-} \\dfrac{1}{2 \\sigma} {\\big [} (x-\\mu_2)^2} - (x-\\mu_1)^2 {\\big ]}  $$\n",
        "\n",
        "D'où\n",
        "\\begin{align}\n",
        "D_{KL} ( \\mathcal{\\mathcal{N}(\\mu_1,\\sigma)} || \\mathcal{N}(\\mu_2,\\sigma)) & =  \\sigma^2 - \\mathbb{E}_{\\mathcal{N}(\\mu_1,\\sigma)} (X-\\mu_2)^2 \\\\\n",
        "& =  \\dfrac{1}{2} - \\dfrac{1}{2 \\sigma^2}(\\sigma^2 - (\\mu_2 - \\mu_1)^2)\\\\\n",
        "& =  \\dfrac{(\\mu_2 - \\mu_1)^2}{2 \\sigma^2}\n",
        "\\end{align}\n",
        "\n",
        "\n",
        "**c.** Cette quantité définit-elle une distance ?\n",
        "\n",
        "On peut montrer qu'elle ne respecte pas l'inégalité triangulaire. Elle n'est pas symétrique non plus. Par exemple, pour deux lois normales quelconques, on a:\n",
        "\n",
        "\\begin{equation}\n",
        "D_{KL} ( \\mathcal{\\mathcal{N}(\\mu_1,\\sigma_1)} || \\mathcal{N}(\\mu_2,\\sigma_2))  = - \\dfrac{1}{2} +  ln(\\dfrac{\\sigma_2}{\\sigma_1}) + \\dfrac{ (\\mu_2 - \\mu_1)^2 + \\sigma_1^2}{2 \\sigma_2^2}\n",
        "\\end{equation}\n",
        "\n",
        "**Remarque** : dans la suite nous utiliserons une version symétrisée de $D_{KL}$. Il s'agit de la divergence de Jensen-Shannon :\n",
        "\n",
        "$$D_{JS} ( \\mathcal{P} || \\mathcal{Q} )  = \\dfrac{1}{2} D_{KL}(\\mathcal{P}||\\mathcal{M}) +  \\dfrac{1}{2} D_{KL}(\\mathcal{Q}||\\mathcal{M})$$ où $\\mathcal{M} = \\dfrac{1}{2} (\\mathcal{Q} + \\mathcal{P})$\n",
        "\n"
      ],
      "metadata": {
        "id": "1wVcX3o6-bU3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Exercice 4** La distance de Wasserstein\n",
        "\n",
        "Pour définir la version la plus simple de la distance de Wasserstein entre deux distributions $\\mathcal{P}$ et $\\mathcal{Q}$ on suppose seulement qu'elles admettent un moment d'ordre 1. Elle est alors définie par :\n",
        "\n",
        "$$ W(\\mathcal{P},\\mathcal{Q}) = \\underset{\\{\\gamma \\in \\Pi(\\mathcal{P},\\mathcal{Q})\\}}{inf} \\mathbb{E}_{(X,Y) \\sim \\gamma} || X - Y || $$\n",
        "\n",
        "Où $\\Pi(\\mathcal{P},\\mathcal{Q})$ désigne l'ensemble de toutes les lois jointes de marginales $\\mathcal{P}$ et $\\mathcal{Q}$ et $||.||$ désigne la norme usuelle de $\\mathbb{R}^d$.\n",
        "\n",
        "**a.** Reprenons la variable $X$ vue à l'exercice 1.\n",
        "Comment calcule-t-on $\\mathbb{E}_{(X_1,X_2)} | X_2 - X_1 |$ ?\n",
        "\n",
        "Il s'agit de calculer :\n",
        "\n",
        "$$I_\\epsilon = \\dfrac{1}{2 \\pi \\sqrt{ (1 - \\epsilon^2)}} \\int_{\\mathbb{R}^2} |x - y| \\: exp(\\text{-} \\dfrac{1}{2 (1 - \\epsilon^2)} (x^2 + y^2 - 2 \\epsilon x y )) \\; dx dy $$\n",
        "Un changement de variable permet de simplifier le calcul. En effet, en posant: $X = x+y$, $Y = x - y $, l'expression à intégrer est séparable. La seule difficulté résidera dans le deuxième facteur, de la forme: $\\int_{\\mathbb{R}} |Y| exp(\\text{-} \\dfrac{1}{2} Y^2)$. On pourra alors reconnaître l'espérance d'une loi demi-normale.\n",
        "\n",
        "Voilà le développement complet (facultatif):\n",
        "\n",
        "Posons $\\kappa = \\dfrac{1}{2 \\pi \\sqrt{ (1 - \\epsilon^2)}}$. Après changement de variables, on a:\n",
        "\\begin{align}\n",
        "I_\\epsilon &=  \\dfrac{\\kappa}{2} \\int_{\\mathbb{R}^2} |Y| \\: exp(\\text{-} \\dfrac{1}{4 (1-\\epsilon^2)} (X^2 + Y^2 -  \\epsilon (X^2 - Y^2) ))  \\; dX dY \\\\\n",
        "&= \\dfrac{\\kappa}{2} \\: \\int_{\\mathbb{R}}  \\: exp(\\text{-} \\dfrac{1}{4 (1+\\epsilon)} X^2 )) \\: dX \\: \\int_{\\mathbb{R}} |Y| \\: exp(\\text{-} \\dfrac{1}{4 (1-\\epsilon)} Y^2 ))  \\; dY  \n",
        "\\end{align}\n",
        "\n",
        "En posant $\\tilde{X} = \\dfrac{X}{\\sqrt{2(1+\\epsilon)}}$ et $\\tilde{Y} = \\dfrac{Y}{\\sqrt{2(1- \\epsilon)}}$, on obtient:\n",
        "\n",
        "$$ \\int_{\\mathbb{R}}  \\: exp(\\text{-} \\dfrac{1}{4 (1+\\epsilon)} X^2 )) \\: dX = \\sqrt{2(1+\\epsilon)} \\sqrt{2 \\pi} $$\n",
        "et:\n",
        "\\begin{align}\n",
        "\\int_{\\mathbb{R}}  \\: |Y| exp(\\text{-} \\dfrac{1}{4 (1-\\epsilon)} Y^2 )) \\: dY &= 4(1-\\epsilon)  \\int_{\\mathbb{R}^+} |\\tilde{Y}| \\: exp(\\text{-} \\dfrac{1}{2} \\tilde{Y}^2 ))  \\; dY \\\\\n",
        "&=4(1-\\epsilon) \\sqrt{\\dfrac{\\pi}{2}} \\mu_{\\mathcal{N}^{1/2}_1}\n",
        "\\end{align}\n",
        "Où $\\mu_{\\mathcal{N}^{1/2}_1} $ est l'espérance d'une [loi demi-normale de variance 1](https://en.wikipedia.org/wiki/Half-normal_distribution). D'après les tables, cette dernière vaut $\\sqrt{\\dfrac{2}{\\pi}} $.\n",
        "\n",
        "On a donc:\n",
        "\n",
        "$$ I_\\epsilon = \\dfrac{1}{ 4 \\pi \\sqrt{ (1 - \\epsilon^2)}}  \\sqrt{2(1+\\epsilon)} \\sqrt{2 \\pi} 4(1-\\epsilon) = 2 \\sqrt{\\dfrac{1 - \\epsilon}{\\pi}}  $$\n",
        "\n",
        "\n",
        "\n",
        "**b.** La loi de $X$, à $\\epsilon$ fixé, représente l'une des lois jointes de  $\\Pi(\\mathcal{N}(0,1),\\mathcal{N}(0,1))$. Mais que vaut $W(\\mathcal{N}(0,1),\\mathcal{N}(0,1))$ ?\n",
        "\n",
        "Cette quantité est clairement nulle: il suffit de considérer le couplage $X_2 = X_1$ pour s'en rendre compte (couplage qui correspond à la valeur $\\epsilon = 1$).\n",
        "\n",
        "\n",
        "\n",
        "**c.** Quelles sont deux propriétés évidentes de $W$ ?\n",
        "Cette quantité est définie positive et symétrique.\n",
        "\n",
        "\n",
        "La distance de Wasserstein a une interprétation physique bien connue : c'est le travail minimum à fournir pour déplacer une distribution de masses vers une autre distribution de masses. En effet, on peut voir une loi jointe $\\gamma$ comme une façon de ventiler chaque élément de masse $p(x)dx$ vers l'ensemble des éléments de masses $\\gamma(x,y)dx dy$. L'analogie s'obtient en supposant que la force appliquée lors de cette \"ventilation\" est proportionnelle à la masse déplacée.\n",
        "\n",
        "**d.** Intuitivement, pour quelle raison la distance de Wasserstein satisfait-elle l'inégalité triangulaire ?\n",
        "\n",
        "Donnons-nous trois lois de probabilités (ou distributions de masses) $\\mathcal{L}_1$, $\\mathcal{L}_2$ et $\\mathcal{L}_3$. Le plus économe transport de $\\mathcal{L}_1$ vers $\\mathcal{L}_3$  est au moins aussi économe que le plus économe des transports de $\\mathcal{L}_1$ vers $\\mathcal{L}_2$ suivi du plus économe des transports de  $\\mathcal{L}_2$ vers $\\mathcal{L}_3$.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "**Remarque**: De façon générale, la distance de Wasserstein peut difficilement être calculée directement. Pour l'évaluer, on peut se ramener à un problème d'optimisation grâce à la [dualité de Kantorovitch-Rubinstein](https://cedricvillani.org/sites/dev/files/old_images/2012/08/preprint-1.pdf), qui établit un lien avec les fonctions lipschitziennes:\n",
        "\n",
        "$$ W(\\mathcal{P},\\mathcal{Q}) = \\underset{\\ f \\in \\mathcal{F}_{1}}{sup} {\\bigg [} \\mathbb{E}_{X \\sim \\mathcal{P}} [ f(X) ] -  \\mathbb{E}_{ X \\sim \\mathcal{Q}} [ f(X) ]  {\\bigg ]} $$\n",
        "\n",
        "\n",
        "où $\\mathcal{F}_{1}$ représente l'ensemble des fonctions 1-lipschitziennes sur $\\mathbb{R}^d$."
      ],
      "metadata": {
        "id": "werV73Cn-s7w"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Exercice 5**\n",
        "Reprenons les notations du TP6 (P6).\n",
        "D'après [l'article qui introduit les GAN](https://arxiv.org/pdf/1406.2661.pdf), qu'est-ce qui garantit la convergence de $\\mathcal{L}_{G(Z)}$ vers $\\mathcal{L}_{X}$ ?\n",
        "Quel outil principal utilise-t-on pour en faire la preuve ?"
      ],
      "metadata": {
        "id": "sJPA8J7gDVIZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Le même article donne des raisons d'espérer la convergence.\n",
        "Voilà les deux points principaux du raisonnement:\n",
        "\n",
        "**A**) Pour un générateur $G$ donné, le discriminateur optimal $D^*$ qui maximise:\n",
        "\n",
        "$$ \\mathbb{E}_X [ln D(X)] + \\mathbb{E}_Z [ln(1 - D(G(Z)))] $$\n",
        "\n",
        "est connu. Il vaut:\n",
        "\n",
        "$$ D^*_G(x) =   \\dfrac{p_X(x)}{p_X(x) + p_{G}(x)} $$\n",
        "\n",
        "où $p_X$ représente la densité associée à la loi \"naturelle\" sur le domaine des images réelles (à supposer qu'elle existe) et $p_{G}(x)$ représente la densité de $G(Z)$.\n",
        "\n",
        "**B**) Le générateur $G^*$ qui maximise :\n",
        "\n",
        "$$ \\mathbb{E}_X [ln D^*_G(X)] + \\mathbb{E}_Z [ln(1 - D^*_G(G(Z)))] $$\n",
        "\n",
        "est connu. Il vérifie: $p_{G^*} = p_X$.\n",
        "\n",
        "Il reste à montrer que les gradients calculés à l'étape d'optimisation sur $G$ donnent bien la direction de $G^*$. Les auteurs le montrent dans le cas où $D^*_G$ est atteint à chaque étape d'optimisation sur $D$. \\\n",
        "Ce point plus délicat ne sera pas démontré ici, mais on pourra garder en tête une conséquence pratique: un discriminateur qui [reste performant](https://machinelearningmastery.com/practical-guide-to-gan-failure-modes/) au cours de l'apprentissage de justesse est à chercher.\n",
        "\n",
        "L'idée derrière la démonstration de **A** est simple. La preuve du point **B**, plus intéressant, fait apparaître la divergence de Jensen-Shannon.\n",
        "\n",
        "**Démonstration** du point **A**:\n",
        "\n",
        "Notons $\\mathcal{X} = \\mathcal{R}^{N_{pixels}}$, l'espace dans lequel vivent les variables aléatoires $X$ et $G(Z)$. On suppose que ces deux variables admettent comme densités $p_X$ et $p_G$, définies sur  $\\mathcal{X}$. \\\n",
        "\n",
        "L'espérance $\\mathbb{E}_Z [ln(1 - D(G(Z)))]$ peut être réécrite $\\int_\\mathcal{X} ln(1-D(x)) \\: p_G (x) \\: dx$. Il s'agit donc de trouver la fonction $D$ qui minimise l'intégrale :\n",
        "\n",
        "$$ \\int_\\mathcal{X} ln(D(x)) \\: p_X(x)  +  ln (1 - D(x) ) p_G (x) \\: dx $$\n",
        "\n",
        "On l'obtient en minimisant l'expression intégrée, i.e. en cherchant $$ \\underset{a}{argmin} \\:\\: b \\: ln(a)  + c \\: ln(1 - a)   $$\n",
        "\n",
        "On montre alors facilement que le minimum est atteint pour $a = \\dfrac{b}{b + c}$.\n",
        "\n",
        "**Démonstration** du point **B**:\n",
        "\n",
        "Posons $\\tilde{X} = G(Z)$, $\\mathcal{P}_X$ et $\\mathcal{P}_G$ les lois de densités $p_X$ et $p_G$, et $\\mathcal{M} = \\dfrac{1}{2} (\\mathcal{Q} + \\mathcal{P})$.\\\n",
        "D'après le point **A**, $G^*$ doit minimiser:\n",
        "\n",
        "\\begin{align}\n",
        "D(p_X, p_G) \\: &=  \\: \\mathbb{E}_X \\: {\\bigg [} \\ln ( \\dfrac{p_X (X)}{p_X (X) + p_G(X)} ) {\\bigg ]} + \\mathbb{E}_Z \\: {\\bigg [} ln ( \\dfrac{p_G(\\tilde{X})}{p_X(\\tilde{X}) + p_G(\\tilde{X})}) {\\bigg ]}\\\\\n",
        "&=  \\: \\dfrac{1}{2}  {\\bigg [} D_{KL} ( \\mathcal{P}_X || \\mathcal{M}_G ) + D_{KL} ( \\mathcal{P}_G || \\mathcal{M}_G) {\\bigg ]}\\\\\\\n",
        "&=  \\: D_{JS} ( \\mathcal{P}_X || \\mathcal{P}_G )\n",
        "\\end{align}\n",
        "\n",
        "Le minimum est atteint lorsque les lois de $X$ et $G(Z)$ sont identiques."
      ],
      "metadata": {
        "id": "EvGxPt-cDep8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "**Exercice n°6** Le principe des Wasserstein GAN.\n",
        "Reprenons le contexte de l'exercice Dans la version où les poids sont seuillés (WGAN), quelle quantité joue le rôle de fonction de coût lors de l'optimisation de $D$ ?"
      ],
      "metadata": {
        "id": "P4q1BwMBEPtZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q1** La minimisation de la distance de Wasserstein entre la distribution générée et la distribution cible est obtenue par un contrôle des gradients associés au discriminateur. Pour quelle raison, intuitivement ?"
      ],
      "metadata": {
        "id": "yUYcPl_1FLQO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "On a évoqué la dualité de Kantorovich-Rubinstein dans la partie introductive. Elle nous dit que la distance de Wasserstein peut être obtenue à partir de fonctions Lipschitziennes. Contrôler les gradients permettra justement de limiter la constante de Lipschitz du discriminateur. Une analyse plus rigoureuse est présentée à la fin de l'exercice."
      ],
      "metadata": {
        "id": "sPxWKpYtFPAz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "En reprenant les notations de tout à l'heure, la fonction de coût que $D$ doit minimiser est une estimation de :\n",
        "\n",
        "$$ \\mathcal{L}(G,D) = \\mathbb{E}_Z(D(G(Z))) - \\mathbb{E}_X(D(X)) $$\n",
        "\n",
        "\n",
        "\n",
        "Le contrôle des gradients associés à chacune des couches de $D$ garantit son caractère 1-Lipschitzien à une constante multiplicative près. Posons:\n",
        "$$ D^*_G = \\underset{D \\in \\mathcal{F}_1}{argmin} \\: \\mathcal{L}(G,D) $$ Selon la dualité de Kantorovitch-Rubinstein, on a :\n",
        "\n",
        "$$ - \\: \\mathcal{L}(G,D^*_G) = W(\\mathcal{P}_X,\\mathcal{P}_G) $$\n",
        "\n",
        "A supposer que $D^*_G$ ait été atteint, l'étape d'optimisation sur $G$ consiste donc à descendre les gradients de $W(\\mathcal{P}_X,\\mathcal{P}_G)$, ce qui, intuitivement, conduit à $p_G = p_X$.\n",
        "\n",
        "Si ces dernières lignes permettent de comprendre l'algorithme, elles ne disent rien de l'avantage de la distance de Wasserstein par rapport à la divergence de Jensen-Shannon. \\\n",
        "\n",
        "\n",
        "........\n",
        "\n",
        "L'avantage de cette distance, c'est sa continuité par rapport à la convergence en loi.  Cet avantage ainsi que les éléments de preuve de la convergence dépassent le cadre de cette fiche d'exercice, font l'objet de la partie théorique de l'article introductif.  "
      ],
      "metadata": {
        "id": "4dB5bBzCF7f8"
      }
    }
  ]
}