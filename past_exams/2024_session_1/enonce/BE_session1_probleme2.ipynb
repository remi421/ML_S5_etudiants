{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Problème: un VIT pour du denoising"
      ],
      "metadata": {
        "id": "-1ciEeyNevrd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Depuis 2017, les **transformers** ont gagné en popularité. Ces réseaux de neurones très performants font intervenir un type de couche qui n'a pas été décrit en TP: la couche \"attentionnelle\". Ce problème présente un VIsual Transformer (VIT) très simple."
      ],
      "metadata": {
        "id": "CAkaS7_je3Is"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pour entraîner notre premier transformer, nous reprenons le contexte d'une tâche de denoising (TP3). Les lignes de code suivantes permettent de visualiser les entrées et les cibles:"
      ],
      "metadata": {
        "id": "J5SamHaEIxk8"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wqz7U32_LjwB"
      },
      "outputs": [],
      "source": [
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! wget https://www.grosfichiers.com/mfh8gy5ZmfB_zaQP9HZKdDk\n",
        "! mv mfh8gy5ZmfB_zaQP9HZKdDk utile_BE.py\n",
        "\n",
        "from utile_BE import *"
      ],
      "metadata": {
        "id": "xoyVAZppo4VS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input, target = gen(6)\n",
        "\n",
        "#versions bruitées (rectangles pleins et bruités)\n",
        "fig0 = plt.figure(0, figsize=(36, 6))\n",
        "voir_batch2D(input, 6, fig0, k=0, min_scale=0,max_scale=1)\n",
        "\n",
        "#versions propres (cellules seules)\n",
        "fig1 = plt.figure(1, figsize=(36, 6))\n",
        "voir_batch2D(target, 6, fig1, k=0, min_scale=0,max_scale=1)"
      ],
      "metadata": {
        "id": "f3EQA_WG599w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Maintenant, chargeons les bibliothèques/fonctions utiles. Pour information, **timm** contient les briques de base qui permettent de coder/d'entraîner/de fine tuner la plupart des modèles récents, et en particulier les transformers (ViT, DEiT, BEiT, etc)."
      ],
      "metadata": {
        "id": "YaZzhqfj_0_0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install timm\n",
        "! pip install einops"
      ],
      "metadata": {
        "id": "JSgnRD9p8n8n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from functools import partial\n",
        "from timm.models.layers import trunc_normal_\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# Pour initialiser les poids:\n",
        "def init_weights(m):\n",
        "    if isinstance(m, nn.Linear):\n",
        "        trunc_normal_(m.weight, std=0.02)\n",
        "        if isinstance(m, nn.Linear) and m.bias is not None:\n",
        "            nn.init.constant_(m.bias, 0)\n",
        "    elif isinstance(m, nn.LayerNorm):\n",
        "        nn.init.constant_(m.bias, 0)\n",
        "        nn.init.constant_(m.weight, 1.0)"
      ],
      "metadata": {
        "id": "j2br-1bI8dBv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Le \"transformer\" a été conçu pour une tâche de traduction de texte. Il opère donc sur une séquence de mots.\\\n",
        "Le \"vision transformer\" est une adaptation du transformer à des tâches de vision par ordinateur. Cette adaptation est fondée sur un découpage de l'image d'entrée en imagettes (\"patchs\").\n",
        "Chaque patch est alors traité comme un caractère musical dans le TP 4 partie 2: il est projeté dans un espace de représentation propre au modèle.\n",
        "\n",
        "Les hyperparamètres ci-dessous précisent, entre autres, les caractéristiques de l'image d'entrée, la taille des patchs (8 x 8), le nombre de dimensions de l'espace de représentation (\"d_model\").\\\n",
        "Le sens des autres hyperparamètres sera donné au fil de l'énoncé.\n"
      ],
      "metadata": {
        "id": "J-J0Ae7pJIzp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "image_size = [64,64]\n",
        "channels = 1\n",
        "patch_size = 8\n",
        "d_model = 32\n",
        "mlp_expansion_ratio = 4\n",
        "d_ff = mlp_expansion_ratio * d_model\n",
        "n_heads = 4\n",
        "n_layers = 12"
      ],
      "metadata": {
        "id": "mXSr1BsmQbKv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **A.** Projection dans l'espace de représentation (Embedding)"
      ],
      "metadata": {
        "id": "NHpeiZzqS20H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q1** Préciser la forme (.shape) du tenseur d'entrée (*input*).\\\n",
        "Que représente la première composante (chiffre 6) ?"
      ],
      "metadata": {
        "id": "vrH15kMOMkyW"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "AJDDC3krMkR2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q2** La classe *PatchEmbedding* suivante permet de découper une image en patchs et projeter les patchs dans un epace de dimension donnée par la variable *d_model*.\n",
        "L'instancier avec les hyperparamètres précédents.  \n",
        "L'appliquer au tenseur *input* et stocker le résultat dans la variable *x*."
      ],
      "metadata": {
        "id": "bohcngPjCkTV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class PatchEmbedding(nn.Module):\n",
        "    def __init__(self, image_size, patch_size, d_model, channels):\n",
        "        super().__init__()\n",
        "\n",
        "        self.image_size = image_size\n",
        "        if image_size[0] % patch_size != 0 or image_size[1] % patch_size != 0:\n",
        "            raise ValueError(\"image dimensions must be divisible by the patch size\")\n",
        "        self.grid_size = image_size[0] // patch_size, image_size[1] // patch_size\n",
        "        self.num_patches = self.grid_size[0] * self.grid_size[1]\n",
        "        self.patch_size = patch_size\n",
        "\n",
        "        self.proj = nn.Conv2d(\n",
        "            channels, d_model, kernel_size=patch_size, stride=patch_size\n",
        "        )\n",
        "\n",
        "    def forward(self, im):\n",
        "        B, C, H, W = im.shape\n",
        "        x = self.proj(im).flatten(2).transpose(1, 2)\n",
        "        return x"
      ],
      "metadata": {
        "id": "KykBXCq1RHUW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "patch_embed = ...\n",
        "x = patch_embed( ... )"
      ],
      "metadata": {
        "id": "D-5h9mvqP9kH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q3** Préciser la forme du tenseur *x*. Pourquoi la seconde composante vaut-elle 64 ?\n"
      ],
      "metadata": {
        "id": "QYDiCzOzPpBD"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "dJFa0sb9P6WW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q4** : l'objet *embed* contient-il des paramètres entraînables ? Quel type de couche de neurones est employé ici ? Préciser le rôle du paramètre *stride* dans le \"découpage\" de l'imagette."
      ],
      "metadata": {
        "id": "TYfKnox41aAB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dans cette opération de découpage, l'information sur la position relative des patchs dans l'image est partiellement perdue. Pour compenser cette perte, on associe à chacune des positions possibles un vecteur de l'espace des représentations (tokénisation). On ajoute ces vecteurs aux projections.\\\n",
        "Ceci se traduit par l'opération suivante:"
      ],
      "metadata": {
        "id": "JPbeFOJnRXBp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pos_embed = nn.Parameter(torch.randn(1, patch_embed.num_patches, d_model))\n",
        "\n",
        "x = x + pos_embed"
      ],
      "metadata": {
        "id": "g_SupfKbS6vW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q5** A ce stade, combien le réseau compte-t-il de paramètres entraînables ?"
      ],
      "metadata": {
        "id": "Pjl9K-PBVCex"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Le tenseur *x* est ensuite passé à travers une série de blocs identiques (*n_layers* représente le nombre de blocs).\\\n",
        "Chaque bloc est constitué, dans l'ordre :\n",
        "- d'une opération de normalisation particulière (nous n'en discuterons pas ici)\n",
        "- d'une couche \"attentionnelle\"\n",
        "- d'une couche nommée \"FeedForward\".\n",
        "\n",
        "Voyons d'abord **la couche attentionnelle**."
      ],
      "metadata": {
        "id": "r2AxyI6fTs9y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **B.** La couche attentionnelle"
      ],
      "metadata": {
        "id": "g5JbYXllTEE_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "L'idée de base est simple : pour \"comprendre\" le contenu d'un patch, il n'est pas nécessaire de consulter tous les autres patches, mais seulement certains d'entre eux. En d'autre termes, il faut prêter \"attention\"  aux patchs pertinents.\\\n",
        "La couche attentionnelle encode un mécanisme de sélection des patchs les plus pertinents. Notez qu'elle opère non directement sur les patchs, mais sur les **vecteurs** auxquels ils ont été associés via l'étape d'embedding (les vecteurs contenus dans *x*).  \n",
        "\n",
        "Précisément, une couche attentionnelle \"simple\" contient les opérations suivantes:\n",
        "- une première application linéaire projette un **vecteur** d'entrée vers trois vecteurs : le premier est appelé \"requête\" (query), le second \"clef\" (key) et le troisième \"valeur\" (value).\n",
        "- pour chaque patch $i$, on évalue la \"pertinence\" du patch $j$ par le produit scalaire entre la requête $q_i$ associée au patch $i$ et la clef $k_j$ associée au patch $j$. Une opération de normalisation propre à la couche attentionnelle est appliquée à l'ensemble de ces produits scalaires.\n",
        "- on associe alors au patch $i$ la somme des valeurs $v_j$ pondérées par ces produits scalaires normalisés.\n",
        "- une seconde application linéaire transforme cette somme pondérée en le vecteur de sortie, de même dimension que le vecteur d'entrée.\n",
        "\n",
        "Dernière subtilité: dans un VIT, ce sont des couches attentionnelles composées (multi-head attention) qui sont utilisées. Chaque \"tête\" correspond à une couche attentionnelle simple.\n",
        "La classe ci-dessous code pour une couche attentionnelle composée :"
      ],
      "metadata": {
        "id": "qaerPAUieXG8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Attention(nn.Module):\n",
        "    def __init__(self, d_model, n_heads):\n",
        "        super().__init__()\n",
        "\n",
        "        # Nombre de \"têtes\"\n",
        "        self.heads = n_heads\n",
        "\n",
        "        # Taille des vecteurs requête, clefs, valeur\n",
        "        self.head_dim = d_model // n_heads\n",
        "\n",
        "        # Scalaire utilisé à l'étape de normalisation\n",
        "        self.scale = self.head_dim ** -0.5\n",
        "\n",
        "        # Première application linéaire\n",
        "        self.qkv = nn.Linear(d_model, n_heads * self.head_dim * 3)\n",
        "\n",
        "        # Seconde application linéaire\n",
        "        self.proj = nn.Linear(n_heads * self.head_dim, d_model)\n",
        "\n",
        "\n",
        "    def forward(self, x, mask=None):\n",
        "        B, N, C = x.shape\n",
        "\n",
        "        # Calcul des requêtes, clefs, valeurs\n",
        "        qkv = (\n",
        "            self.qkv(x)\n",
        "            .reshape(B, N, 3, self.heads, self.head_dim)\n",
        "            .permute(2, 0, 3, 1, 4)\n",
        "        )\n",
        "        q, k, v = (\n",
        "            qkv[0],\n",
        "            qkv[1],\n",
        "            qkv[2],\n",
        "        )\n",
        "        # Produits scalaires\n",
        "        p = (q @ k.transpose(-2, -1))\n",
        "\n",
        "        # A décommenter pour la question 10:\n",
        "        # print(q.shape, k.transpose(-2, -1).shape)\n",
        "\n",
        "        # Etape de normalisation\n",
        "        p *= self.scale\n",
        "        p = p.softmax(dim=-1)\n",
        "\n",
        "        # Somme pondérée\n",
        "        x = (p @ v).transpose(1, 2).reshape(B, N, C)\n",
        "\n",
        "        # Sortie\n",
        "        x = self.proj(x)\n",
        "\n",
        "        return x"
      ],
      "metadata": {
        "id": "zNSDGj95YOAY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q6** Instancier la classe *Attention* et l'appliquer au tenseur *x*. Cette couche conserve-t-elle les dimensions du tenseur d'entrée ?"
      ],
      "metadata": {
        "id": "qG-Yt9i5ZUI-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "attn = Attention(d_model, n_heads)\n",
        "x =  ..."
      ],
      "metadata": {
        "id": "jzOaU3eQZ5Ga"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q7** Que représente le symbole @ pour des tenseurs **d'ordre supérieur à deux** selon la [PEP 465](https://peps.python.org/pep-0465/#:~:text=This%20PEP%20proposes%20the%20minimum,%2C%20and%20%40%20for%20matrix%20multiplication.) ?"
      ],
      "metadata": {
        "id": "EUPv6WsRbluS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q8** Une couche attentionnelle composée est-elle linéaire ?\n",
        "Préciser de quelles opérations proviennent la non-linéarité."
      ],
      "metadata": {
        "id": "itygKSV-qu2K"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q9** Le nombre de poids dépend-il du nombre de têtes ? Justifier."
      ],
      "metadata": {
        "id": "NUTqveNnjKpE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q10** Un nombre de tête plus élevé a-t-il un effet sur la complexité du calcul ? Répondre en considérant les calculs matriciels effectués pour différentes valeurs de *n_heads* (eg. décommenter le *print* dans la classe *Attention*)."
      ],
      "metadata": {
        "id": "iDXBCq82kk50"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **C.** Définition d'un bloc"
      ],
      "metadata": {
        "id": "0taON13VhUUF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q11** Dans un bloc, la couche attentionnelle est suivie d'un réseau de la classe FeedForward (voir ci-dessous).\n",
        "Comment appelle-t-on ce type de couche ?\n",
        "Quelle est-ici la fonction d'activation ? La décrire en quelques mots."
      ],
      "metadata": {
        "id": "JBi9xowSn0cp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class FeedForward(nn.Module):\n",
        "    def __init__(self, d_model, d_ff, out_dim=None):\n",
        "        super().__init__()\n",
        "        self.fc1 = nn.Linear(d_model, d_ff)\n",
        "        self.act = nn.GELU()\n",
        "        if out_dim is None:\n",
        "            out_dim = d_model\n",
        "        self.fc2 = nn.Linear(d_ff, out_dim)\n",
        "\n",
        "    def unwrapped(self):\n",
        "        return self\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.fc1(x)\n",
        "        x = self.act(x)\n",
        "        x = self.fc2(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "upJwcFtXXGIS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q12** Appliquer un objet de cette classe à *x*. A quoi correspond le paramètre *d_ff* ?"
      ],
      "metadata": {
        "id": "R-1R5usLcnBC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ff = ...\n",
        "\n",
        "x = ..."
      ],
      "metadata": {
        "id": "0zqJcicNsscb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Un bloc complet peut maintenant être défini. Il contient les deux couches précédentes et des opérations de normalisation paramétrables :"
      ],
      "metadata": {
        "id": "EjC9W1yRvEeY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Block(nn.Module):\n",
        "    def __init__(self, d_model, n_heads, d_ff):\n",
        "        super().__init__()\n",
        "        self.norm1 = nn.LayerNorm(d_model)\n",
        "        self.norm2 = nn.LayerNorm(d_model)\n",
        "        self.attn = Attention(d_model, n_heads)\n",
        "        self.mlp = FeedForward(d_model, d_ff)\n",
        "\n",
        "\n",
        "    def forward(self, x, mask=None, return_attention=False):\n",
        "        #a modifier (voir question 14)\n",
        "        y = self.attn(self.norm1(x), mask)\n",
        "        y = self.mlp(self.norm2(y))\n",
        "\n",
        "        return y"
      ],
      "metadata": {
        "id": "PiQ47au8tyx8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q13** Vérifier qu'une application successive de plusieurs blocs ne changera pas les dimensions du tenseur d'entrée."
      ],
      "metadata": {
        "id": "_jnOKSeVeNYA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "..."
      ],
      "metadata": {
        "id": "1Da1ZbPsedNs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q14** Dans la figure *1* de l'[article qui définit les Visual Transformers](https://arxiv.org/pdf/2010.11929.pdf), le bloc correspond au rectangle gris à droite de la ligne pointillée verticale.\n",
        "Compléter la méthode forward pour que la correspondance soit parfaite."
      ],
      "metadata": {
        "id": "aTrefcZltYvp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **D.** Définition d'un Denoising-VIT :"
      ],
      "metadata": {
        "id": "oGprWGWhhi6M"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Nous en savons assez pour définir une classe simple de transformers:"
      ],
      "metadata": {
        "id": "1HJ2DfH_yKIw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class VisionTransformer(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        image_size,\n",
        "        patch_size,\n",
        "        n_layers,\n",
        "        d_model,\n",
        "        d_ff,\n",
        "        n_heads,\n",
        "        channels=1,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.patch_embed = PatchEmbedding(\n",
        "            image_size,\n",
        "            patch_size,\n",
        "            d_model,\n",
        "            channels,\n",
        "        )\n",
        "        self.patch_size = patch_size\n",
        "        self.n_layers = n_layers\n",
        "        self.d_model = d_model\n",
        "        self.d_ff = d_ff\n",
        "        self.n_heads = n_heads\n",
        "        self.pos_embed = nn.Parameter(\n",
        "                torch.randn(1, self.patch_embed.num_patches, d_model) # pas +1\n",
        "            )\n",
        "        trunc_normal_(self.pos_embed, std=0.02)\n",
        "\n",
        "        self.blocks = nn.ModuleList(\n",
        "            [Block(d_model, n_heads, d_ff) for i in range(n_layers)]\n",
        "        )\n",
        "        self.norm = nn.LayerNorm(d_model)\n",
        "\n",
        "        self.apply(init_weights)\n",
        "\n",
        "\n",
        "\n",
        "    def forward(self, im):\n",
        "        # Embedding (à compléter):\n",
        "        x = ...\n",
        "\n",
        "        # Cascade de blocs (à compléter):\n",
        "        for blk in self.blocks:\n",
        "            ...\n",
        "\n",
        "        # normalisation finale de l'encodeur:\n",
        "        x = self.norm(x)\n",
        "\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "hv41tI-Jy335"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q15** Compléter la méthode forward de cette classe puis l'instancier avec les paramètres définis plus haut. S'assurer que le modèle instancié peut prendre entrée le batch généré au début du problème. Vérifier la forme du tenseur de sortie."
      ],
      "metadata": {
        "id": "gDjYXtpJzK0Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "...\n",
        "\n",
        "print(encoder(input).shape)"
      ],
      "metadata": {
        "id": "9jRM5EaPgGn5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pour obtenir une prédiction à l'échelle du pixel, nous allons nous donner une classe de décodeurs (*Decoder*). La classe *Denoiser* qui la suit combine  l'encodeur et le décodeur :"
      ],
      "metadata": {
        "id": "pYwzACvZ3qH5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from einops import rearrange\n",
        "\n",
        "class double_conv(nn.Module):\n",
        "    '''(conv => BN => ReLU) * 2'''\n",
        "    def __init__(self, in_ch, out_ch):\n",
        "        super(double_conv, self).__init__()\n",
        "        self.conv = nn.Sequential(\n",
        "            nn.Conv2d(in_ch, out_ch, 3, padding=1),\n",
        "            nn.BatchNorm2d(out_ch),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(out_ch, out_ch, 3, padding=1),\n",
        "            nn.BatchNorm2d(out_ch),\n",
        "            nn.ReLU(inplace=True)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv(x)\n",
        "        return x\n",
        "\n",
        "class Up0(nn.Module):\n",
        "    def __init__(self, in_ch, out_ch, bilinear=False):\n",
        "        super(Up, self).__init__()\n",
        "        self.up = nn.ConvTranspose2d(in_ch, in_ch, kernel_size=2, stride=2)\n",
        "        self.conv = double_conv(in_ch, out_ch)\n",
        "\n",
        "    def forward(self, x1):\n",
        "        x1 = self.up(x1)\n",
        "        x = self.conv(x1)\n",
        "        return x\n",
        "\n",
        "class outconv(nn.Module):\n",
        "    def __init__(self, in_ch, out_ch):\n",
        "        super(outconv, self).__init__()\n",
        "        self.conv = nn.Conv2d(in_ch, out_ch, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    \"\"\"\n",
        "    vaut uniquement si channels = 1 et image_size =64\n",
        "    \"\"\"\n",
        "    def __init__(self, patch_size, d_model):\n",
        "        super().__init__()\n",
        "\n",
        "        self.d_model = d_model\n",
        "        self.ps = patch_size\n",
        "\n",
        "        self.head = nn.Linear(self.d_model, self.ps**2)\n",
        "        self.apply(init_weights)\n",
        "        self.up1 =  Up0(32, 16)\n",
        "        self.up2 =  Up0(16, 8)\n",
        "        self.up3 =  Up0(8, 4)\n",
        "        self.outc = outconv(4, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        x = rearrange(x, \"b (h w) c -> b c h w\", h=8)\n",
        "        x = self.up1(x)\n",
        "        x = self.up2(x)\n",
        "        x = self.up3(x)\n",
        "        x = self.outc(x)\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "14lk1vO6xmKO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Denoiser(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        encoder,\n",
        "        decoder,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "\n",
        "\n",
        "    def forward(self, im):\n",
        "\n",
        "        x = self.encoder(im)\n",
        "        x = self.decoder(x)\n",
        "\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "qnH0S7d9xCbR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q16** Quels sont les rôles de *nn.ConvTranspose2d* et de la fonction *rearrange* dans le code ? En quoi ce décodeur diffère-t-il de celui qu'on trouve dans un UNet ?"
      ],
      "metadata": {
        "id": "R70ff-Y4uPuH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q17** Compléter la classe Decoder pour que la sortie du réseau ait la même taille que l'entrée."
      ],
      "metadata": {
        "id": "J_eiE_WWt4MI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q18** Instancier un Denoiser. Vérifier les dimensions en sortie de ce modèle."
      ],
      "metadata": {
        "id": "03uqKrDBB5Fn"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0RagESf5B-UP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q19** Combien un Denoiser compte-t-il de poids entraînables ?"
      ],
      "metadata": {
        "id": "9YKo9TADEFGv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **E.** Entraînement d'un Denoising-VIT"
      ],
      "metadata": {
        "id": "Lpgw3KpFiq0s"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q20** Entraîner le model sur carte GPU (30 époques, MSE, optimizer d'ADAM avec des paramètres standard). Visualiser la sortie et commenter."
      ],
      "metadata": {
        "id": "jko0hzyyhw6J"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "NUrMznA6imC2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "zLGnPwaADUNr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "a9BqbnFx5nPZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#visualization:\n",
        "\n",
        "model.eval()\n",
        "\n",
        "inputs, targets = gen(6)\n",
        "inputs = inputs.to(device)\n",
        "targets = targets.to(device)\n",
        "\n",
        "outputs = model(inputs)\n",
        "print(outputs.shape)\n",
        "fig = plt.figure(0, figsize=(36, 6))  # first row: inputs\n",
        "voir_batch2D(inputs.cpu(), 6, fig, k=0, min_scale=0, max_scale=1)\n",
        "fig2 = plt.figure(1, figsize=(36, 6))  # second row: ground truth\n",
        "voir_batch2D(targets.detach().cpu(), 6, fig2, k=0, min_scale=0, max_scale=1)\n",
        "fig3 = plt.figure(2, figsize=(36, 6))  # last row: outputs\n",
        "voir_batch2D(outputs.detach().cpu(), 6, fig2, k=0, min_scale=0, max_scale=1)"
      ],
      "metadata": {
        "id": "DUUscMZz5rG4"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "V100"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}